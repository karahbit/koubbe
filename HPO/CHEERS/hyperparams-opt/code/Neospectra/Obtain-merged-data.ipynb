{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import gallodata\n",
    "import features\n",
    "import synth\n",
    "\n",
    "DATA_PATH = os.path.join(os.getcwd(), '..', '..', '..', 'data')\n",
    "NEO_DATA_SRC = '1006/NeoSpectra'\n",
    "GROUND_TRUTH_FILE = os.path.join(DATA_PATH, '1006/Optimal Solutions Data Set from ASL.xlsx')\n",
    "GROUND_TRUTH_INOCULATION_FILE = os.path.join(DATA_PATH, '1006/Optimal Solutions Data Set.xlsx')\n",
    "ADD_TANK_LABEL = True\n",
    "BLIND_TEST_MODE = False\n",
    "\n",
    "NEO_DATA_FILENAME1 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo09262017.csv')\n",
    "NEO_DATA_FILENAME2 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo09272017.csv')\n",
    "NEO_DATA_FILENAME3 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo09282017.csv')\n",
    "NEO_DATA_FILENAME4 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo09292017.csv')\n",
    "NEO_DATA_FILENAME5 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo10022017.csv')\n",
    "NEO_DATA_FILENAME6 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo10032017.csv')\n",
    "NEO_DATA_FILENAME7 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo10042017.csv')\n",
    "NEO_DATA_FILENAME8 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo10052017.csv')\n",
    "NEO_DATA_FILENAME9 = os.path.join(DATA_PATH, NEO_DATA_SRC, 'Gallo10062017.csv')\n",
    "\n",
    "#Extended multiplicative scatter correction\n",
    "USE_EMSC = False\n",
    "NORMALIZE = True\n",
    "EMSC_ORDER = 2\n",
    "USE_DERIVATIVE = False\n",
    "DERIVATIVE_ORDER = 2\n",
    "USE_TRANSMITTANCE_ONLY = False\n",
    "USE_TRANSMITTANCE = False\n",
    "\n",
    "USE_ALL_DAYS = False\n",
    "USE_ALL_DAYS_SPLIT = 5.0/6.0\n",
    "NORMALIZE_OUTPUT = False\n",
    "USE_SYNTHETIC_DATA = True\n",
    "SYNTH_ADDITION_FACTOR = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the ground truth data for all the target variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_file = 'groundTruthMap.log'\n",
    "originalStdOut = sys.stdout\n",
    "sys.stdout = open(output_file, 'w')\n",
    "print('Writing to output file:', output_file)\n",
    "\n",
    "groundTruthMap = {}\n",
    "groundTruthMap = gallodata.generateGroundTruth(GROUND_TRUTH_FILE, groundTruthMap)\n",
    "groundTruthMap = gallodata.generateGroundTruthInnoculation(GROUND_TRUTH_INOCULATION_FILE, groundTruthMap)\n",
    "groundTruthMap = gallodata.generateGroundTruthGrapeColorMap(GROUND_TRUTH_INOCULATION_FILE, groundTruthMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain the train/test data from files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NeoSpectra evaluation\n",
    "'''\n",
    "trainFilesMap = { '09/26/2017' : NEO_DATA_FILENAME1,\n",
    "                  '09/27/2017' : NEO_DATA_FILENAME2,\n",
    "                  '09/28/2017' : NEO_DATA_FILENAME3,\n",
    "                  '09/29/2017' : NEO_DATA_FILENAME4,                  \n",
    "                  '10/03/2017' : NEO_DATA_FILENAME6,\n",
    "                  '10/05/2017' : NEO_DATA_FILENAME8,\n",
    "                  '10/06/2017' : NEO_DATA_FILENAME9\n",
    "                  }\n",
    "testFilesMap = {'10/02/2017' : NEO_DATA_FILENAME5}\n",
    "\n",
    "# This function will generate the test and train data used originally for each target metric,\n",
    "# it will merge them and save them in a NumPy format so we can use the merged data directly in HyperSpace.\n",
    "def train_test_merge(trainFilesMap, testFilesMap):\n",
    "    extractedDataTrainMap = gallodata.getNeospectraDataFromFiles(groundTruthMap, trainFilesMap)\n",
    "    print ('extractedDataTrainMap......................',extractedDataTrainMap)\n",
    "    if USE_SYNTHETIC_DATA:\n",
    "        extractedDataTrainMap = synth.getNeoSynthData(extractedDataTrainMap, SYNTH_ADDITION_FACTOR)\n",
    "    \n",
    "    # Assign the features (level of absorption values) of training data to xDataTrain\n",
    "    # And, features (level of absorption values) of training data to xDataBlind \n",
    "\n",
    "    xDataTrain = extractedDataTrainMap['features']\n",
    "    extractedDataTestMap = gallodata.getNeospectraDataFromFiles(groundTruthMap, testFilesMap, BLIND_TEST_MODE)\n",
    "    xDataBlind = extractedDataTestMap['features'] \n",
    "\n",
    "    # false, So, we will skip the ‘if’ block.\n",
    "    if USE_ALL_DAYS:\n",
    "        allFilesMap = trainFilesMap\n",
    "        for key in testFilesMap:\n",
    "            allFilesMap[key] = testFilesMap[key]\n",
    "        \n",
    "        totalExtractedDataMap = gallodata.getNeospectraDataFromFiles(groundTruthMap, allFilesMap)\n",
    "        \n",
    "        extractedDataTrainMap = {}\n",
    "        extractedDataTestMap = {}\n",
    "        n = totalExtractedDataMap['features'].shape[0]\n",
    "        p = np.random.permutation(n)\n",
    "        for key in totalExtractedDataMap:\n",
    "            totalExtractedDataMap[key] = totalExtractedDataMap[key][p]\n",
    "        for key in totalExtractedDataMap:\n",
    "            data = totalExtractedDataMap[key]\n",
    "            extractedDataTrainMap[key],extractedDataTestMap[key] = np.split(data, [int(USE_ALL_DAYS_SPLIT*data.shape[0])])    \n",
    "            print(extractedDataTrainMap[key].shape,extractedDataTestMap[key].shape)\n",
    "\n",
    "        xDataTrain = extractedDataTrainMap['features']\n",
    "        xDataBlind = extractedDataTestMap['features'] \n",
    "\n",
    "        print('Generated unified randomized data')\n",
    "\n",
    "    # false, So, we will skip the ‘if’ block.\n",
    "    if USE_TRANSMITTANCE_ONLY:\n",
    "        xDataTrain = xDataTrainN\n",
    "        xDataBlind = xDataBlindN\n",
    "    elif USE_TRANSMITTANCE:\n",
    "        xDataTrain = np.concatenate((xDataTrain, xDataTrainN),axis=1)\n",
    "        xDataBlind = np.concatenate((xDataBlind, xDataBlindN),axis=1)\n",
    "    \n",
    "    # false, So, we will skip the ‘if’ block.\n",
    "    if USE_DERIVATIVE:\n",
    "        xDataTrain = features.getGradients(xDataTrain, DERIVATIVE_ORDER)\n",
    "        xDataBlind = features.getGradients(xDataBlind, DERIVATIVE_ORDER)\n",
    "    \n",
    "    # Standardize features by removing the mean and scaling to unit variance\n",
    "    # The idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1. \n",
    "    # Given the distribution of the data, each value in the dataset will have the sample mean value subtracted, and then divided by the standard deviation of the whole dataset.\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    if NORMALIZE:\n",
    "        scaler.fit(xDataTrain)\n",
    "        xDataTrain = scaler.transform(xDataTrain)\n",
    "        xDataBlind = scaler.transform(xDataBlind)\n",
    "\n",
    "    # false, So, we will skip the ‘if’ block.\n",
    "    if USE_EMSC:\n",
    "        xDataTrain = features.emsc(xDataTrain, EMSC_ORDER)\n",
    "        xDataBlind = features.emsc(xDataBlind, EMSC_ORDER)\n",
    "\n",
    "    TargetFeaturesList = ['Sugar', 'Alcohol']\n",
    "    NormalizedFeaturesList = []\n",
    "    # false, So, we will skip the ‘if’ block.\n",
    "    if NORMALIZE_OUTPUT:\n",
    "        for targetMetric in NormalizedFeaturesList:\n",
    "            yTrain = extractedDataTrainMap[targetMetric]\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(yTrain.reshape([-1,1]))\n",
    "        extractedDataTrainMap['normalized_'+targetMetric] = scaler.transform(yTrain.reshape([-1,1]))\n",
    "        extractedDataTestMap['normalized_'+targetMetric] = scaler.transform(extractedDataTestMap[targetMetric].reshape([-1,1]))\n",
    "\n",
    "    excelData = None\n",
    "    # We assign the tank numbers of test data to a 1D array.\n",
    "    if ADD_TANK_LABEL:\n",
    "        headerData = np.array((['Actual Tank']))\n",
    "        yTest = extractedDataTestMap['Tank']\n",
    "        excelDataSub = np.array((yTest)).reshape([-1,1])\n",
    "        excelDataSub = np.concatenate((headerData.reshape([1,-1]), excelDataSub.astype('str')), axis=0) \n",
    "        excelData = excelDataSub\n",
    "    \n",
    "    for targetMetric in TargetFeaturesList:\n",
    "        yTrain = None\n",
    "        yTest = None\n",
    "        # NORMALIZE_OUTPUT = false; So, We skip the 'if' block and 'if not' block\n",
    "        if NORMALIZE_OUTPUT and targetMetric in NormalizedFeaturesList:\n",
    "            yTrain = extractedDataTrainMap['normalized_'+targetMetric]\n",
    "            if not BLIND_TEST_MODE:\n",
    "                yTest = extractedDataTestMap['normalized_'+targetMetric]\n",
    "        else:\n",
    "            yTrain = extractedDataTrainMap[targetMetric]\n",
    "            if not BLIND_TEST_MODE:\n",
    "                yTest = extractedDataTestMap[targetMetric]\n",
    "                \n",
    "        X_combined = np.r_[xDataTrain, xDataBlind]\n",
    "        y_combined = np.r_[yTrain, yTest]\n",
    "        \n",
    "        data_name = 'data_' + targetMetric + '.npy'\n",
    "        \n",
    "        with open(data_name, 'wb') as f:\n",
    "            np.save(f, X_combined)\n",
    "            np.save(f, y_combined)\n",
    "            \n",
    "        print('Data sets have been merged and saved in current directory under ' + data_name)\n",
    "        print('To load the merged data sets, load one by one in order for each target metric: X_combined, y_combined in ' + data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the train/test split, generate the merged data \"X\" and the merged regression target \"y\", necessary to perform hyperparamter optimization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_file = 'train_test_merge.log'\n",
    "sys.stdout = open(output_file, 'w')\n",
    "print('Writing to output file:', output_file)\n",
    "\n",
    "train_test_merge(trainFilesMap, testFilesMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = originalStdOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After getting to this point and having obtained the necessary merged data, we proceed to do our hyperparameter optimization outside of this notebook. All you need to do is run the following command (in the current directory):**\n",
    "\n",
    "**for 2 params:**\n",
    "\n",
    "   **$ mpirun -n 4 python cheers_svr_2params.py --results /path/to/results_dir**\n",
    "   \n",
    "**for 3 params, you will possible need to submit the job to an HPC cluster. If you don't, then just do:**\n",
    "\n",
    "   **$ mpirun -n 8 python cheers_svr_3params.py --results /path/to/results_dir**\n",
    "   \n",
    "**Where n = 2^k, with k = # of hyperparameters and n = # of MPI processes/ranks required**\n",
    "\n",
    "**Lastly, we can come back to reinstantiate our model with our newly optimized hyperparameters.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
