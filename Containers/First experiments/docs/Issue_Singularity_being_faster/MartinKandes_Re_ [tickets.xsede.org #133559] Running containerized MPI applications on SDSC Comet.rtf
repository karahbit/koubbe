{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red69\green60\blue204;
\red53\green134\blue255;\red20\green160\blue194;\red29\green184\blue14;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0\cname textColor;\cssrgb\c0\c0\c0;\cssrgb\c34510\c33725\c83922;
\cssrgb\c25490\c61176\c100000;\cssrgb\c0\c68627\c80392;\cssrgb\c7059\c75294\c5490;}
\vieww20140\viewh12540\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs24 \cf2 From: George Koubbe <glk35@scarletmail.rutgers.edu>\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 19, 2020 at 7:01:56 PM EDT\
To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
Martin,\
\
Also, I have tried using the definition files for CentOS7 that you have on your Github profile, under both the \'93centos\'94 and \'93naked-singularity\'94 repositories, but none of them worked (they produce my old results). It would be helpful if you could share the definition file you used for the container creation as well as the command you used to build it.\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 On Apr 19, 2020, at 5:57 PM, George Koubbe <{\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}}> wrote:\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected. However, I also replicated mine, but this time with the container centos.simg that you provide, and my experiments also yield the same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled inside or outside the container\
2) The container I create is only through the command (I don\'92t do anything else): $\'a0singularity build centos7.sif {\field{\*\fldinst{HYPERLINK "docker://centos:centos7"}}{\fldrslt \cf5 \ul \ulc5 docker://centos:centos7}}\
\
Having said this, what is the difference between the container you build and the one I pull, that makes such a drastic difference in the results?\'a0\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT <{\field{\*\fldinst{HYPERLINK "mailto:help@xsede.org"}}{\fldrslt \cf5 \ul \ulc5 help@xsede.org}}> wrote:\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: {\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}}\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket URL: {\field{\*\fldinst{HYPERLINK "https://portal.xsede.org/group/xup/tickets/-/tickets/133559"}}{\fldrslt \cf5 \ul \ulc5 https://portal.xsede.org/group/xup/tickets/-/tickets/133559}} \
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and run your MPI hello, world code here [1]. \
\
As you can see in the standard output files from each job, there is really no significant difference in the performance when everything is setup correctly. i.e., I think the low runtimes you reported in the Singularity Slack channel were probably either incorrect and/or not an apples-to-apples comparison. \
\
Anyhow, have a look at how I've set things up and let me know if you have any questions. Note, however, the container being used is not the same one you pulled from DockerHub. This one is our standard CentOS 7 Singularity container. I needed a gcc compiler in the container and the DockerHub one does not have one installed by default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, {\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}} wrote:\
\pard\pardeftab720\partightenfactor0
\cf7 Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT <{\field{\*\fldinst{HYPERLINK "mailto:help@xsede.org"}}{\fldrslt \cf5 \ul \ulc5 help@xsede.org}}>\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Re: [{\field{\*\fldinst{HYPERLINK "http://tickets.xsede.org/"}}{\fldrslt \cf5 \ul \ulc5 tickets.xsede.org}} #133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: {\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}}\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: {\field{\*\fldinst{HYPERLINK "https://portal.xsede.org/group/xup/tickets/-"}}{\fldrslt \cf5 \ul \ulc5 https://portal.xsede.org/group/xup/tickets/-}}\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
don\'92t know if I\'92ll get back to it tonight, but definitely will work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<{\field{\*\fldinst{HYPERLINK "https://aka.ms/o0ukef"}}{\fldrslt \cf5 \ul \ulc5 https://aka.ms/o0ukef}}>\
________________________________\
From: Mahidhar Tatineni via RT <{\field{\*\fldinst{HYPERLINK "mailto:help@xsede.org"}}{\fldrslt \cf5 \ul \ulc5 help@xsede.org}}>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <{\field{\*\fldinst{HYPERLINK "mailto:mkandes@sdsc.edu"}}{\fldrslt \cf5 \ul \ulc5 mkandes@sdsc.edu}}>\
Subject: [{\field{\*\fldinst{HYPERLINK "http://tickets.xsede.org/"}}{\fldrslt \cf5 \ul \ulc5 tickets.xsede.org}} #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: {\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}}\
\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-"}}{\fldrslt \cf5 \ul \ulc5 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-}}\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\pard\pardeftab720\partightenfactor0
\cf6 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \
\pard\pardeftab720\partightenfactor0
\cf3 \
\
\page \pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 From: George Koubbe <glk35@scarletmail.rutgers.edu>\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 19, 2020 at 5:57:45 PM EDT\
To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected. However, I also replicated mine, but this time with the container centos.simg that you provide, and my experiments also yield the same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled inside or outside the container\
2) The container I create is only through the command (I don\'92t do anything else): $\'a0singularity build centos7.sif {\field{\*\fldinst{HYPERLINK "docker://centos:centos7"}}{\fldrslt \cf5 \ul \ulc5 docker://centos:centos7}}\
\
Having said this, what is the difference between the container you build and the one I pull, that makes such a drastic difference in the results?\'a0\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT <{\field{\*\fldinst{HYPERLINK "mailto:help@xsede.org"}}{\fldrslt \cf5 \ul \ulc5 help@xsede.org}}> wrote:\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: {\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}}\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket URL: {\field{\*\fldinst{HYPERLINK "https://portal.xsede.org/group/xup/tickets/-/tickets/133559"}}{\fldrslt \cf5 \ul \ulc5 https://portal.xsede.org/group/xup/tickets/-/tickets/133559}} \
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and run your MPI hello, world code here [1]. \
\
As you can see in the standard output files from each job, there is really no significant difference in the performance when everything is setup correctly. i.e., I think the low runtimes you reported in the Singularity Slack channel were probably either incorrect and/or not an apples-to-apples comparison. \
\
Anyhow, have a look at how I've set things up and let me know if you have any questions. Note, however, the container being used is not the same one you pulled from DockerHub. This one is our standard CentOS 7 Singularity container. I needed a gcc compiler in the container and the DockerHub one does not have one installed by default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, {\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}} wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf7 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT <{\field{\*\fldinst{HYPERLINK "mailto:help@xsede.org"}}{\fldrslt \cf5 \ul \ulc5 help@xsede.org}}>\
\pard\pardeftab720\partightenfactor0
\cf6 wrote:\
\pard\pardeftab720\partightenfactor0
\cf7 \
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Re: [{\field{\*\fldinst{HYPERLINK "http://tickets.xsede.org/"}}{\fldrslt \cf5 \ul \ulc5 tickets.xsede.org}} #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf6 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf7 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: {\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}}\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: {\field{\*\fldinst{HYPERLINK "https://portal.xsede.org/group/xup/tickets/-"}}{\fldrslt \cf5 \ul \ulc5 https://portal.xsede.org/group/xup/tickets/-}}\
\pard\pardeftab720\partightenfactor0
\cf6 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf7 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf6 don\'92t know if I\'92ll get back to it tonight, but definitely will work on\
it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf7 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<{\field{\*\fldinst{HYPERLINK "https://aka.ms/o0ukef"}}{\fldrslt \cf5 \ul \ulc5 https://aka.ms/o0ukef}}>\
________________________________\
From: Mahidhar Tatineni via RT <{\field{\*\fldinst{HYPERLINK "mailto:help@xsede.org"}}{\fldrslt \cf5 \ul \ulc5 help@xsede.org}}>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <{\field{\*\fldinst{HYPERLINK "mailto:mkandes@sdsc.edu"}}{\fldrslt \cf5 \ul \ulc5 mkandes@sdsc.edu}}>\
Subject: [{\field{\*\fldinst{HYPERLINK "http://tickets.xsede.org/"}}{\fldrslt \cf5 \ul \ulc5 tickets.xsede.org}} #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf6 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf7 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: {\field{\*\fldinst{HYPERLINK "mailto:glk35@scarletmail.rutgers.edu"}}{\fldrslt \cf5 \ul \ulc5 glk35@scarletmail.rutgers.edu}}\
\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-"}}{\fldrslt \cf5 \ul \ulc5 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-}}\cf6 \
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf7 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf6 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \
\
\page \pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 From: George Koubbe <glk35@scarletmail.rutgers.edu>\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 18, 2020 at 8:52:17 PM EDT\
To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
Martin,\
\
This is excellent news! I will look at it and try to replicate your setup...see where is it that I did something wrong. I will certainly let you know. \
\
Thank you so much,\
George. \
\
\pard\pardeftab720\partightenfactor0
\cf4 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT <help@xsede.org> wrote:\
\
\uc0\u65279 \
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-/tickets/133559 \
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and run your MPI hello, world code here [1]. \
\
As you can see in the standard output files from each job, there is really no significant difference in the performance when everything is setup correctly. i.e., I think the low runtimes you reported in the Singularity Slack channel were probably either incorrect and/or not an apples-to-apples comparison. \
\
Anyhow, have a look at how I've set things up and let me know if you have any questions. Note, however, the container being used is not the same one you pulled from DockerHub. This one is our standard CentOS 7 Singularity container. I needed a gcc compiler in the container and the DockerHub one does not have one installed by default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
\pard\pardeftab720\partightenfactor0
\cf6 On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf7 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT <help@xsede.org>\
\pard\pardeftab720\partightenfactor0
\cf6 wrote:\
\pard\pardeftab720\partightenfactor0
\cf7 \
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf6 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf7 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf6 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf7 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf6 don\'92t know if I\'92ll get back to it tonight, but definitely will work on\
it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf7 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<https://aka.ms/o0ukef>\
________________________________\
From: Mahidhar Tatineni via RT <help@xsede.org>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu>\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf6 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf7 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket <URL:\
\pard\pardeftab720\partightenfactor0
\cf6 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf7 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf6 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 From: "Martin Kandes via RT" <help@xsede.org>\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 17, 2020 at 8:15:20 PM EDT\
To: glk35@scarletmail.rutgers.edu\
Reply-To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-/tickets/133559 \
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I don\'92t know if I\'92ll get back to it tonight, but definitely will work on it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<https://aka.ms/o0ukef>\
________________________________\
From: Mahidhar Tatineni via RT <help@xsede.org>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu>\
Subject: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL: https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\page \pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 From: "Martin Kandes via RT" <help@xsede.org>\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 16, 2020 at 10:01:40 PM EDT\
To: glk35@scarletmail.rutgers.edu\
Reply-To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
\
Thu Apr 16 21:01:39 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mahidhar\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-/tickets/133559 \
\
\
P.S. I\'92m also curious what the use case is here. Can you let us know what you\'92re trying to accomplish? As I mentioned, we\'92ve thought of doing this before as well. But never found a strong reason to do so yet.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<https://aka.ms/o0ukef>\
________________________________\
From: Martin Kandes via RT <help@xsede.org>\
Sent: Thursday, April 16, 2020 6:57:46 PM\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
\
\
Thu Apr 16 20:57:45 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mahidhar\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL: https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1MFeXDzc$ \'a0>\
\
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow with the results.\
\
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack channel as someone else DM\'92d me and was curious about the issue too.\
\
Get Outlook for iOS<https://urldefense.com/v3/__https://aka.ms/o0ukef__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1IVSDe_Y$ >\
________________________________\
From: Ellen Buskuehl via RT <help@xsede.org>\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
\
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: buskuehl\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL: https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$ \'a0>\
\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug" partition of Comet, requesting 2 nodes and using the full 48 cores of said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64 $HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind /lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind /etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside the container is faster than outside, according only to the commands I typed of course. When I changed the MPI implementation to mvapich2, running the executable inside the container was slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would mean to me. Thank you!\
George Koubbe.\
\
\
\
\page \pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 From: "Martin Kandes via RT" <help@xsede.org>\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 16, 2020 at 9:57:46 PM EDT\
To: glk35@scarletmail.rutgers.edu\
Reply-To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
\
Thu Apr 16 20:57:45 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mahidhar\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-/tickets/133559 \
\
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow with the results.\
\
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack channel as someone else DM\'92d me and was curious about the issue too.\
\
Get Outlook for iOS<https://aka.ms/o0ukef>\
________________________________\
From: Ellen Buskuehl via RT <help@xsede.org>\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
\
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: buskuehl\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL: https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$ \'a0>\
\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug" partition of Comet, requesting 2 nodes and using the full 48 cores of said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64 $HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind /lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind /etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside the container is faster than outside, according only to the commands I typed of course. When I changed the MPI implementation to mvapich2, running the executable inside the container was slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would mean to me. Thank you!\
George Koubbe.\
\
\
}