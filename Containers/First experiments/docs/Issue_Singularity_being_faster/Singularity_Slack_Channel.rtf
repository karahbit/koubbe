{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red53\green134\blue255;}
{\*\expandedcolortbl;;\cssrgb\c25490\c61176\c100000;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf0 \expnd0\expndtw0\kerning0
Hello,\
\
I am running a containerized MPI Hello World application on SDSC Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug" partition of Comet, requesting 2 nodes and using the full 48 cores of said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI \
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif {\field{\*\fldinst{HYPERLINK "docker://centos:centos7"}}{\fldrslt \cf2 \ul \ulc2 docker://centos:centos7}}\
$ wget {\field{\*\fldinst{HYPERLINK "https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c"}}{\fldrslt \cf2 \ul \ulc2 https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c}}\
$ export PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64 $HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif {\field{\*\fldinst{HYPERLINK "docker://centos:centos7"}}{\fldrslt \cf2 \ul \ulc2 docker://centos:centos7}}\
$ wget {\field{\*\fldinst{HYPERLINK "https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c"}}{\fldrslt \cf2 \ul \ulc2 https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c}}\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind /lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind /etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside the container is faster than outside, according only to the commands I typed of course. When I changed the MPI implementation to mvapich2, running the executable inside the container was slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would mean to me. Thank you!\
George Koubbe.\kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0
\cf0 \
There is a library dependency on the MPI implementation (faster on Intel), on Comet at least. I got in touch with SDSC developer and he also said it was interesting since they have not tried Intel compilers and MPI libraries, and that he was going to take a look into it. His exact words on 4/14/2020:\
mkandes\'a0\
@George Koubbe: That's kind of odd. I've not experimented with bind mounting our Intel compilers and/or Intel MPI into a container before. I've thought about it, but never really had to do it. Do you have infiniband drivers and everything for high-speed networking in your container too? Did you compile the executable both inside and outside the container for each test? IntelMPI on Comet is served over NFS ... it's not node local. So there's definitely a network between you and your MPI. But it's not clear to me that you're actually running Intel MPI inside the container, only outside.\
\
One of the Singularity developers said:\
Dave Trudgian (Sylabs)\'a0\
sometimes a container can be faster because the SIF is a single file, and I/O on that single file is more efficient on HPC parallel filesystems than natively accessing lots of files. There is less metadata overhead for the IO with the containerized version.\
\
\
Another person said:\
George Koubbe  1 day ago\
Update: There is something going on behind the scenes with my job scheduler that is messing my paths up. After adding the following to the beginning of my python script, numpy was detected\
import sys\
sys.path.insert(0, '/usr/local/lib64/python3.6/site-packages')\
(edited)\
\
Pontus Freyhult  15 hours ago\
As you seem to be onto, the environment can play tricks on you.\
For troubleshooting, it's often useful to report what env outputs. When trying to work out why things are broken, -e/--cleanenv may also be useful.\
When writing containers, it's often a good idea to make sure to manage the variables that affects your tools. While it may seem unneeded if your only building a container for yourself, I'd still argue that it's useful down the line (so things does not start acting weird just because you've loaded a module you have not normally or similar).\
[While I try to encourage those I interact with and help with containers, I must also admit that it does not tend to be a huge problem, more rare one-off issues like this.]\
\
George Koubbe  12 minutes ago\
This is great advice and much appreciated! If you don\'92t mind me asking, what does --cleanenv do? (edited) \
\
As in https://sylabs.io/guides/3.5/user-guide/environment_and_metadata.html\
\
The --cleanenv option can be used to remove the host environment and execute a container with a minimal environment. Without the --cleanenv flag, the environment on the host system will be present within the container at run time.\
\
If you need to change the $PATH of your container at run time there are a few special environmental variables you can use:\
\
SINGULARITYENV_PREPEND_PATH=/good/stuff/at/beginning to prepend directories to the beginning of the $PATH\
SINGULARITYENV_APPEND_PATH=/good/stuff/at/end to append directories to the end of the $PATH\
SINGULARITYENV_PATH=/a/new/path to override the $PATH within the container}