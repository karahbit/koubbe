{\rtf1\ansi\ansicpg1252\cocoartf2512
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red69\green60\blue204;
\red20\green160\blue194;\red29\green184\blue14;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0\cname textColor;\cssrgb\c0\c0\c0;\cssrgb\c34510\c33725\c83922;
\cssrgb\c0\c68627\c80392;\cssrgb\c7059\c75294\c5490;}
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs24 \cf2 From: "Martin Kandes via RT" <help@xsede.org>\
Subject: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 22, 2020 at 1:24:10 PM EDT\
To: glk35@scarletmail.rutgers.edu\
Reply-To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 \
Wed Apr 22 12:24:09 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-/tickets/133559 \
\
\
George,\
\
No problem. Yeah, I'm not sure what else to try. The only thing I can think of is maybe there is/was a bug in Singularity 3.5.0 that has something about the bind mounts not working properly and preventing the container from using/seeing the infiniband drivers and libraries on the underlying host system. That's really my only hypothesis right now. But again, we usually just install compatible drivers and libraries inside the container.\
\
Anyhow, if I sort this out eventually, I'll let you know. But I'm not sure we'll sort it out anytime soon.\
\
Marty Kandes\
SDSC User Services Group\
\
On Wed Apr 22 12:03:12 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Yeah, well, I have no clue what is going on then. It is what it is and\
\'a0\'a0thank you so much for your support throughout this whole issue\
\'a0\'a0anyway!\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 22, 2020, at 12:36 PM, Martin Kandes via RT <help@xsede.org>\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Wed Apr 22 11:36:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
George,\
\
I build the containers using the build command: sudo singularity\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0build containter.simg container.deffile\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 18:40:57 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
This issue is very interesting indeed. It was nice of you to even\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0go\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0further and perform bandwidth and latency tests, which confirms\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0considerable performance difference between our containers,\
\'a0although we have not been able to locate the source of the\
\'a0difference so far. I do build with Singularity 3.5.0 installed\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0from\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0source. What is the exact command you use for building?\
\
And you are right regarding your intuition of what we want to do.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0We\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0are still exploring the options of how we are going to distribute\
\'a0the containers to end users depending on the application they try\
\'a0to run. Of course, as you point out, the bindings would need to\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0be\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0specific to the remote resource.\
\
Best,\
George.\
\
On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT <help@xsede.org>\
\'a0wrote:\
\
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559\
\
\
George,\
\
A side note: I finally had a look over the project you're working\
\'a0on. It looks pretty interesting. So is the idea here that you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0might\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0--bind mount the local compilers/mpi on a resource into users\
\'a0containers when the container runs on that resource? If so, I'm\
\'a0still thinking this would be more complicated to support than\
\'a0simply having a set of static containers to choose from that\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0would\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0be compatible the compiler/mpi you find/discover at a resource.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0But\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0maybe I'm missing something here.\
\
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\'a0there\
is some sort of performance difference between the 3 containers\
\'a0I'm\
testing with here, one of them is your version of the container.\
But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0test\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 problem with your container [1]. Your numbers are the lower\
runtimes in the middle. But as you can see, they do fluctuate to\
the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0ran\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the\
OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\'a0you\
can see here, the results are more consistent, with your\
\'a0container\
underperforming. The fundamental problem seems to be that your\
container does not properly utilize Comet's infiniband network.\
This can be seen in the high latency [4]. The latency should be\
\'a0on\
the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\'a0difference\
I see right now is that your container was built with\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Singularity\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 3.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2?\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Also,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0how\
did you install your copy of Singularity 3.5.0? Did you compile\
from source? Or is that a binary executable that you downloaded\
\'a0and\
installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity-\
omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity-\
omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\'a0<help@xsede.org>\
wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\'a0formatted\
container. Can you provide we with a path to your working\
directory\
on Comet where I can review the output files for your tests?\
There\
might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 was\
the one built using the latest version from the 'naked-\
singularity'\
repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0'centos'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 one\
is a slightly updated version, but should not be significantly\
different in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones\
OS.\
As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0yes,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it\
shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0run\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 a\
code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\'a0our\
CentOS 7 container that might been causing a problem. What\
version\
of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0built\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\'a0wrote:\
Martin,\
\
Also, I have tried using the definition files for CentOS7\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0that\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
have on your Github profile, under both the \'93centos\'94 and\
\'93naked-\
singularity\'94 repositories, but none of them worked (they\
produce\
my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
command\
you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
<glk35@scarletmail.rutgers.edu> wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0expected.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 However, I also replicated mine, but this time with the\
container\
centos.simg that you provide, and my experiments also yield\
the\
same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0compiled\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 inside\
or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0don\'92t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7 <docker://centos:centos7>\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
<help@xsede.org\
<mailto:help@xsede.org>> wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
<mailto:glk35@scarletmail.rutgers.edu>\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 <https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
compile\
and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
there\
is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
reported\
in\
the Singularity Slack channel were probably either incorrect\
and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0know\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 if\
you have any questions. Note, however, the container being\
used\
is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
compiler\
in\
the container and the DockerHub one does not have one\
installed\
by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
<mailto:glk35@scarletmail.rutgers.edu> wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
<help@xsede.org\
<mailto:help@xsede.org>>\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
<http://tickets.xsede.org/>\
#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
<mailto:glk35@scarletmail.rutgers.edu>\
Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
<https://portal.xsede.org/group/xup/tickets/->\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
out.\
I\
don\'92t know if I\'92ll get back to it tonight, but definitely\
will\
work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<https://aka.ms/o0ukef\
<https://aka.ms/o0ukef>>\
________________________________\
From: Mahidhar Tatineni via RT <help@xsede.org\
<mailto:help@xsede.org>>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu\
<mailto:mkandes@sdsc.edu>>\
Subject: [tickets.xsede.org <http://tickets.xsede.org/>\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
<mailto:glk35@scarletmail.rutgers.edu>\
Status: open\
Ticket <URL:\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
\page \pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 From: "Martin Kandes via RT" <help@xsede.org>\
Subject: [tickets.xsede.org #133559] Resolved: Running containerized MPI applications on SDSC Comet\
Date: April 22, 2020 at 1:24:12 PM EDT\
To: glk35@scarletmail.rutgers.edu\
Reply-To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 According to our records, your request has been resolved. If you have any further questions or concerns, please respond to this message.\
\
\
----------------------------------------------------------------\
\'a0Your initial request was\
----------------------------------------------------------------\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug" partition of Comet, requesting 2 nodes and using the full 48 cores of said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI \
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c\
$ export PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64 $HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind /lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind /etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside the container is faster than outside, according only to the commands I typed of course. When I changed the MPI implementation to mvapich2, running the executable inside the container was slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would mean to me. Thank you!\
George Koubbe.\
\
\
\
----------------------------------------------------------------\
\'a0\'a0Complete Ticket History\
----------------------------------------------------------------\
\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Thu Apr 16 20:57:45 2020\
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow\
with the results.\
\
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack\
channel as someone else DM\'92d me and was curious about the issue too.\
\
Get Outlook for iOS\
________________________________\
From: Ellen Buskuehl via RT\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: buskuehl\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug"\
partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for\
both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind\
/lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside\
the container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was slightly slower than\
outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
mean to me. Thank you!\
George Koubbe.\
\
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Thu Apr 16 20:57:45 2020\
\
\
\
<meta http-equiv="Content-Type" content="text/html; charset=Windows-\
1252">\
\
\
\
\
<div style="direction: ltr;">Hi George,\
\
\
<div style="direction: ltr;">I\'92ve almost solved this issue already.\
I\'92ll get back to you tomorrow with the results.\
\
\
<div style="direction: ltr;">Marty Kandes\
<div style="direction: ltr;">SDSC User Services Group\
\
\
<div style="direction: ltr;">P.S. I\'92ll provide you solution here and\
on the Singularly Slack channel as someone else DM\'92d me and was\
curious about the issue too.\
\
\
\
<div class="ms-outlook-ios-signature">Get <a\
href="https://aka.ms/o0ukef">Outlook for iOS\
\
<hr style="display:inline-block;width:98%" tabindex="-1">\
<div id="divRplyFwdMsg" dir="ltr"><font face="Calibri, sans-serif"\
style="font-size:11pt" color="#000000">From: Ellen Buskuehl via RT\
&lt;help@xsede.org&gt;\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
&nbsp;\
\
<div class="BodyFragment"><font size="2"><span style="font-\
size:11pt;">\
<div class="PlainText">\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
&nbsp;Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Queue: 0-SDSC\
&nbsp;&nbsp;&nbsp;&nbsp; Subject: Running containerized MPI\
applications on SDSC Comet\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Owner: buskuehl\
&nbsp; Requestors: glk35@scarletmail.rutgers.edu\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status: open\
&nbsp;Ticket &lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$">\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$&nbsp;\
&gt;\
\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug&quot;\
partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for\
both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$">\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
\
$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$">\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind\
/lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside\
the container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was\
slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
mean to me. Thank you!\
George Koubbe.\
\
&nbsp;\
\
\
\
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Thu Apr 16 21:01:39 2020\
\
P.S. I\'92m also curious what the use case is here. Can you let us know\
what you\'92re trying to accomplish? As I mentioned, we\'92ve thought of\
doing this before as well. But never found a strong reason to do so\
yet.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Martin Kandes via RT\
Sent: Thursday, April 16, 2020 6:57:46 PM\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Thu Apr 16 20:57:45 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mahidhar\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1MFeXDzc$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow\
with the results.\
\
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack\
channel as someone else DM\'92d me and was curious about the issue too.\
\
Get Outlook for\
iOS<https://urldefense.com/v3/__https://aka.ms/o0ukef__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1IVSDe_Y$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 ________________________________\
From: Ellen Buskuehl via RT\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: buskuehl\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug"\
partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for\
both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind\
/lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside\
the container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was slightly slower than\
outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
mean to me. Thank you!\
George Koubbe.\
\
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Thu Apr 16 21:01:39 2020\
\
\
\
<meta http-equiv="Content-Type" content="text/html; charset=Windows-\
1252">\
\
\
\
\
<div style="direction: ltr;">P.S. I\'92m also curious what the use case\
is here. Can you let us know what you\'92re trying to accomplish? As I\
mentioned, we\'92ve thought of doing this before as well. But never found\
a strong reason to do so yet.\
\
\
<div style="direction: ltr;">Marty Kandes\
<div style="direction: ltr;">SDSC User Services Group\
\
\
\
<div class="ms-outlook-ios-signature">Get <a\
href="https://aka.ms/o0ukef">Outlook for iOS\
\
<hr style="display:inline-block;width:98%" tabindex="-1">\
<div id="divRplyFwdMsg" dir="ltr"><font face="Calibri, sans-serif"\
style="font-size:11pt" color="#000000">From: Martin Kandes via RT\
&lt;help@xsede.org&gt;\
Sent: Thursday, April 16, 2020 6:57:46 PM\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
&nbsp;\
\
<div class="BodyFragment"><font size="2"><span style="font-\
size:11pt;">\
<div class="PlainText">\
Thu Apr 16 20:57:45 2020: Request 133559 was acted upon.\
&nbsp;Transaction: Correspondence added by mckandes\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Queue: 0-SDSC\
&nbsp;&nbsp;&nbsp;&nbsp; Subject: Re: [tickets.xsede.org #133559]\
Running containerized MPI applications on SDSC Comet\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Owner: mahidhar\
&nbsp; Requestors: glk35@scarletmail.rutgers.edu\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status: open\
&nbsp;Ticket &lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1MFeXDzc$">\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1MFeXDzc$&nbsp;\
&gt;\
\
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow\
with the results.\
\
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack\
channel as someone else DM\'92d me and was curious about the issue too.\
\
Get Outlook for iOS&lt;<a\
href="">https://urldefense.com/v3/__https://aka.ms/o0ukef__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1IVSDe_Y$\
&gt;\
________________________________\
From: Ellen Buskuehl via RT &lt;help@xsede.org&gt;\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
&nbsp;Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Queue: 0-SDSC\
&nbsp;&nbsp;&nbsp;&nbsp; Subject: Running containerized MPI\
applications on SDSC Comet\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Owner: buskuehl\
&nbsp; Requestors: glk35@scarletmail.rutgers.edu\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status: open\
&nbsp;Ticket &lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$">\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$&nbsp;\
&gt;\
\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug&quot;\
partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for\
both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$">\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$">\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind\
/lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside\
the container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was\
slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
mean to me. Thank you!\
George Koubbe.\
\
\
\
\
\
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Thu Apr 16 21:14:36 2020\
\
This is great news Martin!\
\
As a software engineer myself, I know the struggle when someone has an\
issue and asks individually instead of officially opening a ticket.\
That \'a0is why I opened the ticket here as well.\
\
Regarding the use case:\
\
I work at RADICAL-Lab at Rutgers University - New Brunswick. We have\
developed a set of software tools (https://radical-\
cybertools.github.io ) that are architected for scalable,\
interoperable and sustainable approaches to support science on a range\
of high-performance and distributed computing systems.\
\
As we move forward and embrace new technologies, I intend to do with\
the present work a performance characterization of the execution of\
containerized scientific applications on HPC platforms using RADICAL-\
Cybertools (RCT). This will allow us to make decisions based on best\
software engineering practices and integrate them into RCT so we can\
continue to support our users in terms of all the benefits containers\
can bring.\
\
Thank you so much for looking into this in such a quick and efficient\
fashion,\
George Koubbe.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 16, 2020, at 9:57 PM, Martin Kandes via RT \'a0wrote:\
\
\
Thu Apr 16 20:57:45 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'a0\'a0\'a0Owner: mahidhar\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 with the results.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 channel as someone else DM\'92d me and was curious about the issue too.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Get Outlook for iOS\
________________________________\
From: Ellen Buskuehl via RT\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: buskuehl\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
1. I just tried running an mpirun command directly on the \'93debug"\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 2. I made sure the MPI implementation (Intel MPI 18.1) is the same\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 for both containerized/non containerized \'93hello world\'94 executable.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 00:30:00 --wait=0 --export=ALL /bin/bash\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 00:30:00 --wait=0 --export=ALL /bin/bash\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 --bind /lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 inside the container is faster than outside, according only to the\
commands I typed of course. When I changed the MPI implementation to\
mvapich2, running the executable inside the container was slightly\
slower than outside (this is what I originally expected).\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 mean to me. Thank you!\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 George Koubbe.\
\
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Thu Apr 16 21:14:36 2020\
\
<meta http-equiv="Content-Type" content="text/html; charset=utf-\
8"><body style="word-wrap: break-word; -webkit-nbsp-mode: space; line-\
break: after-white-space;" class="">This is great news Martin!<div\
class=""><br class=""><div class="">As a software engineer myself, I\
know the struggle when someone has an issue and asks individually\
instead of officially opening a ticket. That &nbsp;is why I opened the\
ticket here as well.<div class=""><br class=""><div class="">Regarding\
the use case:<div class=""><br class=""><div class="">I work at\
RADICAL-Lab at Rutgers University - New Brunswick. We have developed a\
set of software tools (<a href="https://radical-cybertools.github.io"\
class="">https://radical-cybertools.github.io) that are architected\
for scalable, interoperable and sustainable approaches to support\
science on a range of high-performance and distributed computing\
systems.<div class=""><br class=""><div class="">As we move forward\
and embrace new technologies, I intend to do with the present work a\
performance characterization of the execution of containerized\
scientific applications on HPC platforms using RADICAL-Cybertools\
(RCT). This will allow us to make decisions based on best software\
engineering practices and integrate them into RCT so we can continue\
to support our users in terms of all the benefits containers can\
bring.<div class=""><br class=""><div class="">Thank you so much for\
looking into this in such a quick and efficient fashion,<div\
class="">George Koubbe.<br class=""><br class=""><blockquote\
type="cite" class=""><div class="">On Apr 16, 2020, at 9:57 PM, Martin\
Kandes via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org&gt; wrote:<br class="Apple-interchange-\
newline"><div class=""><div class=""><br class="">Thu Apr 16 20:57:45\
2020: Request 133559 was acted upon.<br class=""> Transaction:\
Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Subject: Re: [<a\
href="http://tickets.xsede.org" class="">tickets.xsede.org #133559]\
Running containerized MPI applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mahidhar<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class=""> Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-/tickets/133559"\
class="">https://portal.xsede.org/group/xup/tickets/-/tickets/133559\
<br class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">I\'92ve almost solved this issue already. I\'92ll get back to you\
tomorrow with the results.<br class=""><br class="">Marty Kandes<br\
class="">SDSC User Services Group<br class=""><br class="">P.S. I\'92ll\
provide you solution here and on the Singularly Slack channel as\
someone else DM\'92d me and was curious about the issue too.<br\
class=""><br class="">Get Outlook for iOS&lt;<a\
href="https://aka.ms/o0ukef" class="">https://aka.ms/o0ukef&gt;<br\
class="">________________________________<br class="">From: Ellen\
Buskuehl via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org&gt;<br class="">Sent: Thursday, April 16, 2020\
6:39:27 PM<br class="">Subject: [<a href="http://tickets.xsede.org"\
class="">tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet<br class=""><br class=""><br class="">Thu\
Apr 16 20:39:26 2020: Request 133559 was acted upon.<br class="">\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl<br\
class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br\
class=""> &nbsp;&nbsp;&nbsp;&nbsp;Subject: Running containerized MPI\
applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: buskuehl<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class=""> Ticket\
&lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$\
&nbsp;&gt;<br class=""><br class=""><br class="">Hello,<br\
class=""><br class="">I am running a containerized MPI Hello World\
application on SDSC Comet.<br class=""><br class="">1. I just tried\
running an mpirun command directly on the \'93debug" partition of Comet,\
requesting 2 nodes and using the full 48 cores of said nodes.<br\
class="">2. I made sure the MPI implementation (Intel MPI 18.1) is the\
same for both containerized/non containerized \'93hello world\'94\
executable.<br class=""><br class=""><br class="">This is exactly what\
I do since I login to comet:<br class=""><br class="">For Intel MPI<br\
class=""><br class="">On login node:<br class="">$ module purge<br\
class="">$ module load intel singularity<br class="">$ singularity\
build centos-mpi.sif <a href="docker://centos:centos7"\
class="">docker://centos:centos7<br class="">$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$"\
class="">https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$<br\
class="">$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH<br\
class="">$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH<br\
class="">$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/<br\
class="">$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib<br\
class="">$ srun --partition=compute --pty --nodes=2 --ntasks-per-\
node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash<br class=""><br\
class="">On compute node:<br class="">$ module purge<br class="">$\
module load intel singularity<br class="">$ mpicc mpi_hello_world.c -o\
hello_world_intel<br class="">$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel<br class=""><br\
class=""><br class=""><br class="">For mvapich2:<br class=""><br\
class="">On login node:<br class="">$ module load singularity<br\
class="">$ singularity build centos-mpi.sif <a\
href="docker://centos:centos7" class="">docker://centos:centos7<br\
class="">$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$"\
class="">https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$<br\
class="">$ export\
SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/<br class="">$\
export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d<br\
class="">$ srun --partition=compute --pty --nodes=2 --ntasks-per-\
node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash<br class=""><br\
class="">On compute node:<br class="">$ mpicc mpi_hello_world.c -o\
hello_world_mpich<br class="">$ mpirun -n 48 singularity exec --bind\
/opt/mvapich2/intel/ib/ --bind /lib64 --bind\
/opt/intel/2018.1.163/lib/intel64 --bind /etc/libibverbs.d\
$HOME/centos-openmpi.sif $HOME/hello_world_mpich<br class=""><br\
class=""><br class=""><br class="">The Question:<br class="">I can\'92t\
find an explanation on why running \'93hello_world_intel\'94 inside the\
container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was slightly slower than\
outside (this is what I originally expected).<br class=""><br\
class=""><br class=""><br class="">The command I use to run outside\
the container is just:<br class="">$ mpirun -n 48\
$HOME/hello_world_intel<br class=""><br class=""><br class="">The\
command used to measure elapsed time was:<br class="">$ /usr/bin/time\
-v mpirun ... \'85 \'85<br class=""><br class=""><br class="">Just to give\
you exact numbers, when running with intel I got:<br class="">non-\
containerized: ~3.36 s<br class="">containerized: ~0.48 s<br\
class=""><br class="">When running with mvapich2, I got:<br\
class="">non-containerized: ~3.37 s<br class="">containerized: ~4.01\
s<br class=""><br class=""><br class=""><br class=""><br class="">Your\
help would be greatly appreciated, you don\'92t know what it would mean\
to me. Thank you!<br class="">George Koubbe.<br class=""><br\
class=""><br class=""><br class="">\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Fri Apr 17 19:15:18 2020\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I don\'92t\
know if I\'92ll get back to it tonight, but definitely will work on it\
again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Fri Apr 17 19:15:18 2020\
\
\
\
<meta http-equiv="Content-Type" content="text/html; charset=Windows-\
1252">\
\
\
\
\
<div style="direction: ltr;">Hi George,\
\
\
<div style="direction: ltr;">There\'92s still at least one more issue I\'92m\
trying to sort out. I don\'92t know if I\'92ll get back to it tonight, but\
definitely will work on it again tomorrow and keep you posted.\
\
\
<div style="direction: ltr;">Marty Kandes\
<div style="direction: ltr;">SDSC User Services Group\
\
\
\
<div class="ms-outlook-ios-signature">Get <a\
href="https://aka.ms/o0ukef">Outlook for iOS\
\
<hr style="display:inline-block;width:98%" tabindex="-1">\
<div id="divRplyFwdMsg" dir="ltr"><font face="Calibri, sans-serif"\
style="font-size:11pt" color="#000000">From: Mahidhar Tatineni via RT\
&lt;help@xsede.org&gt;\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin &lt;mkandes@sdsc.edu&gt;\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
&nbsp;\
\
<div class="BodyFragment"><font size="2"><span style="font-\
size:11pt;">\
<div class="PlainText">\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
&nbsp;Transaction: Given to mckandes by mahidhar\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Queue: 0-SDSC\
&nbsp;&nbsp;&nbsp;&nbsp; Subject: Running containerized MPI\
applications on SDSC Comet\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Owner: mckandes\
&nbsp; Requestors: glk35@scarletmail.rutgers.edu\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status: open\
&nbsp;Ticket &lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$">\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$&nbsp; &gt;\
\
This transaction appears to have no content\
\
\
\
\
\
------------------------------------------------\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Fri Apr 17 19:30:30 2020\
\
Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT \'a0wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 don\'92t know if I\'92ll get back to it tonight, but definitely will work on\
it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Sat Apr 18 17:20:07 2020\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job, there is\
really no significant difference in the performance when everything is\
setup correctly. i.e., I think the low runtimes you reported in the\
Singularity Slack channel were probably either incorrect and/or not an\
apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know if you\
have any questions. Note, however, the container being used is not the\
same one you pulled from DockerHub. This one is our standard CentOS 7\
Singularity container. I needed a gcc compiler in the container and\
the DockerHub one does not have one installed by default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 don\'92t know if I\'92ll get back to it tonight, but definitely will work\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sat Apr 18 19:52:23 2020\
\
Martin,\
\
This is excellent news! I will look at it and try to replicate your\
setup...see where is it that I did something wrong. I will certainly\
let you know.\
\
Thank you so much,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT \'a0wrote:\
\
\uc0\u65279 \
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
As you can see in the standard output files from each job, there is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 really no significant difference in the performance when everything is\
setup correctly. i.e., I think the low runtimes you reported in the\
Singularity Slack channel were probably either incorrect and/or not an\
apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Anyhow, have a look at how I've set things up and let me know if you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 have any questions. Note, however, the container being used is not the\
same one you pulled from DockerHub. This one is our standard CentOS 7\
Singularity container. I needed a gcc compiler in the container and\
the DockerHub one does not have one installed by default.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 don\'92t know if I\'92ll get back to it tonight, but definitely will work\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket >\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sun Apr 19 16:57:50 2020\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected. However,\
I also replicated mine, but this time with the container centos.simg\
that you provide, and my experiments also yield the same results as\
yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled inside\
or outside the container\
2) The container I create is only through the command (I don\'92t do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the container you\
build and the one I pull, that makes such a drastic difference in the\
results?\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT \'a0wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
As you can see in the standard output files from each job, there is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 really no significant difference in the performance when everything is\
setup correctly. i.e., I think the low runtimes you reported in the\
Singularity Slack channel were probably either incorrect and/or not an\
apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Anyhow, have a look at how I've set things up and let me know if you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 have any questions. Note, however, the container being used is not the\
same one you pulled from DockerHub. This one is our standard CentOS 7\
Singularity container. I needed a gcc compiler in the container and\
the DockerHub one does not have one installed by default.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 don\'92t know if I\'92ll get back to it tonight, but definitely will work\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket >\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sun Apr 19 16:57:50 2020\
\
<meta http-equiv="Content-Type" content="text/html; charset=utf-\
8"><body style="word-wrap: break-word; -webkit-nbsp-mode: space; line-\
break: after-white-space;" class="">Dear Martin,<div class=""><br\
class=""><div class="">I have narrowed down my problem to the\
container itself.<br class=""><div class=""><br class=""><div\
class="">I just replicated your experiments and it worked as expected.\
However, I also replicated mine, but this time with the container\
centos.simg that you provide, and my experiments also yield the same\
results as yours.<div class=""><br class=""><div class="">It\'92s worth\
noting that:<div class=""><br class=""><div class="">1) It did not\
make a difference if the executable is compiled inside or outside the\
container<div class="">2) The container I create is only through the\
command (I don\'92t do anything else): $&nbsp;singularity build\
centos7.sif <a href="docker://centos:centos7"\
class="">docker://centos:centos7<div class=""><br class=""><div\
class="">Having said this, what is the difference between the\
container you build and the one I pull, that makes such a drastic\
difference in the results?&nbsp;<div class=""><br class=""><div\
class="">Thank you,<div class="">George.<div class=""><br\
class=""><blockquote type="cite" class=""><div class="">On Apr 18,\
2020, at 6:20 PM, Martin Kandes via RT &lt;<a\
href="mailto:help@xsede.org" class="">help@xsede.org&gt; wrote:<br\
class="Apple-interchange-newline"><div class=""><div class=""><br\
class="">Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.<br\
class=""> Transaction: Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Subject: Running containerized MPI\
applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class=""> Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-/tickets/133559"\
class="">https://portal.xsede.org/group/xup/tickets/-/tickets/133559\
<br class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">I've placed a copy of my example batch job scripts that\
compile and run your MPI hello, world code here [1]. <br class=""><br\
class="">As you can see in the standard output files from each job,\
there is really no significant difference in the performance when\
everything is setup correctly. i.e., I think the low runtimes you\
reported in the Singularity Slack channel were probably either\
incorrect and/or not an apples-to-apples comparison. <br class=""><br\
class="">Anyhow, have a look at how I've set things up and let me know\
if you have any questions. Note, however, the container being used is\
not the same one you pulled from DockerHub. This one is our standard\
CentOS 7 Singularity container. I needed a gcc compiler in the\
container and the DockerHub one does not have one installed by\
default.<br class=""><br class="">Marty Kandes<br class="">SDSC User\
Services Group<br class=""><br class="">[1]<br class=""><br\
class="">/share/apps/examples/singularity/bind-host-compilers-mpi<br\
class=""><br class="">On Fri Apr 17 19:30:30 2020, <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu wrote:<br class=""><blockquote\
type="cite" class="">Thank you Martin, I appreciate your support.<br\
class=""><br class="">George.<br class=""><br class=""><blockquote\
type="cite" class="">On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
&lt;<a href="mailto:help@xsede.org" class="">help@xsede.org&gt;<br\
class="">wrote:<br class=""><blockquote type="cite" class=""><br\
class=""><br class="">Fri Apr 17 19:15:18 2020: Request 133559 was\
acted upon.<br class="">Transaction: Correspondence added by\
mckandes<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br\
class=""> &nbsp;&nbsp;&nbsp;Subject: Re: [<a\
href="http://tickets.xsede.org" class="">tickets.xsede.org #133559]\
Running containerized<br class="">MPI applications on SDSC Comet<br\
class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class=""> Requestors:\
<a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br\
class="">/tickets/133559<br class=""><blockquote type="cite"\
class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">There\'92s still at least one more issue I\'92m trying to sort out.\
I<br class="">don\'92t know if I\'92ll get back to it tonight, but\
definitely will work on<br class="">it again tomorrow and keep you\
posted.<br class=""><blockquote type="cite" class=""><br\
class="">Marty Kandes<br class="">SDSC User Services Group<br\
class=""><br class="">Get Outlook for iOS&lt;<a\
href="https://aka.ms/o0ukef" class="">https://aka.ms/o0ukef&gt;<br\
class="">________________________________<br class="">From: Mahidhar\
Tatineni via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org&gt;<br class="">Sent: Thursday, April 16, 2020\
10:19:20 PM<br class="">To: Kandes, Martin &lt;<a\
href="mailto:mkandes@sdsc.edu" class="">mkandes@sdsc.edu&gt;<br\
class="">Subject: [<a href="http://tickets.xsede.org"\
class="">tickets.xsede.org #133559] Running containerized MPI<br\
class="">applications on SDSC Comet<br class=""><blockquote\
type="cite" class=""><br class=""><br class="">Fri Apr 17 00:19:20\
2020: Request 133559 was acted upon.<br class="">Transaction: Given to\
mckandes by mahidhar<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue:\
0-SDSC<br class=""> &nbsp;&nbsp;&nbsp;Subject: Running containerized\
MPI applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class=""> Requestors:\
<a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket &lt;URL:<br\
class=""><a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
<br class="">2nOMdZ3ikPm94px2MBcFY$ &nbsp;&gt;<br class=""><blockquote\
type="cite" class=""><br class="">This transaction appears to have no\
content<br class=""><br class=""><br class=""><br class=""><br\
class=""><br class="">\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sun Apr 19 18:02:01 2020\
\
Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
have on your Github profile, under both the \'93centos\'94 and \'93naked-\
singularity\'94 repositories, but none of them worked (they produce my\
old results). It would be helpful if you could share the definition\
file you used for the container creation as well as the command you\
used to build it.\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 19, 2020, at 5:57 PM, George Koubbe \'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 However, I also replicated mine, but this time with the container\
centos.simg that you provide, and my experiments also yield the same\
results as yours.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled inside\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 or outside the container\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 2) The container I create is only through the command (I don\'92t do\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Having said this, what is the difference between the container you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 build and the one I pull, that makes such a drastic difference in the\
results?\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT <help@xsede.org >\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
As you can see in the standard output files from each job, there is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 really no significant difference in the performance when everything is\
setup correctly. i.e., I think the low runtimes you reported in the\
Singularity Slack channel were probably either incorrect and/or not an\
apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Anyhow, have a look at how I've set things up and let me know if\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you have any questions. Note, however, the container being used is not\
the same one you pulled from DockerHub. This one is our standard\
CentOS 7 Singularity container. I needed a gcc compiler in the\
container and the DockerHub one does not have one installed by\
default.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu \'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT <help@xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org \'a0#133559] Running\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 containerized\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
don\'92t know if I\'92ll get back to it tonight, but definitely will\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 work on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<https://aka.ms/o0ukef >\
________________________________\
From: Mahidhar Tatineni via RT <help@xsede.org >\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu >\
Subject: [tickets.xsede.org \'a0#133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket >>\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sun Apr 19 18:02:01 2020\
\
<meta http-equiv="Content-Type" content="text/html; charset=utf-\
8"><body style="word-wrap: break-word; -webkit-nbsp-mode: space; line-\
break: after-white-space;" class="">Martin,<div class=""><br\
class=""><div class="">Also, I have tried using the definition files\
for CentOS7 that you have on your Github profile, under both the\
\'93centos\'94 and \'93naked-singularity\'94 repositories, but none of them worked\
(they produce my old results). It would be helpful if you could share\
the definition file you used for the container creation as well as the\
command you used to build it.<div class=""><br class=""><div\
class="">Thank you,<div class="">George.<br class=""><br\
class=""><blockquote type="cite" class=""><div class="">On Apr 19,\
2020, at 5:57 PM, George Koubbe &lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu&gt; wrote:<br class="Apple-\
interchange-newline"><div class=""><meta http-equiv="Content-Type"\
content="text/html; charset=utf-8" class=""><div style="word-wrap:\
break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"\
class="">Dear Martin,<div class=""><br class=""><div class="">I have\
narrowed down my problem to the container itself.<br class=""><div\
class=""><br class=""><div class="">I just replicated your experiments\
and it worked as expected. However, I also replicated mine, but this\
time with the container centos.simg that you provide, and my\
experiments also yield the same results as yours.<div class=""><br\
class=""><div class="">It\'92s worth noting that:<div class=""><br\
class=""><div class="">1) It did not make a difference if the\
executable is compiled inside or outside the container<div class="">2)\
The container I create is only through the command (I don\'92t do\
anything else): $&nbsp;singularity build centos7.sif <a\
href="docker://centos:centos7" class="">docker://centos:centos7<div\
class=""><br class=""><div class="">Having said this, what is the\
difference between the container you build and the one I pull, that\
makes such a drastic difference in the results?&nbsp;<div class=""><br\
class=""><div class="">Thank you,<div class="">George.<div\
class=""><div class=""><br class=""><blockquote type="cite"\
class=""><div class="">On Apr 18, 2020, at 6:20 PM, Martin Kandes via\
RT &lt;<a href="mailto:help@xsede.org" class="">help@xsede.org&gt;\
wrote:<br class="Apple-interchange-newline"><div class=""><div\
class=""><br class="">Sat Apr 18 17:20:07 2020: Request 133559 was\
acted upon.<br class=""> Transaction: Correspondence added by\
mckandes<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-\
SDSC<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;Subject: Running\
containerized MPI applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class=""> Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-/tickets/133559"\
class="">https://portal.xsede.org/group/xup/tickets/-/tickets/133559\
<br class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">I've placed a copy of my example batch job scripts that\
compile and run your MPI hello, world code here [1]. <br class=""><br\
class="">As you can see in the standard output files from each job,\
there is really no significant difference in the performance when\
everything is setup correctly. i.e., I think the low runtimes you\
reported in the Singularity Slack channel were probably either\
incorrect and/or not an apples-to-apples comparison. <br class=""><br\
class="">Anyhow, have a look at how I've set things up and let me know\
if you have any questions. Note, however, the container being used is\
not the same one you pulled from DockerHub. This one is our standard\
CentOS 7 Singularity container. I needed a gcc compiler in the\
container and the DockerHub one does not have one installed by\
default.<br class=""><br class="">Marty Kandes<br class="">SDSC User\
Services Group<br class=""><br class="">[1]<br class=""><br\
class="">/share/apps/examples/singularity/bind-host-compilers-mpi<br\
class=""><br class="">On Fri Apr 17 19:30:30 2020, <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu wrote:<br class=""><blockquote\
type="cite" class="">Thank you Martin, I appreciate your support.<br\
class=""><br class="">George.<br class=""><br class=""><blockquote\
type="cite" class="">On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
&lt;<a href="mailto:help@xsede.org" class="">help@xsede.org&gt;<br\
class="">wrote:<br class=""><blockquote type="cite" class=""><br\
class=""><br class="">Fri Apr 17 19:15:18 2020: Request 133559 was\
acted upon.<br class="">Transaction: Correspondence added by\
mckandes<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br\
class=""> &nbsp;&nbsp;&nbsp;Subject: Re: [<a\
href="http://tickets.xsede.org/" class="">tickets.xsede.org #133559]\
Running containerized<br class="">MPI applications on SDSC Comet<br\
class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class=""> Requestors:\
<a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br\
class="">/tickets/133559<br class=""><blockquote type="cite"\
class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">There\'92s still at least one more issue I\'92m trying to sort out.\
I<br class="">don\'92t know if I\'92ll get back to it tonight, but\
definitely will work on<br class="">it again tomorrow and keep you\
posted.<br class=""><blockquote type="cite" class=""><br\
class="">Marty Kandes<br class="">SDSC User Services Group<br\
class=""><br class="">Get Outlook for iOS&lt;<a\
href="https://aka.ms/o0ukef" class="">https://aka.ms/o0ukef&gt;<br\
class="">________________________________<br class="">From: Mahidhar\
Tatineni via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org&gt;<br class="">Sent: Thursday, April 16, 2020\
10:19:20 PM<br class="">To: Kandes, Martin &lt;<a\
href="mailto:mkandes@sdsc.edu" class="">mkandes@sdsc.edu&gt;<br\
class="">Subject: [<a href="http://tickets.xsede.org/"\
class="">tickets.xsede.org #133559] Running containerized MPI<br\
class="">applications on SDSC Comet<br class=""><blockquote\
type="cite" class=""><br class=""><br class="">Fri Apr 17 00:19:20\
2020: Request 133559 was acted upon.<br class="">Transaction: Given to\
mckandes by mahidhar<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue:\
0-SDSC<br class=""> &nbsp;&nbsp;&nbsp;Subject: Running containerized\
MPI applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class=""> Requestors:\
<a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket &lt;URL:<br\
class=""><a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
<br class="">2nOMdZ3ikPm94px2MBcFY$ &nbsp;&gt;<br class=""><blockquote\
type="cite" class=""><br class="">This transaction appears to have no\
content<br class=""><br class=""><br class=""><br class=""><br\
class=""><br class=""><br class="">\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Mon Apr 20 11:41:51 2020\
\
Hi George,\
\
The container available in the examples directory I provided you was\
the one built using the latest version from the 'naked-singularity'\
repository. It's also available here on Comet [1]. The 'centos' one is\
a slightly updated version, but should not be significantly different\
in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones OS. As I\
already mentioned, it doesn't even have a gcc compiler and supporting\
libraries, which were needed to support the Intel compilers and\
IntelMPI being mounted in the container. And yes, it shouldn't really\
make a difference between compiling within the container or outside\
the container as the software environments should be pretty close in\
nature. However, if I compile and run a code within a container, I\
usually just compile inside the container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
CentOS 7 container that might been causing a problem. What version of\
Singularity are you building the container with? Is it built on a\
Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
\'a0\'a0have on your Github profile, under both the \'93centos\'94 and \'93naked-\
\'a0\'a0singularity\'94 repositories, but none of them worked (they produce\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 my\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0old results). It would be helpful if you could share the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 definition\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0file you used for the container creation as well as the command\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0used to build it.\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 19, 2020, at 5:57 PM, George Koubbe\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0However, I also replicated mine, but this time with the container\
\'a0\'a0centos.simg that you provide, and my experiments also yield the\
\'a0\'a0same results as yours.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 inside\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0or outside the container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 2) The container I create is only through the command (I don\'92t do\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0anything else): $ singularity build centos7.sif\
\'a0\'a0docker://centos:centos7\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Having said this, what is the difference between the container you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0build and the one I pull, that makes such a drastic difference in\
\'a0\'a0the results?\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT \'a0\'a0\'a0\'a0> wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559 \'a0\'a0\'a0\'a0/tickets/133559>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
As you can see in the standard output files from each job, there\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 is\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0really no significant difference in the performance when\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 everything\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is setup correctly. i.e., I think the low runtimes you reported\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 in\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the Singularity Slack channel were probably either incorrect\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and/or\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0not an apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Anyhow, have a look at how I've set things up and let me know if\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you have any questions. Note, however, the container being used\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 is\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0not the same one you pulled from DockerHub. This one is our\
\'a0\'a0standard CentOS 7 Singularity container. I needed a gcc compiler\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 in\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the container and the DockerHub one does not have one installed\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 by\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0default.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT \'a0\'a0\'a0\'a0>\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0#133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 /tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
don\'92t know if I\'92ll get back to it tonight, but definitely will\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0work on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS \'a0\'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0\'a0\'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu >\
Subject: [tickets.xsede.org \'a0#133559]\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket \'a0>>>\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Mon Apr 20 19:00:13 2020\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based formatted\
container. Can you provide we with a path to your working directory on\
Comet where I can review the output files for your tests? There might\
be some clues as to why you're seeing a different runtime.\
\
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Hi George,\
\
The container available in the examples directory I provided you was\
\'a0\'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity'\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0repository. It's also available here on Comet [1]. The 'centos'\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 one\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is a slightly updated version, but should not be significantly\
\'a0\'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0\'a0DockerHub is that the one from DockerHub is a very barebones OS.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 As\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0\'a0supporting libraries, which were needed to support the Intel\
\'a0\'a0compilers and IntelMPI being mounted in the container. And yes,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 it\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0shouldn't really make a difference between compiling within the\
\'a0\'a0container or outside the container as the software environments\
\'a0\'a0should be pretty close in nature. However, if I compile and run a\
\'a0\'a0code within a container, I usually just compile inside the\
\'a0\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
\'a0\'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 version\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0of Singularity are you building the container with? Is it built\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
\'a0\'a0have on your Github profile, under both the \'93centos\'94 and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \'93naked-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0singularity\'94 repositories, but none of them worked (they\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 produce\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0my\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0old results). It would be helpful if you could share the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0definition\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0file you used for the container creation as well as the command\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0used to build it.\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 19, 2020, at 5:57 PM, George Koubbe\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0centos.simg that you provide, and my experiments also yield the\
\'a0\'a0same results as yours.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0inside\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0or outside the container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 2) The container I create is only through the command (I don\'92t\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 do\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0anything else): $ singularity build centos7.sif\
\'a0\'a0docker://centos:centos7\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Having said this, what is the difference between the container\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0build and the one I pull, that makes such a drastic difference\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 in\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0the results?\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT \'a0> \'a0\'a0\'a0>\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0/tickets/133559 \'a0> \'a0\'a0\'a0/tickets/133559>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0and\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
As you can see in the standard output files from each job,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 there\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0really no significant difference in the performance when\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0everything\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0is setup correctly. i.e., I think the low runtimes you reported\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0the Singularity Slack channel were probably either incorrect\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0and/or\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0not an apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Anyhow, have a look at how I've set things up and let me know\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 if\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0you have any questions. Note, however, the container being used\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0not the same one you pulled from DockerHub. This one is our\
\'a0\'a0standard CentOS 7 Singularity container. I needed a gcc\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compiler\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0the container and the DockerHub one does not have one installed\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0by\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0default.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0#133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 /tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely will\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0work on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS > \'a0\'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0> \'a0\'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu >\
Subject: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0#133559]\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket \'a0> >>>\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Mon Apr 20 19:09:55 2020\
\
Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT \'a0wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based formatted\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container. Can you provide we with a path to your working directory on\
Comet where I can review the output files for your tests? There might\
be some clues as to why you're seeing a different runtime.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Hi George,\
\
The container available in the examples directory I provided you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 was\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity'\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0repository. It's also available here on Comet [1]. The 'centos'\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 one\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0is a slightly updated version, but should not be significantly\
\'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0DockerHub is that the one from DockerHub is a very barebones OS.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 As\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0supporting libraries, which were needed to support the Intel\
\'a0compilers and IntelMPI being mounted in the container. And yes,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 it\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0shouldn't really make a difference between compiling within the\
\'a0container or outside the container as the software environments\
\'a0should be pretty close in nature. However, if I compile and run a\
\'a0code within a container, I usually just compile inside the\
\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
\'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 version\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0of Singularity are you building the container with? Is it built\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
\'a0have on your Github profile, under both the \'93centos\'94 and \'93naked-\
\'a0singularity\'94 repositories, but none of them worked (they produce\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0my\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0old results). It would be helpful if you could share the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0definition\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0file you used for the container creation as well as the command\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0centos.simg that you provide, and my experiments also yield the\
\'a0same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0inside\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0or outside the container\
2) The container I create is only through the command (I don\'92t do\
\'a0anything else): $ singularity build centos7.sif\
\'a0docker://centos:centos7\
\
Having said this, what is the difference between the container\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0build and the one I pull, that makes such a drastic difference\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT >> \'a0\'a0> wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559 >> \'a0\'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0and\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job, there\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0is\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0really no significant difference in the performance when\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0everything\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is setup correctly. i.e., I think the low runtimes you reported\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the Singularity Slack channel were probably either incorrect\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0and/or\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know if\
\'a0you have any questions. Note, however, the container being used\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0is\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0not the same one you pulled from DockerHub. This one is our\
\'a0standard CentOS 7 Singularity container. I needed a gcc compiler\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the container and the DockerHub one does not have one installed\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0by\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Re: [tickets.xsede.org\
\'a0#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely will\
\'a0work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS>> \'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT >> \'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu >\
Subject: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0#133559]\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket >>>>>\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Mon Apr 20 19:09:55 2020\
\
<meta http-equiv="Content-Type" content="text/html; charset=utf-\
8"><body style="word-wrap: break-word; -webkit-nbsp-mode: space; line-\
break: after-white-space;" class="">Martin,<div class=""><br\
class=""><div class="">Definitely, the path to my current working\
directory is:<div class=""><br class=""><div\
class="">/home/karahbit/test/not_working<div class=""><br\
class=""><div class="">Once there, the exact commands I am running to\
see results are:<div class=""><br class=""><div class=""><div\
class="">$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24\
-t 00:30:00 --wait=0 --export=ALL /bin/bash<div class=""><br\
class=""><div class="">$ module purge<div class=""><br class=""><div\
class="">$ module load intel intelmpi<div class=""><br class=""><div\
class="">$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64<div\
class=""><br class=""><div class="">$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4<div\
class=""><br class=""><div class="">$ time -p mpirun singularity exec\
--bind /opt centos-repo.simg ./hello_world_intel<div class=""><br\
class=""><div class=""><br class=""><div class="">And I see the\
following result:<div class=""><br class=""><div class=""><img apple-\
inline="yes" id="8D362959-506F-4DF4-B489-27B2A381763F"\
src="cid:D3CB1005-818D-4A4E-BF8D-22B8E42FDE9C@fios-router.home"\
class=""><div class=""><br class=""><div class="">I appreciate your\
support on this matter,<div class="">George.<div class=""><br\
class=""><blockquote type="cite" class=""><div class="">On Apr 20,\
2020, at 8:00 PM, Martin Kandes via RT &lt;<a\
href="mailto:help@xsede.org" class="">help@xsede.org&gt; wrote:<br\
class="Apple-interchange-newline"><div class=""><div class=""><br\
class="">Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.<br\
class=""> Transaction: Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Subject: Running containerized MPI\
applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: user_wait<br class=""> Ticket\
URL: <a href="https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559" class="">https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 <br class=""><br class=""><br class="">George,<br\
class=""><br class="">I'm still not seeing a problem using a newer\
*.sif-based formatted container. Can you provide we with a path to\
your working directory on Comet where I can review the output files\
for your tests? There might be some clues as to why you're seeing a\
different runtime.<br class=""><br class="">Thanks,<br class=""><br\
class="">Marty Kandes<br class="">SDSC User Services Group<br\
class=""><br class="">On Mon Apr 20 11:41:51 2020, mckandes wrote:<br\
class=""><blockquote type="cite" class="">Hi George,<br class=""><br\
class="">The container available in the examples directory I provided\
you was<br class=""> &nbsp;&nbsp;the one built using the latest\
version from the 'naked-singularity'<br class="">\
&nbsp;&nbsp;repository. It's also available here on Comet [1]. The\
'centos' one<br class=""> &nbsp;&nbsp;is a slightly updated version,\
but should not be significantly<br class=""> &nbsp;&nbsp;different in\
functionality.<br class=""><br class="">The primary difference between\
our container and the one from<br class=""> &nbsp;&nbsp;DockerHub is\
that the one from DockerHub is a very barebones OS. As<br class="">\
&nbsp;&nbsp;I already mentioned, it doesn't even have a gcc compiler\
and<br class=""> &nbsp;&nbsp;supporting libraries, which were needed\
to support the Intel<br class=""> &nbsp;&nbsp;compilers and IntelMPI\
being mounted in the container. And yes, it<br class="">\
&nbsp;&nbsp;shouldn't really make a difference between compiling\
within the<br class=""> &nbsp;&nbsp;container or outside the container\
as the software environments<br class=""> &nbsp;&nbsp;should be pretty\
close in nature. However, if I compile and run a<br class="">\
&nbsp;&nbsp;code within a container, I usually just compile inside\
the<br class=""> &nbsp;&nbsp;container as it's the safer option.<br\
class=""><br class="">Anyhow, I'm not quite sure what went wrong with\
your build of our<br class=""> &nbsp;&nbsp;CentOS 7 container that\
might been causing a problem. What version<br class=""> &nbsp;&nbsp;of\
Singularity are you building the container with? Is it built on<br\
class=""> &nbsp;&nbsp;a Linux system?<br class=""><br class="">Marty\
Kandes<br class="">SDSC User Services Group<br class=""><br\
class="">[1]<br class=""><br class="">[mkandes@comet-ln2 ~]$ ls<br\
class="">\
&nbsp;&nbsp;/share/apps/compute/singularity/images/centos/<br\
class="">centos-openmpi.simg<br class="">centos.simg<br\
class="">centos-v6.10-20190916.simg<br class="">centos-v7.6.1810-\
20190513.simg<br class="">centos-v7.6.1810-openmpi-v1.8.4-\
20190514.simg<br class="">centos-v7.7.1908-20190914.simg<br\
class="">centos-v7.7.1908-20190919.simg<br class="">centos-v7.7.1908-\
20200129.simg<br class="">centos-v7.7.1908-openmpi-v1.8.4-\
20190919.simg<br class="">centos-v7.7.1908-openmpi-v3.1.4-\
20200211.simg<br class="">[mkandes@comet-ln2 ~]$<br class=""><br\
class="">On Sun Apr 19 18:02:01 2020, <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu wrote:<br class=""><blockquote\
type="cite" class="">Martin,<br class=""><br class="">Also, I have\
tried using the definition files for CentOS7 that you<br class="">\
&nbsp;&nbsp;have on your Github profile, under both the \'93centos\'94 and\
\'93naked-<br class=""> &nbsp;&nbsp;singularity\'94 repositories, but none\
of them worked (they produce<br class=""> &nbsp;&nbsp;my<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;old results).\
It would be helpful if you could share the<br class="">\
&nbsp;&nbsp;definition<br class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;file you used for the container creation as well as the\
command<br class=""> &nbsp;&nbsp;you<br class=""><blockquote\
type="cite" class=""> &nbsp;&nbsp;used to build it.<br class=""><br\
class="">Thank you,<br class="">George.<br class=""><br\
class=""><blockquote type="cite" class="">On Apr 19, 2020, at 5:57 PM,\
George Koubbe<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu&gt; wrote:<br\
class=""><blockquote type="cite" class=""><br class="">Dear Martin,<br\
class=""><br class="">I have narrowed down my problem to the container\
itself.<br class=""><br class="">I just replicated your experiments\
and it worked as expected.<br class=""> &nbsp;&nbsp;However, I also\
replicated mine, but this time with the container<br class="">\
&nbsp;&nbsp;centos.simg that you provide, and my experiments also\
yield the<br class=""> &nbsp;&nbsp;same results as yours.<br\
class=""><blockquote type="cite" class=""><br class="">It\'92s worth\
noting that:<br class=""><br class="">1) It did not make a difference\
if the executable is compiled<br class=""> &nbsp;&nbsp;inside<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;or outside the\
container<br class=""><blockquote type="cite" class="">2) The\
container I create is only through the command (I don\'92t do<br\
class=""> &nbsp;&nbsp;anything else): $ singularity build\
centos7.sif<br class=""> &nbsp;&nbsp;<a href="docker://centos:centos7"\
class="">docker://centos:centos7 &lt;<a href="docker://centos:centos7"\
class="">docker://centos:centos7&gt;<br class=""><blockquote\
type="cite" class=""><br class="">Having said this, what is the\
difference between the container you<br class=""> &nbsp;&nbsp;build\
and the one I pull, that makes such a drastic difference in<br\
class=""> &nbsp;&nbsp;the results?<br class=""><blockquote type="cite"\
class=""><br class="">Thank you,<br class="">George.<br class=""><br\
class=""><blockquote type="cite" class="">On Apr 18, 2020, at 6:20 PM,\
Martin Kandes via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:help@xsede.org" class="">mailto:help@xsede.org&gt;&gt;\
wrote:<br class=""><blockquote type="cite" class=""><blockquote\
type="cite" class=""><br class=""><br class="">Sat Apr 18 17:20:07\
2020: Request 133559 was acted upon.<br class="">Transaction:\
Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;&nbsp;Subject: Running containerized MPI applications on\
SDSC Comet<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner:\
mckandes<br class=""> Requestors: <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">mailto:glk35@scarletmail.rutgers.edu&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""> &nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket URL:\
<a href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br class="">\
&nbsp;&nbsp;/tickets/133559 &lt;<a\
href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br class="">\
&nbsp;&nbsp;/tickets/133559&gt;<br class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><br class=""><br class="">Hi\
George,<br class=""><br class="">I've placed a copy of my example\
batch job scripts that compile<br class=""> &nbsp;&nbsp;and<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;run your MPI\
hello, world code here [1].<br class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><br class="">As you can see\
in the standard output files from each job, there<br class="">\
&nbsp;&nbsp;is<br class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;really no significant difference in the performance\
when<br class=""> &nbsp;&nbsp;everything<br class=""><blockquote\
type="cite" class=""> &nbsp;&nbsp;is setup correctly. i.e., I think\
the low runtimes you reported<br class=""> &nbsp;&nbsp;in<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;the Singularity\
Slack channel were probably either incorrect<br class="">\
&nbsp;&nbsp;and/or<br class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;not an apples-to-apples comparison.<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><br class="">Anyhow, have a look at how I've set things up\
and let me know if<br class=""> &nbsp;&nbsp;you have any questions.\
Note, however, the container being used<br class=""> &nbsp;&nbsp;is<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;not the same\
one you pulled from DockerHub. This one is our<br class="">\
&nbsp;&nbsp;standard CentOS 7 Singularity container. I needed a gcc\
compiler<br class=""> &nbsp;&nbsp;in<br class=""><blockquote\
type="cite" class=""> &nbsp;&nbsp;the container and the DockerHub one\
does not have one installed<br class=""> &nbsp;&nbsp;by<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;default.<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><br class="">Marty Kandes<br class="">SDSC User Services\
Group<br class=""><br class="">[1]<br class=""><br\
class="">/share/apps/examples/singularity/bind-host-compilers-mpi<br\
class=""><br class="">On Fri Apr 17 19:30:30 2020, <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">mailto:glk35@scarletmail.rutgers.edu&gt; wrote:<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class="">Thank you Martin, I\
appreciate your support.<br class=""><br class="">George.<br\
class=""><br class=""><blockquote type="cite" class="">On Apr 17,\
2020, at 8:15 PM, Martin Kandes via RT<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:help@xsede.org" class="">help@xsede.org<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:help@xsede.org" class="">mailto:help@xsede.org&gt;&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class="">wrote:<br\
class=""><blockquote type="cite" class=""><br class=""><br\
class="">Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.<br\
class="">Transaction: Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;Subject: Re: [<a href="http://tickets.xsede.org"\
class="">tickets.xsede.org &lt;<a href="http://tickets.xsede.org/"\
class="">http://tickets.xsede.org/&gt;<br class="">\
&nbsp;&nbsp;#133559] Running containerized<br class=""><blockquote\
type="cite" class=""><blockquote type="cite" class=""><blockquote\
type="cite" class="">MPI applications on SDSC Comet<br\
class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class="">Requestors: <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">mailto:glk35@scarletmail.rutgers.edu&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""> &nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br class="">\
&nbsp;&nbsp;&lt;<a href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class="">/tickets/133559<br\
class=""><blockquote type="cite" class=""><br class=""><br class="">Hi\
George,<br class=""><br class="">There\'92s still at least one more issue\
I\'92m trying to sort out. I<br class="">don\'92t know if I\'92ll get back to\
it tonight, but definitely will<br class=""> &nbsp;&nbsp;work on<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class="">it again tomorrow and keep\
you posted.<br class=""><blockquote type="cite" class=""><br\
class="">Marty Kandes<br class="">SDSC User Services Group<br\
class=""><br class="">Get Outlook for iOS&lt;<a\
href="https://aka.ms/o0ukef" class="">https://aka.ms/o0ukef<br\
class=""> &nbsp;&nbsp;&lt;<a href="https://aka.ms/o0ukef"\
class="">https://aka.ms/o0ukef&gt;&gt;<br class=""><blockquote\
type="cite" class=""><blockquote type="cite" class=""><blockquote\
type="cite" class=""><blockquote type="cite"\
class="">________________________________<br class="">From: Mahidhar\
Tatineni via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:help@xsede.org" class="">mailto:help@xsede.org&gt;&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class="">Sent: Thursday, April 16, 2020 10:19:20 PM<br class="">To:\
Kandes, Martin &lt;<a href="mailto:mkandes@sdsc.edu"\
class="">mkandes@sdsc.edu &lt;<a href="mailto:mkandes@sdsc.edu"\
class="">mailto:mkandes@sdsc.edu&gt;&gt;<br class="">Subject: [<a\
href="http://tickets.xsede.org" class="">tickets.xsede.org &lt;<a\
href="http://tickets.xsede.org/"\
class="">http://tickets.xsede.org/&gt;<br class="">\
&nbsp;&nbsp;#133559]<br class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;Running containerized MPI<br class=""><blockquote\
type="cite" class=""><blockquote type="cite" class=""><blockquote\
type="cite" class="">applications on SDSC Comet<br\
class=""><blockquote type="cite" class=""><br class=""><br\
class="">Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.<br\
class="">Transaction: Given to mckandes by mahidhar<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;Subject: Running containerized MPI applications on SDSC<br\
class=""> &nbsp;&nbsp;Comet<br class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""> &nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br\
class="">Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">mailto:glk35@scarletmail.rutgers.edu&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""> &nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket\
&lt;URL:<br class=""><br class=""><br class=""> &nbsp;&nbsp;<a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
<br class=""><blockquote type="cite" class=""><br class="">\
&nbsp;&nbsp;&lt;<a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
<br class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><br class=""><blockquote type="cite" class=""><blockquote\
type="cite" class="">2nOMdZ3ikPm94px2MBcFY$ &nbsp;&gt;<br\
class=""><blockquote type="cite" class=""><br class="">This\
transaction appears to have no content<br class=""><br class=""><br\
class=""><br class=""><br class=""><br class=""><br class=""><br\
class=""><br class=""><br class=""><br class=""><br class="">\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Tue Apr 21 17:51:28 2020\
\
Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge there\
is some sort of performance difference between the 3 containers I'm\
testing with here, one of them is your version of the container. But\
each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World test\
problem with your container [1]. Your numbers are the lower runtimes\
in the middle. But as you can see, they do fluctuate to the average as\
well for all containers.\
\
To move away from the simple Hello, World test problem, I then ran the\
OSU Microbenchmarks for both bandwidth [2] and latency [3]. As you can\
see here, the results are more consistent, with your container\
underperforming. The fundamental problem seems to be that your\
container does not properly utilize Comet's infiniband network. This\
can be seen in the high latency [4]. The latency should be on the\
order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only difference\
I see right now is that your container was built with Singularity\
3.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2? Also, how\
did you install your copy of Singularity 3.5.0? Did you compile from\
source? Or is that a binary executable that you downloaded and\
installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-singularity-\
omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-singularity-\
omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a0\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based formatted\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container. Can you provide we with a path to your working\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 directory\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on Comet where I can review the output files for your tests?\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 There\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0might be some clues as to why you're seeing a different runtime.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Hi George,\
\
The container available in the examples directory I provided you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0was\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0singularity'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0repository. It's also available here on Comet [1]. The 'centos'\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0one\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is a slightly updated version, but should not be significantly\
\'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0DockerHub is that the one from DockerHub is a very barebones\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 OS.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0As\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0supporting libraries, which were needed to support the Intel\
\'a0compilers and IntelMPI being mounted in the container. And yes,\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0it\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0shouldn't really make a difference between compiling within the\
\'a0container or outside the container as the software environments\
\'a0should be pretty close in nature. However, if I compile and run\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 a\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0code within a container, I usually just compile inside the\
\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
\'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0version\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0of Singularity are you building the container with? Is it built\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
Martin,\
\
Also, I have tried using the definition files for CentOS7 that\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0have on your Github profile, under both the \'93centos\'94 and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \'93naked-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0singularity\'94 repositories, but none of them worked (they\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 produce\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0my\
\'a0old results). It would be helpful if you could share the\
\'a0definition\
\'a0file you used for the container creation as well as the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 command\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you\
\'a0used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0centos.simg that you provide, and my experiments also yield\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\'a0inside\
\'a0or outside the container\
2) The container I create is only through the command (I don\'92t\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 do\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0anything else): $ singularity build centos7.sif\
\'a0docker://centos:centos7\
\
Having said this, what is the difference between the container\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0build and the one I pull, that makes such a drastic difference\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559 \'a0>>> \'a0\'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compile\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0and\
\'a0run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is\
\'a0really no significant difference in the performance when\
\'a0everything\
\'a0is setup correctly. i.e., I think the low runtimes you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 reported\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
\'a0the Singularity Slack channel were probably either incorrect\
\'a0and/or\
\'a0not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 if\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you have any questions. Note, however, the container being\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 used\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is\
\'a0not the same one you pulled from DockerHub. This one is our\
\'a0standard CentOS 7 Singularity container. I needed a gcc\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compiler\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
\'a0the container and the DockerHub one does not have one\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 installed\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0by\
\'a0default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Re: [tickets.xsede.org\
\'a0#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 out.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 will\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS >>> \'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0>>> \'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin \'a0\'a0\'a0\'a0>\
Subject: [tickets.xsede.org\
\'a0#133559]\
\'a0Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\'a0Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket \'a0>>>>>>\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Tue Apr 21 18:04:08 2020\
\
George,\
\
A side note: I finally had a look over the project you're working on.\
It looks pretty interesting. So is the idea here that you might --bind\
mount the local compilers/mpi on a resource into users containers when\
the container runs on that resource? If so, I'm still thinking this\
would be more complicated to support than simply having a set of\
static containers to choose from that would be compatible the\
compiler/mpi you find/discover at a resource. But maybe I'm missing\
something here.\
\
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 there\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is some sort of performance difference between the 3 containers\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 I'm\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0testing with here, one of them is your version of the container.\
\'a0\'a0But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World test\
\'a0\'a0problem with your container [1]. Your numbers are the lower\
\'a0\'a0runtimes in the middle. But as you can see, they do fluctuate to\
\'a0\'a0the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then ran\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0can see here, the results are more consistent, with your\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0underperforming. The fundamental problem seems to be that your\
\'a0\'a0container does not properly utilize Comet's infiniband network.\
\'a0\'a0This can be seen in the high latency [4]. The latency should be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I see right now is that your container was built with Singularity\
\'a0\'a03.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2? Also,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 how\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0did you install your copy of Singularity 3.5.0? Did you compile\
\'a0\'a0from source? Or is that a binary executable that you downloaded\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0\'a0singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a0\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 formatted\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0container. Can you provide we with a path to your working\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0directory\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0on Comet where I can review the output files for your tests?\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0There\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0was\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0singularity'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 'centos'\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0one\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is a slightly updated version, but should not be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 significantly\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0DockerHub is that the one from DockerHub is a very barebones\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0OS.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0As\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0supporting libraries, which were needed to support the Intel\
\'a0compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 yes,\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0it\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0a\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0code within a container, I usually just compile inside the\
\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 our\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0version\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 built\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7 that\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0have on your Github profile, under both the \'93centos\'94 and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'93naked-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0singularity\'94 repositories, but none of them worked (they\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0produce\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0my\
\'a0old results). It would be helpful if you could share the\
\'a0definition\
\'a0file you used for the container creation as well as the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0command\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you\
\'a0used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0centos.simg that you provide, and my experiments also yield\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\'a0inside\
\'a0or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 don\'92t\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0do\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0anything else): $ singularity build centos7.sif\
\'a0docker://centos:centos7\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559 \'a0> >>> \'a0\'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0compile\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0and\
\'a0run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is\
\'a0really no significant difference in the performance when\
\'a0everything\
\'a0is setup correctly. i.e., I think the low runtimes you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0reported\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
\'a0the Singularity Slack channel were probably either incorrect\
\'a0and/or\
\'a0not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 know\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0if\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you have any questions. Note, however, the container being\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0used\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is\
\'a0not the same one you pulled from DockerHub. This one is our\
\'a0standard CentOS 7 Singularity container. I needed a gcc\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0compiler\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
\'a0the container and the DockerHub one does not have one\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0installed\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0by\
\'a0default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Re: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0out.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0will\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS > >>> \'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0> >>> \'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin \'a0> \'a0\'a0\'a0>\
Subject: [tickets.xsede.org\
\'a0#133559]\
\'a0Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\'a0Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket \'a0> >>>>>>\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Tue Apr 21 18:40:57 2020\
\
Martin,\
\
This issue is very interesting indeed. It was nice of you to even go\
further and perform bandwidth and latency tests, which confirms the\
considerable performance difference between our containers, although\
we have not been able to locate the source of the difference so far. I\
do build with Singularity 3.5.0 installed from source. What is the\
exact command you use for building?\
\
And you are right regarding your intuition of what we want to do. We\
are still exploring the options of how we are going to distribute the\
containers to end users depending on the application they try to run.\
Of course, as you point out, the bindings would need to be specific to\
the remote resource.\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT \'a0wrote:\
\
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
George,\
\
A side note: I finally had a look over the project you're working\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on. It looks pretty interesting. So is the idea here that you might\
--bind mount the local compilers/mpi on a resource into users\
containers when the container runs on that resource? If so, I'm still\
thinking this would be more complicated to support than simply having\
a set of static containers to choose from that would be compatible the\
compiler/mpi you find/discover at a resource. But maybe I'm missing\
something here.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 there\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0is some sort of performance difference between the 3 containers\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 I'm\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0testing with here, one of them is your version of the container.\
\'a0But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World test\
\'a0problem with your container [1]. Your numbers are the lower\
\'a0runtimes in the middle. But as you can see, they do fluctuate to\
\'a0the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then ran\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0can see here, the results are more consistent, with your\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0underperforming. The fundamental problem seems to be that your\
\'a0container does not properly utilize Comet's infiniband network.\
\'a0This can be seen in the high latency [4]. The latency should be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0I see right now is that your container was built with Singularity\
\'a03.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2? Also,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 how\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0did you install your copy of Singularity 3.5.0? Did you compile\
\'a0from source? Or is that a binary executable that you downloaded\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\'a0wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 formatted\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0container. Can you provide we with a path to your working\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0directory\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0on Comet where I can review the output files for your tests?\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0There\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0might be some clues as to why you're seeing a different runtime.\
\
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided you\
\'a0was\
the one built using the latest version from the 'naked-\
\'a0singularity'\
repository. It's also available here on Comet [1]. The 'centos'\
\'a0one\
is a slightly updated version, but should not be significantly\
different in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0OS.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And yes,\
\'a0it\
shouldn't really make a difference between compiling within the\
container or outside the container as the software environments\
should be pretty close in nature. However, if I compile and run\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0a\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 our\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 CentOS 7 container that might been causing a problem. What\
\'a0version\
of Singularity are you building the container with? Is it built\
\'a0on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7 that\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 have on your Github profile, under both the \'93centos\'94 and\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'93naked-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 singularity\'94 repositories, but none of them worked (they\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0produce\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0command\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
However, I also replicated mine, but this time with the\
\'a0container\
centos.simg that you provide, and my experiments also yield\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
inside\
or outside the container\
2) The container I create is only through the command (I don\'92t\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0do\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the container\
\'a0you\
build and the one I pull, that makes such a drastic difference\
\'a0in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 >>>>> \'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0compile\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0reported\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the Singularity Slack channel were probably either incorrect\
and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0if\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you have any questions. Note, however, the container being\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0used\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0compiler\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the container and the DockerHub one does not have one\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0installed\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 #133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0out.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I\
don\'92t know if I\'92ll get back to it tonight, but definitely\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0will\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS>>>>> \'a0>\
________________________________\
From: Mahidhar Tatineni via RT >>>>> \'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin >> \'a0\'a0>\
Subject: [tickets.xsede.org\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket >>>>>>>>\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Wed Apr 22 11:36:13 2020\
\
George,\
\
I build the containers using the build command: sudo singularity build\
containter.simg container.deffile\
\
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 18:40:57 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
This issue is very interesting indeed. It was nice of you to even go\
\'a0\'a0further and perform bandwidth and latency tests, which confirms\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0considerable performance difference between our containers,\
\'a0\'a0although we have not been able to locate the source of the\
\'a0\'a0difference so far. I do build with Singularity 3.5.0 installed\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 from\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0source. What is the exact command you use for building?\
\
And you are right regarding your intuition of what we want to do. We\
\'a0\'a0are still exploring the options of how we are going to distribute\
\'a0\'a0the containers to end users depending on the application they try\
\'a0\'a0to run. Of course, as you point out, the bindings would need to\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 be\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0specific to the remote resource.\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
George,\
\
A side note: I finally had a look over the project you're working\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on. It looks pretty interesting. So is the idea here that you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 might\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0--bind mount the local compilers/mpi on a resource into users\
\'a0\'a0containers when the container runs on that resource? If so, I'm\
\'a0\'a0still thinking this would be more complicated to support than\
\'a0\'a0simply having a set of static containers to choose from that\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 would\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0be compatible the compiler/mpi you find/discover at a resource.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 But\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0maybe I'm missing something here.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is some sort of performance difference between the 3 containers\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I'm\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0testing with here, one of them is your version of the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 test\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0problem with your container [1]. Your numbers are the lower\
\'a0runtimes in the middle. But as you can see, they do fluctuate\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 to\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 ran\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0can see here, the results are more consistent, with your\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0underperforming. The fundamental problem seems to be that your\
\'a0container does not properly utilize Comet's infiniband network.\
\'a0This can be seen in the high latency [4]. The latency should be\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I see right now is that your container was built with\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Singularity\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a03.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2?\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Also,\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0how\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0did you install your copy of Singularity 3.5.0? Did you compile\
\'a0from source? Or is that a binary executable that you downloaded\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0and\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0singularity-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0singularity-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0formatted\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0container. Can you provide we with a path to your working\
\'a0directory\
\'a0on Comet where I can review the output files for your tests?\
\'a0There\
\'a0might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0was\
the one built using the latest version from the 'naked-\
\'a0singularity'\
repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 'centos'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0one\
is a slightly updated version, but should not be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 significantly\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 different in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones\
\'a0OS.\
\'a0As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 yes,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0it\
shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0a\
code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0our\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 CentOS 7 container that might been causing a problem. What\
\'a0version\
of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 built\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 that\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you\
have on your Github profile, under both the \'93centos\'94 and\
\'a0\'93naked-\
singularity\'94 repositories, but none of them worked (they\
\'a0produce\
my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
\'a0command\
you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 expected.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 However, I also replicated mine, but this time with the\
\'a0container\
centos.simg that you provide, and my experiments also yield\
\'a0the\
same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compiled\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 inside\
or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 don\'92t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you\
build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\'a0Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 \'a0>>>>>> \'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
\'a0compile\
and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
\'a0there\
is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
\'a0reported\
in\
the Singularity Slack channel were probably either incorrect\
and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 know\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0if\
you have any questions. Note, however, the container being\
\'a0used\
is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
\'a0compiler\
in\
the container and the DockerHub one does not have one\
\'a0installed\
by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
\
#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
\'a0out.\
\'a0I\
don\'92t know if I\'92ll get back to it tonight, but definitely\
\'a0will\
work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS >>>>>> \'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0>>>>>> \'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin \'a0>>> \'a0\'a0>\
Subject: [tickets.xsede.org\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket \'a0>>>>>>>>>\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Wed Apr 22 12:03:12 2020\
\
Martin,\
\
Yeah, well, I have no clue what is going on then. It is what it is and\
thank you so much for your support throughout this whole issue anyway!\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 22, 2020, at 12:36 PM, Martin Kandes via RT \'a0wrote:\
\
\
Wed Apr 22 11:36:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
George,\
\
I build the containers using the build command: sudo singularity\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 build containter.simg container.deffile\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 18:40:57 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Martin,\
\
This issue is very interesting indeed. It was nice of you to even\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 go\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0further and perform bandwidth and latency tests, which confirms\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0considerable performance difference between our containers,\
\'a0although we have not been able to locate the source of the\
\'a0difference so far. I do build with Singularity 3.5.0 installed\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 from\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0source. What is the exact command you use for building?\
\
And you are right regarding your intuition of what we want to do.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 We\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0are still exploring the options of how we are going to distribute\
\'a0the containers to end users depending on the application they try\
\'a0to run. Of course, as you point out, the bindings would need to\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 be\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0specific to the remote resource.\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
George,\
\
A side note: I finally had a look over the project you're working\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0on. It looks pretty interesting. So is the idea here that you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 might\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0--bind mount the local compilers/mpi on a resource into users\
\'a0containers when the container runs on that resource? If so, I'm\
\'a0still thinking this would be more complicated to support than\
\'a0simply having a set of static containers to choose from that\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 would\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0be compatible the compiler/mpi you find/discover at a resource.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 But\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0maybe I'm missing something here.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 is some sort of performance difference between the 3 containers\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0I'm\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 testing with here, one of them is your version of the container.\
But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 test\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 problem with your container [1]. Your numbers are the lower\
runtimes in the middle. But as you can see, they do fluctuate to\
the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 ran\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 can see here, the results are more consistent, with your\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 underperforming. The fundamental problem seems to be that your\
container does not properly utilize Comet's infiniband network.\
This can be seen in the high latency [4]. The latency should be\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 I see right now is that your container was built with\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Singularity\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 3.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2?\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Also,\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0how\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 did you install your copy of Singularity 3.5.0? Did you compile\
from source? Or is that a binary executable that you downloaded\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0and\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0singularity-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0singularity-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0formatted\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container. Can you provide we with a path to your working\
directory\
on Comet where I can review the output files for your tests?\
There\
might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 was\
the one built using the latest version from the 'naked-\
singularity'\
repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 'centos'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 one\
is a slightly updated version, but should not be significantly\
different in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones\
OS.\
As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 yes,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it\
shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 a\
code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0our\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 CentOS 7 container that might been causing a problem. What\
version\
of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 built\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 that\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
have on your Github profile, under both the \'93centos\'94 and\
\'93naked-\
singularity\'94 repositories, but none of them worked (they\
produce\
my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
command\
you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 expected.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 However, I also replicated mine, but this time with the\
container\
centos.simg that you provide, and my experiments also yield\
the\
same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compiled\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 inside\
or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 don\'92t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 >>>>>>> /tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
compile\
and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
there\
is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
reported\
in\
the Singularity Slack channel were probably either incorrect\
and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 know\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 if\
you have any questions. Note, however, the container being\
used\
is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
compiler\
in\
the container and the DockerHub one does not have one\
installed\
by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
\
#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
out.\
I\
don\'92t know if I\'92ll get back to it tonight, but definitely\
will\
work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS>>>>>>> >\
________________________________\
From: Mahidhar Tatineni via RT >>>>>>> >\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin >>>> \'a0>\
Subject: [tickets.xsede.org\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
Status: open\
Ticket >>>>>>>>>>\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Wed Apr 22 12:24:09 2020\
\
George,\
\
No problem. Yeah, I'm not sure what else to try. The only thing I can\
think of is maybe there is/was a bug in Singularity 3.5.0 that has\
something about the bind mounts not working properly and preventing\
the container from using/seeing the infiniband drivers and libraries\
on the underlying host system. That's really my only hypothesis right\
now. But again, we usually just install compatible drivers and\
libraries inside the container.\
\
Anyhow, if I sort this out eventually, I'll let you know. But I'm not\
sure we'll sort it out anytime soon.\
\
Marty Kandes\
SDSC User Services Group\
\
On Wed Apr 22 12:03:12 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Yeah, well, I have no clue what is going on then. It is what it is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0thank you so much for your support throughout this whole issue\
\'a0\'a0anyway!\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 22, 2020, at 12:36 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Wed Apr 22 11:36:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
George,\
\
I build the containers using the build command: sudo singularity\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0build containter.simg container.deffile\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 18:40:57 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
This issue is very interesting indeed. It was nice of you to even\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0go\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0further and perform bandwidth and latency tests, which confirms\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0considerable performance difference between our containers,\
\'a0although we have not been able to locate the source of the\
\'a0difference so far. I do build with Singularity 3.5.0 installed\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0from\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0source. What is the exact command you use for building?\
\
And you are right regarding your intuition of what we want to do.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0We\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0are still exploring the options of how we are going to\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 distribute\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the containers to end users depending on the application they\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 try\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0to run. Of course, as you point out, the bindings would need to\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0be\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0specific to the remote resource.\
\
Best,\
George.\
\
On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT\
\'a0wrote:\
\
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559\
\
\
George,\
\
A side note: I finally had a look over the project you're\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 working\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0on. It looks pretty interesting. So is the idea here that you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0might\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0--bind mount the local compilers/mpi on a resource into users\
\'a0containers when the container runs on that resource? If so, I'm\
\'a0still thinking this would be more complicated to support than\
\'a0simply having a set of static containers to choose from that\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0would\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0be compatible the compiler/mpi you find/discover at a resource.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0But\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0maybe I'm missing something here.\
\
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\'a0there\
is some sort of performance difference between the 3\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 containers\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I'm\
testing with here, one of them is your version of the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0test\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 problem with your container [1]. Your numbers are the lower\
runtimes in the middle. But as you can see, they do fluctuate\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 to\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0ran\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the\
OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\'a0you\
can see here, the results are more consistent, with your\
\'a0container\
underperforming. The fundamental problem seems to be that your\
container does not properly utilize Comet's infiniband\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 network.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 This can be seen in the high latency [4]. The latency should\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 be\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0on\
the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\'a0difference\
I see right now is that your container was built with\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Singularity\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 3.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2?\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Also,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0how\
did you install your copy of Singularity 3.5.0? Did you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compile\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 from source? Or is that a binary executable that you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 downloaded\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0and\
installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity-\
omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity-\
omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 are:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 -t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 00:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\
wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\'a0formatted\
container. Can you provide we with a path to your working\
directory\
on Comet where I can review the output files for your tests?\
There\
might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 was\
the one built using the latest version from the 'naked-\
singularity'\
repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0'centos'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 one\
is a slightly updated version, but should not be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 significantly\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 different in functionality.\
\
The primary difference between our container and the one\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 from\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 DockerHub is that the one from DockerHub is a very barebones\
OS.\
As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0yes,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it\
shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0run\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 a\
code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 of\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0our\
CentOS 7 container that might been causing a problem. What\
version\
of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0built\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\'a0wrote:\
Martin,\
\
Also, I have tried using the definition files for CentOS7\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0that\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
have on your Github profile, under both the \'93centos\'94 and\
\'93naked-\
singularity\'94 repositories, but none of them worked (they\
produce\
my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
command\
you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0expected.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 However, I also replicated mine, but this time with the\
container\
centos.simg that you provide, and my experiments also yield\
the\
same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0compiled\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 inside\
or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0don\'92t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 \'a0>>>>>>>> /tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
compile\
and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 job,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 there\
is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
reported\
in\
the Singularity Slack channel were probably either\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 incorrect\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0know\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 if\
you have any questions. Note, however, the container being\
used\
is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
compiler\
in\
the container and the DockerHub one does not have one\
installed\
by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 upon.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Transaction: Correspondence added by mckandes\
\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
\
#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
Status: open\
Ticket URL:\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 sort\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 out.\
I\
don\'92t know if I\'92ll get back to it tonight, but\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 definitely\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 will\
work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS >>>>>>>> >\
________________________________\
From: Mahidhar Tatineni via RT \'a0>>>>>>>> >\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin \'a0>>>>> \'a0>\
Subject: [tickets.xsede.org\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 upon.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Transaction: Given to mckandes by mahidhar\
\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
Status: open\
Ticket \'a0>>>>>>>>>>>\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
\
\
-------------------------------------------------------------------------\
\'a0\'a0\'a0Common Information\
-------------------------------------------------------------------------\
\
There is no need to reply to this message unless you want to RE-OPEN your\
ticket with ID [tickets.xsede.org #133559].\
\
If you want to simply add a COMMENT to this ticket without re-opening the ticket, click below:\
mailto:helpcomments@tickets.xsede.org?subject=[tickets.xsede.org%20#133559]&body=%20\
\
\
Please note:\
- ALWAYS include the string [tickets.xsede.org #133559] in the subject line of all future correspondence about this issue.\
- Do NOT attach or include the content of previous emails already sent to you by rt.\
\
Subject: Resolved: Running containerized MPI applications on SDSC Comet\
\
\
According to our records, your request has been resolved. If you have any further questions or concerns, please respond to this message.\
\
\
----------------------------------------------------------------\
\'a0Your initial request was\
----------------------------------------------------------------\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug" partition of Comet, requesting 2 nodes and using the full 48 cores of said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI \
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c\
$ export PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64 $HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind /lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind /etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside the container is faster than outside, according only to the commands I typed of course. When I changed the MPI implementation to mvapich2, running the executable inside the container was slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would mean to me. Thank you!\
George Koubbe.\
\
\
\
----------------------------------------------------------------\
\'a0\'a0Complete Ticket History\
----------------------------------------------------------------\
\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Thu Apr 16 20:57:45 2020\
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow\
with the results.\
\
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack\
channel as someone else DM\'92d me and was curious about the issue too.\
\
Get Outlook for iOS\
________________________________\
From: Ellen Buskuehl via RT\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: buskuehl\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug"\
partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for\
both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind\
/lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside\
the container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was slightly slower than\
outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
mean to me. Thank you!\
George Koubbe.\
\
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Thu Apr 16 20:57:45 2020\
\
\
\
<meta http-equiv="Content-Type" content="text/html; charset=Windows-\
1252">\
\
\
\
\
<div style="direction: ltr;">Hi George,\
\
\
<div style="direction: ltr;">I\'92ve almost solved this issue already.\
I\'92ll get back to you tomorrow with the results.\
\
\
<div style="direction: ltr;">Marty Kandes\
<div style="direction: ltr;">SDSC User Services Group\
\
\
<div style="direction: ltr;">P.S. I\'92ll provide you solution here and\
on the Singularly Slack channel as someone else DM\'92d me and was\
curious about the issue too.\
\
\
\
<div class="ms-outlook-ios-signature">Get <a\
href="https://aka.ms/o0ukef">Outlook for iOS\
\
<hr style="display:inline-block;width:98%" tabindex="-1">\
<div id="divRplyFwdMsg" dir="ltr"><font face="Calibri, sans-serif"\
style="font-size:11pt" color="#000000">From: Ellen Buskuehl via RT\
&lt;help@xsede.org&gt;\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
&nbsp;\
\
<div class="BodyFragment"><font size="2"><span style="font-\
size:11pt;">\
<div class="PlainText">\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
&nbsp;Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Queue: 0-SDSC\
&nbsp;&nbsp;&nbsp;&nbsp; Subject: Running containerized MPI\
applications on SDSC Comet\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Owner: buskuehl\
&nbsp; Requestors: glk35@scarletmail.rutgers.edu\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status: open\
&nbsp;Ticket &lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$">\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$&nbsp;\
&gt;\
\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug&quot;\
partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for\
both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$">\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
\
$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$">\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind\
/lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside\
the container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was\
slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
mean to me. Thank you!\
George Koubbe.\
\
&nbsp;\
\
\
\
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Thu Apr 16 21:01:39 2020\
\
P.S. I\'92m also curious what the use case is here. Can you let us know\
what you\'92re trying to accomplish? As I mentioned, we\'92ve thought of\
doing this before as well. But never found a strong reason to do so\
yet.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Martin Kandes via RT\
Sent: Thursday, April 16, 2020 6:57:46 PM\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Thu Apr 16 20:57:45 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mahidhar\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1MFeXDzc$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow\
with the results.\
\
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack\
channel as someone else DM\'92d me and was curious about the issue too.\
\
Get Outlook for\
iOS<https://urldefense.com/v3/__https://aka.ms/o0ukef__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1IVSDe_Y$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 ________________________________\
From: Ellen Buskuehl via RT\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: buskuehl\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug"\
partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for\
both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind\
/lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside\
the container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was slightly slower than\
outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
mean to me. Thank you!\
George Koubbe.\
\
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Thu Apr 16 21:01:39 2020\
\
\
\
<meta http-equiv="Content-Type" content="text/html; charset=Windows-\
1252">\
\
\
\
\
<div style="direction: ltr;">P.S. I\'92m also curious what the use case\
is here. Can you let us know what you\'92re trying to accomplish? As I\
mentioned, we\'92ve thought of doing this before as well. But never found\
a strong reason to do so yet.\
\
\
<div style="direction: ltr;">Marty Kandes\
<div style="direction: ltr;">SDSC User Services Group\
\
\
\
<div class="ms-outlook-ios-signature">Get <a\
href="https://aka.ms/o0ukef">Outlook for iOS\
\
<hr style="display:inline-block;width:98%" tabindex="-1">\
<div id="divRplyFwdMsg" dir="ltr"><font face="Calibri, sans-serif"\
style="font-size:11pt" color="#000000">From: Martin Kandes via RT\
&lt;help@xsede.org&gt;\
Sent: Thursday, April 16, 2020 6:57:46 PM\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
&nbsp;\
\
<div class="BodyFragment"><font size="2"><span style="font-\
size:11pt;">\
<div class="PlainText">\
Thu Apr 16 20:57:45 2020: Request 133559 was acted upon.\
&nbsp;Transaction: Correspondence added by mckandes\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Queue: 0-SDSC\
&nbsp;&nbsp;&nbsp;&nbsp; Subject: Re: [tickets.xsede.org #133559]\
Running containerized MPI applications on SDSC Comet\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Owner: mahidhar\
&nbsp; Requestors: glk35@scarletmail.rutgers.edu\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status: open\
&nbsp;Ticket &lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1MFeXDzc$">\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1MFeXDzc$&nbsp;\
&gt;\
\
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow\
with the results.\
\
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack\
channel as someone else DM\'92d me and was curious about the issue too.\
\
Get Outlook for iOS&lt;<a\
href="">https://urldefense.com/v3/__https://aka.ms/o0ukef__;!!Mih3wA!RYbNFUV5G43wozvpLCUogxNv6fXTOMLlzem70FK4XZfbXfx8Car3zbg1IVSDe_Y$\
&gt;\
________________________________\
From: Ellen Buskuehl via RT &lt;help@xsede.org&gt;\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
&nbsp;Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Queue: 0-SDSC\
&nbsp;&nbsp;&nbsp;&nbsp; Subject: Running containerized MPI\
applications on SDSC Comet\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Owner: buskuehl\
&nbsp; Requestors: glk35@scarletmail.rutgers.edu\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status: open\
&nbsp;Ticket &lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$">\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$&nbsp;\
&gt;\
\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug&quot;\
partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for\
both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$">\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$">\
https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind\
/lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside\
the container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was\
slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
mean to me. Thank you!\
George Koubbe.\
\
\
\
\
\
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Thu Apr 16 21:14:36 2020\
\
This is great news Martin!\
\
As a software engineer myself, I know the struggle when someone has an\
issue and asks individually instead of officially opening a ticket.\
That \'a0is why I opened the ticket here as well.\
\
Regarding the use case:\
\
I work at RADICAL-Lab at Rutgers University - New Brunswick. We have\
developed a set of software tools (https://radical-\
cybertools.github.io ) that are architected for scalable,\
interoperable and sustainable approaches to support science on a range\
of high-performance and distributed computing systems.\
\
As we move forward and embrace new technologies, I intend to do with\
the present work a performance characterization of the execution of\
containerized scientific applications on HPC platforms using RADICAL-\
Cybertools (RCT). This will allow us to make decisions based on best\
software engineering practices and integrate them into RCT so we can\
continue to support our users in terms of all the benefits containers\
can bring.\
\
Thank you so much for looking into this in such a quick and efficient\
fashion,\
George Koubbe.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 16, 2020, at 9:57 PM, Martin Kandes via RT \'a0wrote:\
\
\
Thu Apr 16 20:57:45 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'a0\'a0\'a0Owner: mahidhar\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Hi George,\
\
I\'92ve almost solved this issue already. I\'92ll get back to you tomorrow\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 with the results.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
P.S. I\'92ll provide you solution here and on the Singularly Slack\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 channel as someone else DM\'92d me and was curious about the issue too.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Get Outlook for iOS\
________________________________\
From: Ellen Buskuehl via RT\
Sent: Thursday, April 16, 2020 6:39:27 PM\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Thu Apr 16 20:39:26 2020: Request 133559 was acted upon.\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: buskuehl\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
1. I just tried running an mpirun command directly on the \'93debug"\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 partition of Comet, requesting 2 nodes and using the full 48 cores of\
said nodes.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 2. I made sure the MPI implementation (Intel MPI 18.1) is the same\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 for both containerized/non containerized \'93hello world\'94 executable.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
This is exactly what I do since I login to comet:\
\
For Intel MPI\
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 00:30:00 --wait=0 --export=ALL /bin/bash\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 $ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 00:30:00 --wait=0 --export=ALL /bin/bash\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 --bind /lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind\
/etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 inside the container is faster than outside, according only to the\
commands I typed of course. When I changed the MPI implementation to\
mvapich2, running the executable inside the container was slightly\
slower than outside (this is what I originally expected).\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 mean to me. Thank you!\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 George Koubbe.\
\
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Thu Apr 16 21:14:36 2020\
\
<meta http-equiv="Content-Type" content="text/html; charset=utf-\
8"><body style="word-wrap: break-word; -webkit-nbsp-mode: space; line-\
break: after-white-space;" class="">This is great news Martin!<div\
class=""><br class=""><div class="">As a software engineer myself, I\
know the struggle when someone has an issue and asks individually\
instead of officially opening a ticket. That &nbsp;is why I opened the\
ticket here as well.<div class=""><br class=""><div class="">Regarding\
the use case:<div class=""><br class=""><div class="">I work at\
RADICAL-Lab at Rutgers University - New Brunswick. We have developed a\
set of software tools (<a href="https://radical-cybertools.github.io"\
class="">https://radical-cybertools.github.io) that are architected\
for scalable, interoperable and sustainable approaches to support\
science on a range of high-performance and distributed computing\
systems.<div class=""><br class=""><div class="">As we move forward\
and embrace new technologies, I intend to do with the present work a\
performance characterization of the execution of containerized\
scientific applications on HPC platforms using RADICAL-Cybertools\
(RCT). This will allow us to make decisions based on best software\
engineering practices and integrate them into RCT so we can continue\
to support our users in terms of all the benefits containers can\
bring.<div class=""><br class=""><div class="">Thank you so much for\
looking into this in such a quick and efficient fashion,<div\
class="">George Koubbe.<br class=""><br class=""><blockquote\
type="cite" class=""><div class="">On Apr 16, 2020, at 9:57 PM, Martin\
Kandes via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org&gt; wrote:<br class="Apple-interchange-\
newline"><div class=""><div class=""><br class="">Thu Apr 16 20:57:45\
2020: Request 133559 was acted upon.<br class=""> Transaction:\
Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Subject: Re: [<a\
href="http://tickets.xsede.org" class="">tickets.xsede.org #133559]\
Running containerized MPI applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mahidhar<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class=""> Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-/tickets/133559"\
class="">https://portal.xsede.org/group/xup/tickets/-/tickets/133559\
<br class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">I\'92ve almost solved this issue already. I\'92ll get back to you\
tomorrow with the results.<br class=""><br class="">Marty Kandes<br\
class="">SDSC User Services Group<br class=""><br class="">P.S. I\'92ll\
provide you solution here and on the Singularly Slack channel as\
someone else DM\'92d me and was curious about the issue too.<br\
class=""><br class="">Get Outlook for iOS&lt;<a\
href="https://aka.ms/o0ukef" class="">https://aka.ms/o0ukef&gt;<br\
class="">________________________________<br class="">From: Ellen\
Buskuehl via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org&gt;<br class="">Sent: Thursday, April 16, 2020\
6:39:27 PM<br class="">Subject: [<a href="http://tickets.xsede.org"\
class="">tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet<br class=""><br class=""><br class="">Thu\
Apr 16 20:39:26 2020: Request 133559 was acted upon.<br class="">\
Transaction: Queue changed from 0-Help to 0-SDSC by buskuehl<br\
class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br\
class=""> &nbsp;&nbsp;&nbsp;&nbsp;Subject: Running containerized MPI\
applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: buskuehl<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class=""> Ticket\
&lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28jILjf9U$\
&nbsp;&gt;<br class=""><br class=""><br class="">Hello,<br\
class=""><br class="">I am running a containerized MPI Hello World\
application on SDSC Comet.<br class=""><br class="">1. I just tried\
running an mpirun command directly on the \'93debug" partition of Comet,\
requesting 2 nodes and using the full 48 cores of said nodes.<br\
class="">2. I made sure the MPI implementation (Intel MPI 18.1) is the\
same for both containerized/non containerized \'93hello world\'94\
executable.<br class=""><br class=""><br class="">This is exactly what\
I do since I login to comet:<br class=""><br class="">For Intel MPI<br\
class=""><br class="">On login node:<br class="">$ module purge<br\
class="">$ module load intel singularity<br class="">$ singularity\
build centos-mpi.sif <a href="docker://centos:centos7"\
class="">docker://centos:centos7<br class="">$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$"\
class="">https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$<br\
class="">$ export\
PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH<br\
class="">$ export\
LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH<br\
class="">$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/<br\
class="">$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib<br\
class="">$ srun --partition=compute --pty --nodes=2 --ntasks-per-\
node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash<br class=""><br\
class="">On compute node:<br class="">$ module purge<br class="">$\
module load intel singularity<br class="">$ mpicc mpi_hello_world.c -o\
hello_world_intel<br class="">$ mpirun -n 48 singularity exec --bind\
/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64\
$HOME/centos-openmpi.sif $HOME/hello_world_intel<br class=""><br\
class=""><br class=""><br class="">For mvapich2:<br class=""><br\
class="">On login node:<br class="">$ module load singularity<br\
class="">$ singularity build centos-mpi.sif <a\
href="docker://centos:centos7" class="">docker://centos:centos7<br\
class="">$ wget <a\
href="https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$"\
class="">https://urldefense.com/v3/__https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-\
pages/tutorials/mpi-hello-\
world/code/mpi_hello_world.c__;!!Mih3wA!Wag1ea9cOMC7JDY6WAXLd7OPtIzotufBM9tCSOSw9Le1Ua_673BU3Z28rdmTinI$<br\
class="">$ export\
SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/<br class="">$\
export\
SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d<br\
class="">$ srun --partition=compute --pty --nodes=2 --ntasks-per-\
node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash<br class=""><br\
class="">On compute node:<br class="">$ mpicc mpi_hello_world.c -o\
hello_world_mpich<br class="">$ mpirun -n 48 singularity exec --bind\
/opt/mvapich2/intel/ib/ --bind /lib64 --bind\
/opt/intel/2018.1.163/lib/intel64 --bind /etc/libibverbs.d\
$HOME/centos-openmpi.sif $HOME/hello_world_mpich<br class=""><br\
class=""><br class=""><br class="">The Question:<br class="">I can\'92t\
find an explanation on why running \'93hello_world_intel\'94 inside the\
container is faster than outside, according only to the commands I\
typed of course. When I changed the MPI implementation to mvapich2,\
running the executable inside the container was slightly slower than\
outside (this is what I originally expected).<br class=""><br\
class=""><br class=""><br class="">The command I use to run outside\
the container is just:<br class="">$ mpirun -n 48\
$HOME/hello_world_intel<br class=""><br class=""><br class="">The\
command used to measure elapsed time was:<br class="">$ /usr/bin/time\
-v mpirun ... \'85 \'85<br class=""><br class=""><br class="">Just to give\
you exact numbers, when running with intel I got:<br class="">non-\
containerized: ~3.36 s<br class="">containerized: ~0.48 s<br\
class=""><br class="">When running with mvapich2, I got:<br\
class="">non-containerized: ~3.37 s<br class="">containerized: ~4.01\
s<br class=""><br class=""><br class=""><br class=""><br class="">Your\
help would be greatly appreciated, you don\'92t know what it would mean\
to me. Thank you!<br class="">George Koubbe.<br class=""><br\
class=""><br class=""><br class="">\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Fri Apr 17 19:15:18 2020\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I don\'92t\
know if I\'92ll get back to it tonight, but definitely will work on it\
again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Fri Apr 17 19:15:18 2020\
\
\
\
<meta http-equiv="Content-Type" content="text/html; charset=Windows-\
1252">\
\
\
\
\
<div style="direction: ltr;">Hi George,\
\
\
<div style="direction: ltr;">There\'92s still at least one more issue I\'92m\
trying to sort out. I don\'92t know if I\'92ll get back to it tonight, but\
definitely will work on it again tomorrow and keep you posted.\
\
\
<div style="direction: ltr;">Marty Kandes\
<div style="direction: ltr;">SDSC User Services Group\
\
\
\
<div class="ms-outlook-ios-signature">Get <a\
href="https://aka.ms/o0ukef">Outlook for iOS\
\
<hr style="display:inline-block;width:98%" tabindex="-1">\
<div id="divRplyFwdMsg" dir="ltr"><font face="Calibri, sans-serif"\
style="font-size:11pt" color="#000000">From: Mahidhar Tatineni via RT\
&lt;help@xsede.org&gt;\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin &lt;mkandes@sdsc.edu&gt;\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
applications on SDSC Comet\
&nbsp;\
\
<div class="BodyFragment"><font size="2"><span style="font-\
size:11pt;">\
<div class="PlainText">\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
&nbsp;Transaction: Given to mckandes by mahidhar\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Queue: 0-SDSC\
&nbsp;&nbsp;&nbsp;&nbsp; Subject: Running containerized MPI\
applications on SDSC Comet\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Owner: mckandes\
&nbsp; Requestors: glk35@scarletmail.rutgers.edu\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Status: open\
&nbsp;Ticket &lt;URL: <a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$">\
https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$&nbsp; &gt;\
\
This transaction appears to have no content\
\
\
\
\
\
------------------------------------------------\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Fri Apr 17 19:30:30 2020\
\
Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT \'a0wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 don\'92t know if I\'92ll get back to it tonight, but definitely will work on\
it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket <URL:\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Sat Apr 18 17:20:07 2020\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job, there is\
really no significant difference in the performance when everything is\
setup correctly. i.e., I think the low runtimes you reported in the\
Singularity Slack channel were probably either incorrect and/or not an\
apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know if you\
have any questions. Note, however, the container being used is not the\
same one you pulled from DockerHub. This one is our standard CentOS 7\
Singularity container. I needed a gcc compiler in the container and\
the DockerHub one does not have one installed by default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 don\'92t know if I\'92ll get back to it tonight, but definitely will work\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sat Apr 18 19:52:23 2020\
\
Martin,\
\
This is excellent news! I will look at it and try to replicate your\
setup...see where is it that I did something wrong. I will certainly\
let you know.\
\
Thank you so much,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT \'a0wrote:\
\
\uc0\u65279 \
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
As you can see in the standard output files from each job, there is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 really no significant difference in the performance when everything is\
setup correctly. i.e., I think the low runtimes you reported in the\
Singularity Slack channel were probably either incorrect and/or not an\
apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Anyhow, have a look at how I've set things up and let me know if you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 have any questions. Note, however, the container being used is not the\
same one you pulled from DockerHub. This one is our standard CentOS 7\
Singularity container. I needed a gcc compiler in the container and\
the DockerHub one does not have one installed by default.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 don\'92t know if I\'92ll get back to it tonight, but definitely will work\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket >\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sun Apr 19 16:57:50 2020\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected. However,\
I also replicated mine, but this time with the container centos.simg\
that you provide, and my experiments also yield the same results as\
yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled inside\
or outside the container\
2) The container I create is only through the command (I don\'92t do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the container you\
build and the one I pull, that makes such a drastic difference in the\
results?\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT \'a0wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
As you can see in the standard output files from each job, there is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 really no significant difference in the performance when everything is\
setup correctly. i.e., I think the low runtimes you reported in the\
Singularity Slack channel were probably either incorrect and/or not an\
apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Anyhow, have a look at how I've set things up and let me know if you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 have any questions. Note, however, the container being used is not the\
same one you pulled from DockerHub. This one is our standard CentOS 7\
Singularity container. I needed a gcc compiler in the container and\
the DockerHub one does not have one installed by default.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Thank you Martin, I appreciate your support.\
\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org #133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 MPI applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 don\'92t know if I\'92ll get back to it tonight, but definitely will work\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 it again tomorrow and keep you posted.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS\
________________________________\
From: Mahidhar Tatineni via RT\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin\
Subject: [tickets.xsede.org #133559] Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 applications on SDSC Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket >\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
This transaction appears to have no content\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sun Apr 19 16:57:50 2020\
\
<meta http-equiv="Content-Type" content="text/html; charset=utf-\
8"><body style="word-wrap: break-word; -webkit-nbsp-mode: space; line-\
break: after-white-space;" class="">Dear Martin,<div class=""><br\
class=""><div class="">I have narrowed down my problem to the\
container itself.<br class=""><div class=""><br class=""><div\
class="">I just replicated your experiments and it worked as expected.\
However, I also replicated mine, but this time with the container\
centos.simg that you provide, and my experiments also yield the same\
results as yours.<div class=""><br class=""><div class="">It\'92s worth\
noting that:<div class=""><br class=""><div class="">1) It did not\
make a difference if the executable is compiled inside or outside the\
container<div class="">2) The container I create is only through the\
command (I don\'92t do anything else): $&nbsp;singularity build\
centos7.sif <a href="docker://centos:centos7"\
class="">docker://centos:centos7<div class=""><br class=""><div\
class="">Having said this, what is the difference between the\
container you build and the one I pull, that makes such a drastic\
difference in the results?&nbsp;<div class=""><br class=""><div\
class="">Thank you,<div class="">George.<div class=""><br\
class=""><blockquote type="cite" class=""><div class="">On Apr 18,\
2020, at 6:20 PM, Martin Kandes via RT &lt;<a\
href="mailto:help@xsede.org" class="">help@xsede.org&gt; wrote:<br\
class="Apple-interchange-newline"><div class=""><div class=""><br\
class="">Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.<br\
class=""> Transaction: Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Subject: Running containerized MPI\
applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class=""> Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-/tickets/133559"\
class="">https://portal.xsede.org/group/xup/tickets/-/tickets/133559\
<br class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">I've placed a copy of my example batch job scripts that\
compile and run your MPI hello, world code here [1]. <br class=""><br\
class="">As you can see in the standard output files from each job,\
there is really no significant difference in the performance when\
everything is setup correctly. i.e., I think the low runtimes you\
reported in the Singularity Slack channel were probably either\
incorrect and/or not an apples-to-apples comparison. <br class=""><br\
class="">Anyhow, have a look at how I've set things up and let me know\
if you have any questions. Note, however, the container being used is\
not the same one you pulled from DockerHub. This one is our standard\
CentOS 7 Singularity container. I needed a gcc compiler in the\
container and the DockerHub one does not have one installed by\
default.<br class=""><br class="">Marty Kandes<br class="">SDSC User\
Services Group<br class=""><br class="">[1]<br class=""><br\
class="">/share/apps/examples/singularity/bind-host-compilers-mpi<br\
class=""><br class="">On Fri Apr 17 19:30:30 2020, <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu wrote:<br class=""><blockquote\
type="cite" class="">Thank you Martin, I appreciate your support.<br\
class=""><br class="">George.<br class=""><br class=""><blockquote\
type="cite" class="">On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
&lt;<a href="mailto:help@xsede.org" class="">help@xsede.org&gt;<br\
class="">wrote:<br class=""><blockquote type="cite" class=""><br\
class=""><br class="">Fri Apr 17 19:15:18 2020: Request 133559 was\
acted upon.<br class="">Transaction: Correspondence added by\
mckandes<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br\
class=""> &nbsp;&nbsp;&nbsp;Subject: Re: [<a\
href="http://tickets.xsede.org" class="">tickets.xsede.org #133559]\
Running containerized<br class="">MPI applications on SDSC Comet<br\
class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class=""> Requestors:\
<a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br\
class="">/tickets/133559<br class=""><blockquote type="cite"\
class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">There\'92s still at least one more issue I\'92m trying to sort out.\
I<br class="">don\'92t know if I\'92ll get back to it tonight, but\
definitely will work on<br class="">it again tomorrow and keep you\
posted.<br class=""><blockquote type="cite" class=""><br\
class="">Marty Kandes<br class="">SDSC User Services Group<br\
class=""><br class="">Get Outlook for iOS&lt;<a\
href="https://aka.ms/o0ukef" class="">https://aka.ms/o0ukef&gt;<br\
class="">________________________________<br class="">From: Mahidhar\
Tatineni via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org&gt;<br class="">Sent: Thursday, April 16, 2020\
10:19:20 PM<br class="">To: Kandes, Martin &lt;<a\
href="mailto:mkandes@sdsc.edu" class="">mkandes@sdsc.edu&gt;<br\
class="">Subject: [<a href="http://tickets.xsede.org"\
class="">tickets.xsede.org #133559] Running containerized MPI<br\
class="">applications on SDSC Comet<br class=""><blockquote\
type="cite" class=""><br class=""><br class="">Fri Apr 17 00:19:20\
2020: Request 133559 was acted upon.<br class="">Transaction: Given to\
mckandes by mahidhar<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue:\
0-SDSC<br class=""> &nbsp;&nbsp;&nbsp;Subject: Running containerized\
MPI applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class=""> Requestors:\
<a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket &lt;URL:<br\
class=""><a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
<br class="">2nOMdZ3ikPm94px2MBcFY$ &nbsp;&gt;<br class=""><blockquote\
type="cite" class=""><br class="">This transaction appears to have no\
content<br class=""><br class=""><br class=""><br class=""><br\
class=""><br class="">\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sun Apr 19 18:02:01 2020\
\
Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
have on your Github profile, under both the \'93centos\'94 and \'93naked-\
singularity\'94 repositories, but none of them worked (they produce my\
old results). It would be helpful if you could share the definition\
file you used for the container creation as well as the command you\
used to build it.\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 19, 2020, at 5:57 PM, George Koubbe \'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 However, I also replicated mine, but this time with the container\
centos.simg that you provide, and my experiments also yield the same\
results as yours.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled inside\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 or outside the container\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 2) The container I create is only through the command (I don\'92t do\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Having said this, what is the difference between the container you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 build and the one I pull, that makes such a drastic difference in the\
results?\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT <help@xsede.org >\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
As you can see in the standard output files from each job, there is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 really no significant difference in the performance when everything is\
setup correctly. i.e., I think the low runtimes you reported in the\
Singularity Slack channel were probably either incorrect and/or not an\
apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Anyhow, have a look at how I've set things up and let me know if\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you have any questions. Note, however, the container being used is not\
the same one you pulled from DockerHub. This one is our standard\
CentOS 7 Singularity container. I needed a gcc compiler in the\
container and the DockerHub one does not have one installed by\
default.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu \'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT <help@xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org \'a0#133559] Running\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 containerized\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
don\'92t know if I\'92ll get back to it tonight, but definitely will\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 work on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<https://aka.ms/o0ukef >\
________________________________\
From: Mahidhar Tatineni via RT <help@xsede.org >\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu >\
Subject: [tickets.xsede.org \'a0#133559] Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket >>\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Sun Apr 19 18:02:01 2020\
\
<meta http-equiv="Content-Type" content="text/html; charset=utf-\
8"><body style="word-wrap: break-word; -webkit-nbsp-mode: space; line-\
break: after-white-space;" class="">Martin,<div class=""><br\
class=""><div class="">Also, I have tried using the definition files\
for CentOS7 that you have on your Github profile, under both the\
\'93centos\'94 and \'93naked-singularity\'94 repositories, but none of them worked\
(they produce my old results). It would be helpful if you could share\
the definition file you used for the container creation as well as the\
command you used to build it.<div class=""><br class=""><div\
class="">Thank you,<div class="">George.<br class=""><br\
class=""><blockquote type="cite" class=""><div class="">On Apr 19,\
2020, at 5:57 PM, George Koubbe &lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu&gt; wrote:<br class="Apple-\
interchange-newline"><div class=""><meta http-equiv="Content-Type"\
content="text/html; charset=utf-8" class=""><div style="word-wrap:\
break-word; -webkit-nbsp-mode: space; line-break: after-white-space;"\
class="">Dear Martin,<div class=""><br class=""><div class="">I have\
narrowed down my problem to the container itself.<br class=""><div\
class=""><br class=""><div class="">I just replicated your experiments\
and it worked as expected. However, I also replicated mine, but this\
time with the container centos.simg that you provide, and my\
experiments also yield the same results as yours.<div class=""><br\
class=""><div class="">It\'92s worth noting that:<div class=""><br\
class=""><div class="">1) It did not make a difference if the\
executable is compiled inside or outside the container<div class="">2)\
The container I create is only through the command (I don\'92t do\
anything else): $&nbsp;singularity build centos7.sif <a\
href="docker://centos:centos7" class="">docker://centos:centos7<div\
class=""><br class=""><div class="">Having said this, what is the\
difference between the container you build and the one I pull, that\
makes such a drastic difference in the results?&nbsp;<div class=""><br\
class=""><div class="">Thank you,<div class="">George.<div\
class=""><div class=""><br class=""><blockquote type="cite"\
class=""><div class="">On Apr 18, 2020, at 6:20 PM, Martin Kandes via\
RT &lt;<a href="mailto:help@xsede.org" class="">help@xsede.org&gt;\
wrote:<br class="Apple-interchange-newline"><div class=""><div\
class=""><br class="">Sat Apr 18 17:20:07 2020: Request 133559 was\
acted upon.<br class=""> Transaction: Correspondence added by\
mckandes<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-\
SDSC<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;Subject: Running\
containerized MPI applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class=""> Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-/tickets/133559"\
class="">https://portal.xsede.org/group/xup/tickets/-/tickets/133559\
<br class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">I've placed a copy of my example batch job scripts that\
compile and run your MPI hello, world code here [1]. <br class=""><br\
class="">As you can see in the standard output files from each job,\
there is really no significant difference in the performance when\
everything is setup correctly. i.e., I think the low runtimes you\
reported in the Singularity Slack channel were probably either\
incorrect and/or not an apples-to-apples comparison. <br class=""><br\
class="">Anyhow, have a look at how I've set things up and let me know\
if you have any questions. Note, however, the container being used is\
not the same one you pulled from DockerHub. This one is our standard\
CentOS 7 Singularity container. I needed a gcc compiler in the\
container and the DockerHub one does not have one installed by\
default.<br class=""><br class="">Marty Kandes<br class="">SDSC User\
Services Group<br class=""><br class="">[1]<br class=""><br\
class="">/share/apps/examples/singularity/bind-host-compilers-mpi<br\
class=""><br class="">On Fri Apr 17 19:30:30 2020, <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu wrote:<br class=""><blockquote\
type="cite" class="">Thank you Martin, I appreciate your support.<br\
class=""><br class="">George.<br class=""><br class=""><blockquote\
type="cite" class="">On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
&lt;<a href="mailto:help@xsede.org" class="">help@xsede.org&gt;<br\
class="">wrote:<br class=""><blockquote type="cite" class=""><br\
class=""><br class="">Fri Apr 17 19:15:18 2020: Request 133559 was\
acted upon.<br class="">Transaction: Correspondence added by\
mckandes<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br\
class=""> &nbsp;&nbsp;&nbsp;Subject: Re: [<a\
href="http://tickets.xsede.org/" class="">tickets.xsede.org #133559]\
Running containerized<br class="">MPI applications on SDSC Comet<br\
class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class=""> Requestors:\
<a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br\
class="">/tickets/133559<br class=""><blockquote type="cite"\
class=""><br class=""><br class="">Hi George,<br class=""><br\
class="">There\'92s still at least one more issue I\'92m trying to sort out.\
I<br class="">don\'92t know if I\'92ll get back to it tonight, but\
definitely will work on<br class="">it again tomorrow and keep you\
posted.<br class=""><blockquote type="cite" class=""><br\
class="">Marty Kandes<br class="">SDSC User Services Group<br\
class=""><br class="">Get Outlook for iOS&lt;<a\
href="https://aka.ms/o0ukef" class="">https://aka.ms/o0ukef&gt;<br\
class="">________________________________<br class="">From: Mahidhar\
Tatineni via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org&gt;<br class="">Sent: Thursday, April 16, 2020\
10:19:20 PM<br class="">To: Kandes, Martin &lt;<a\
href="mailto:mkandes@sdsc.edu" class="">mkandes@sdsc.edu&gt;<br\
class="">Subject: [<a href="http://tickets.xsede.org/"\
class="">tickets.xsede.org #133559] Running containerized MPI<br\
class="">applications on SDSC Comet<br class=""><blockquote\
type="cite" class=""><br class=""><br class="">Fri Apr 17 00:19:20\
2020: Request 133559 was acted upon.<br class="">Transaction: Given to\
mckandes by mahidhar<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue:\
0-SDSC<br class=""> &nbsp;&nbsp;&nbsp;Subject: Running containerized\
MPI applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class=""> Requestors:\
<a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket &lt;URL:<br\
class=""><a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
<br class="">2nOMdZ3ikPm94px2MBcFY$ &nbsp;&gt;<br class=""><blockquote\
type="cite" class=""><br class="">This transaction appears to have no\
content<br class=""><br class=""><br class=""><br class=""><br\
class=""><br class=""><br class="">\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Mon Apr 20 11:41:51 2020\
\
Hi George,\
\
The container available in the examples directory I provided you was\
the one built using the latest version from the 'naked-singularity'\
repository. It's also available here on Comet [1]. The 'centos' one is\
a slightly updated version, but should not be significantly different\
in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones OS. As I\
already mentioned, it doesn't even have a gcc compiler and supporting\
libraries, which were needed to support the Intel compilers and\
IntelMPI being mounted in the container. And yes, it shouldn't really\
make a difference between compiling within the container or outside\
the container as the software environments should be pretty close in\
nature. However, if I compile and run a code within a container, I\
usually just compile inside the container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
CentOS 7 container that might been causing a problem. What version of\
Singularity are you building the container with? Is it built on a\
Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
\'a0\'a0have on your Github profile, under both the \'93centos\'94 and \'93naked-\
\'a0\'a0singularity\'94 repositories, but none of them worked (they produce\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 my\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0old results). It would be helpful if you could share the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 definition\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0file you used for the container creation as well as the command\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0used to build it.\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 19, 2020, at 5:57 PM, George Koubbe\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0However, I also replicated mine, but this time with the container\
\'a0\'a0centos.simg that you provide, and my experiments also yield the\
\'a0\'a0same results as yours.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 inside\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0or outside the container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 2) The container I create is only through the command (I don\'92t do\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0anything else): $ singularity build centos7.sif\
\'a0\'a0docker://centos:centos7\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Having said this, what is the difference between the container you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0build and the one I pull, that makes such a drastic difference in\
\'a0\'a0the results?\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT \'a0\'a0\'a0\'a0> wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559 \'a0\'a0\'a0\'a0/tickets/133559>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
As you can see in the standard output files from each job, there\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 is\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0really no significant difference in the performance when\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 everything\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is setup correctly. i.e., I think the low runtimes you reported\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 in\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the Singularity Slack channel were probably either incorrect\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and/or\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0not an apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Anyhow, have a look at how I've set things up and let me know if\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you have any questions. Note, however, the container being used\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 is\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0not the same one you pulled from DockerHub. This one is our\
\'a0\'a0standard CentOS 7 Singularity container. I needed a gcc compiler\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 in\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the container and the DockerHub one does not have one installed\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 by\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0default.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT \'a0\'a0\'a0\'a0>\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0#133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 /tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
don\'92t know if I\'92ll get back to it tonight, but definitely will\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0work on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS \'a0\'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0\'a0\'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu >\
Subject: [tickets.xsede.org \'a0#133559]\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket \'a0>>>\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Mon Apr 20 19:00:13 2020\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based formatted\
container. Can you provide we with a path to your working directory on\
Comet where I can review the output files for your tests? There might\
be some clues as to why you're seeing a different runtime.\
\
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Hi George,\
\
The container available in the examples directory I provided you was\
\'a0\'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity'\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0repository. It's also available here on Comet [1]. The 'centos'\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 one\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is a slightly updated version, but should not be significantly\
\'a0\'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0\'a0DockerHub is that the one from DockerHub is a very barebones OS.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 As\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0\'a0supporting libraries, which were needed to support the Intel\
\'a0\'a0compilers and IntelMPI being mounted in the container. And yes,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 it\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0shouldn't really make a difference between compiling within the\
\'a0\'a0container or outside the container as the software environments\
\'a0\'a0should be pretty close in nature. However, if I compile and run a\
\'a0\'a0code within a container, I usually just compile inside the\
\'a0\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
\'a0\'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 version\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0of Singularity are you building the container with? Is it built\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
\'a0\'a0have on your Github profile, under both the \'93centos\'94 and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \'93naked-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0singularity\'94 repositories, but none of them worked (they\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 produce\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0my\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0old results). It would be helpful if you could share the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0definition\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0file you used for the container creation as well as the command\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0used to build it.\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 19, 2020, at 5:57 PM, George Koubbe\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0centos.simg that you provide, and my experiments also yield the\
\'a0\'a0same results as yours.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0inside\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0or outside the container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 2) The container I create is only through the command (I don\'92t\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 do\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0anything else): $ singularity build centos7.sif\
\'a0\'a0docker://centos:centos7\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Having said this, what is the difference between the container\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0build and the one I pull, that makes such a drastic difference\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 in\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0the results?\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT \'a0> \'a0\'a0\'a0>\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0/tickets/133559 \'a0> \'a0\'a0\'a0/tickets/133559>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0and\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
As you can see in the standard output files from each job,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 there\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0really no significant difference in the performance when\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0everything\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0is setup correctly. i.e., I think the low runtimes you reported\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0the Singularity Slack channel were probably either incorrect\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0and/or\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0not an apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Anyhow, have a look at how I've set things up and let me know\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 if\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0you have any questions. Note, however, the container being used\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0not the same one you pulled from DockerHub. This one is our\
\'a0\'a0standard CentOS 7 Singularity container. I needed a gcc\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compiler\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0the container and the DockerHub one does not have one installed\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0by\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0default.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0#133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 /tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely will\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0work on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS > \'a0\'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0> \'a0\'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu >\
Subject: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0#133559]\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket \'a0> >>>\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Mon Apr 20 19:09:55 2020\
\
Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\
$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT \'a0wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based formatted\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container. Can you provide we with a path to your working directory on\
Comet where I can review the output files for your tests? There might\
be some clues as to why you're seeing a different runtime.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Hi George,\
\
The container available in the examples directory I provided you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 was\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity'\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0repository. It's also available here on Comet [1]. The 'centos'\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 one\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0is a slightly updated version, but should not be significantly\
\'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0DockerHub is that the one from DockerHub is a very barebones OS.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 As\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0supporting libraries, which were needed to support the Intel\
\'a0compilers and IntelMPI being mounted in the container. And yes,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 it\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0shouldn't really make a difference between compiling within the\
\'a0container or outside the container as the software environments\
\'a0should be pretty close in nature. However, if I compile and run a\
\'a0code within a container, I usually just compile inside the\
\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
\'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 version\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0of Singularity are you building the container with? Is it built\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
\'a0have on your Github profile, under both the \'93centos\'94 and \'93naked-\
\'a0singularity\'94 repositories, but none of them worked (they produce\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0my\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0old results). It would be helpful if you could share the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0definition\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0file you used for the container creation as well as the command\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0centos.simg that you provide, and my experiments also yield the\
\'a0same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0inside\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0or outside the container\
2) The container I create is only through the command (I don\'92t do\
\'a0anything else): $ singularity build centos7.sif\
\'a0docker://centos:centos7\
\
Having said this, what is the difference between the container\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0build and the one I pull, that makes such a drastic difference\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT >> \'a0\'a0> wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559 >> \'a0\'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0and\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job, there\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0is\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0really no significant difference in the performance when\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0everything\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is setup correctly. i.e., I think the low runtimes you reported\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the Singularity Slack channel were probably either incorrect\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0and/or\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know if\
\'a0you have any questions. Note, however, the container being used\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0is\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0not the same one you pulled from DockerHub. This one is our\
\'a0standard CentOS 7 Singularity container. I needed a gcc compiler\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the container and the DockerHub one does not have one installed\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0by\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Re: [tickets.xsede.org\
\'a0#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely will\
\'a0work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS>> \'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT >> \'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu >\
Subject: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0#133559]\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket >>>>>\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Mon Apr 20 19:09:55 2020\
\
<meta http-equiv="Content-Type" content="text/html; charset=utf-\
8"><body style="word-wrap: break-word; -webkit-nbsp-mode: space; line-\
break: after-white-space;" class="">Martin,<div class=""><br\
class=""><div class="">Definitely, the path to my current working\
directory is:<div class=""><br class=""><div\
class="">/home/karahbit/test/not_working<div class=""><br\
class=""><div class="">Once there, the exact commands I am running to\
see results are:<div class=""><br class=""><div class=""><div\
class="">$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24\
-t 00:30:00 --wait=0 --export=ALL /bin/bash<div class=""><br\
class=""><div class="">$ module purge<div class=""><br class=""><div\
class="">$ module load intel intelmpi<div class=""><br class=""><div\
class="">$ export\
SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64<div\
class=""><br class=""><div class="">$ export\
SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4<div\
class=""><br class=""><div class="">$ time -p mpirun singularity exec\
--bind /opt centos-repo.simg ./hello_world_intel<div class=""><br\
class=""><div class=""><br class=""><div class="">And I see the\
following result:<div class=""><br class=""><div class=""><img apple-\
inline="yes" id="8D362959-506F-4DF4-B489-27B2A381763F"\
src="cid:D3CB1005-818D-4A4E-BF8D-22B8E42FDE9C@fios-router.home"\
class=""><div class=""><br class=""><div class="">I appreciate your\
support on this matter,<div class="">George.<div class=""><br\
class=""><blockquote type="cite" class=""><div class="">On Apr 20,\
2020, at 8:00 PM, Martin Kandes via RT &lt;<a\
href="mailto:help@xsede.org" class="">help@xsede.org&gt; wrote:<br\
class="Apple-interchange-newline"><div class=""><div class=""><br\
class="">Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.<br\
class=""> Transaction: Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Subject: Running containerized MPI\
applications on SDSC Comet<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class="">\
&nbsp;Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Status: user_wait<br class=""> Ticket\
URL: <a href="https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559" class="">https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 <br class=""><br class=""><br class="">George,<br\
class=""><br class="">I'm still not seeing a problem using a newer\
*.sif-based formatted container. Can you provide we with a path to\
your working directory on Comet where I can review the output files\
for your tests? There might be some clues as to why you're seeing a\
different runtime.<br class=""><br class="">Thanks,<br class=""><br\
class="">Marty Kandes<br class="">SDSC User Services Group<br\
class=""><br class="">On Mon Apr 20 11:41:51 2020, mckandes wrote:<br\
class=""><blockquote type="cite" class="">Hi George,<br class=""><br\
class="">The container available in the examples directory I provided\
you was<br class=""> &nbsp;&nbsp;the one built using the latest\
version from the 'naked-singularity'<br class="">\
&nbsp;&nbsp;repository. It's also available here on Comet [1]. The\
'centos' one<br class=""> &nbsp;&nbsp;is a slightly updated version,\
but should not be significantly<br class=""> &nbsp;&nbsp;different in\
functionality.<br class=""><br class="">The primary difference between\
our container and the one from<br class=""> &nbsp;&nbsp;DockerHub is\
that the one from DockerHub is a very barebones OS. As<br class="">\
&nbsp;&nbsp;I already mentioned, it doesn't even have a gcc compiler\
and<br class=""> &nbsp;&nbsp;supporting libraries, which were needed\
to support the Intel<br class=""> &nbsp;&nbsp;compilers and IntelMPI\
being mounted in the container. And yes, it<br class="">\
&nbsp;&nbsp;shouldn't really make a difference between compiling\
within the<br class=""> &nbsp;&nbsp;container or outside the container\
as the software environments<br class=""> &nbsp;&nbsp;should be pretty\
close in nature. However, if I compile and run a<br class="">\
&nbsp;&nbsp;code within a container, I usually just compile inside\
the<br class=""> &nbsp;&nbsp;container as it's the safer option.<br\
class=""><br class="">Anyhow, I'm not quite sure what went wrong with\
your build of our<br class=""> &nbsp;&nbsp;CentOS 7 container that\
might been causing a problem. What version<br class=""> &nbsp;&nbsp;of\
Singularity are you building the container with? Is it built on<br\
class=""> &nbsp;&nbsp;a Linux system?<br class=""><br class="">Marty\
Kandes<br class="">SDSC User Services Group<br class=""><br\
class="">[1]<br class=""><br class="">[mkandes@comet-ln2 ~]$ ls<br\
class="">\
&nbsp;&nbsp;/share/apps/compute/singularity/images/centos/<br\
class="">centos-openmpi.simg<br class="">centos.simg<br\
class="">centos-v6.10-20190916.simg<br class="">centos-v7.6.1810-\
20190513.simg<br class="">centos-v7.6.1810-openmpi-v1.8.4-\
20190514.simg<br class="">centos-v7.7.1908-20190914.simg<br\
class="">centos-v7.7.1908-20190919.simg<br class="">centos-v7.7.1908-\
20200129.simg<br class="">centos-v7.7.1908-openmpi-v1.8.4-\
20190919.simg<br class="">centos-v7.7.1908-openmpi-v3.1.4-\
20200211.simg<br class="">[mkandes@comet-ln2 ~]$<br class=""><br\
class="">On Sun Apr 19 18:02:01 2020, <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu wrote:<br class=""><blockquote\
type="cite" class="">Martin,<br class=""><br class="">Also, I have\
tried using the definition files for CentOS7 that you<br class="">\
&nbsp;&nbsp;have on your Github profile, under both the \'93centos\'94 and\
\'93naked-<br class=""> &nbsp;&nbsp;singularity\'94 repositories, but none\
of them worked (they produce<br class=""> &nbsp;&nbsp;my<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;old results).\
It would be helpful if you could share the<br class="">\
&nbsp;&nbsp;definition<br class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;file you used for the container creation as well as the\
command<br class=""> &nbsp;&nbsp;you<br class=""><blockquote\
type="cite" class=""> &nbsp;&nbsp;used to build it.<br class=""><br\
class="">Thank you,<br class="">George.<br class=""><br\
class=""><blockquote type="cite" class="">On Apr 19, 2020, at 5:57 PM,\
George Koubbe<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu&gt; wrote:<br\
class=""><blockquote type="cite" class=""><br class="">Dear Martin,<br\
class=""><br class="">I have narrowed down my problem to the container\
itself.<br class=""><br class="">I just replicated your experiments\
and it worked as expected.<br class=""> &nbsp;&nbsp;However, I also\
replicated mine, but this time with the container<br class="">\
&nbsp;&nbsp;centos.simg that you provide, and my experiments also\
yield the<br class=""> &nbsp;&nbsp;same results as yours.<br\
class=""><blockquote type="cite" class=""><br class="">It\'92s worth\
noting that:<br class=""><br class="">1) It did not make a difference\
if the executable is compiled<br class=""> &nbsp;&nbsp;inside<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;or outside the\
container<br class=""><blockquote type="cite" class="">2) The\
container I create is only through the command (I don\'92t do<br\
class=""> &nbsp;&nbsp;anything else): $ singularity build\
centos7.sif<br class=""> &nbsp;&nbsp;<a href="docker://centos:centos7"\
class="">docker://centos:centos7 &lt;<a href="docker://centos:centos7"\
class="">docker://centos:centos7&gt;<br class=""><blockquote\
type="cite" class=""><br class="">Having said this, what is the\
difference between the container you<br class=""> &nbsp;&nbsp;build\
and the one I pull, that makes such a drastic difference in<br\
class=""> &nbsp;&nbsp;the results?<br class=""><blockquote type="cite"\
class=""><br class="">Thank you,<br class="">George.<br class=""><br\
class=""><blockquote type="cite" class="">On Apr 18, 2020, at 6:20 PM,\
Martin Kandes via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:help@xsede.org" class="">mailto:help@xsede.org&gt;&gt;\
wrote:<br class=""><blockquote type="cite" class=""><blockquote\
type="cite" class=""><br class=""><br class="">Sat Apr 18 17:20:07\
2020: Request 133559 was acted upon.<br class="">Transaction:\
Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;&nbsp;Subject: Running containerized MPI applications on\
SDSC Comet<br class=""> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Owner:\
mckandes<br class=""> Requestors: <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">mailto:glk35@scarletmail.rutgers.edu&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""> &nbsp;&nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket URL:\
<a href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br class="">\
&nbsp;&nbsp;/tickets/133559 &lt;<a\
href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br class="">\
&nbsp;&nbsp;/tickets/133559&gt;<br class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><br class=""><br class="">Hi\
George,<br class=""><br class="">I've placed a copy of my example\
batch job scripts that compile<br class=""> &nbsp;&nbsp;and<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;run your MPI\
hello, world code here [1].<br class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><br class="">As you can see\
in the standard output files from each job, there<br class="">\
&nbsp;&nbsp;is<br class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;really no significant difference in the performance\
when<br class=""> &nbsp;&nbsp;everything<br class=""><blockquote\
type="cite" class=""> &nbsp;&nbsp;is setup correctly. i.e., I think\
the low runtimes you reported<br class=""> &nbsp;&nbsp;in<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;the Singularity\
Slack channel were probably either incorrect<br class="">\
&nbsp;&nbsp;and/or<br class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;not an apples-to-apples comparison.<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><br class="">Anyhow, have a look at how I've set things up\
and let me know if<br class=""> &nbsp;&nbsp;you have any questions.\
Note, however, the container being used<br class=""> &nbsp;&nbsp;is<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;not the same\
one you pulled from DockerHub. This one is our<br class="">\
&nbsp;&nbsp;standard CentOS 7 Singularity container. I needed a gcc\
compiler<br class=""> &nbsp;&nbsp;in<br class=""><blockquote\
type="cite" class=""> &nbsp;&nbsp;the container and the DockerHub one\
does not have one installed<br class=""> &nbsp;&nbsp;by<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;default.<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><br class="">Marty Kandes<br class="">SDSC User Services\
Group<br class=""><br class="">[1]<br class=""><br\
class="">/share/apps/examples/singularity/bind-host-compilers-mpi<br\
class=""><br class="">On Fri Apr 17 19:30:30 2020, <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">mailto:glk35@scarletmail.rutgers.edu&gt; wrote:<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class="">Thank you Martin, I\
appreciate your support.<br class=""><br class="">George.<br\
class=""><br class=""><blockquote type="cite" class="">On Apr 17,\
2020, at 8:15 PM, Martin Kandes via RT<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:help@xsede.org" class="">help@xsede.org<br\
class=""><blockquote type="cite" class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:help@xsede.org" class="">mailto:help@xsede.org&gt;&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class="">wrote:<br\
class=""><blockquote type="cite" class=""><br class=""><br\
class="">Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.<br\
class="">Transaction: Correspondence added by mckandes<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;Subject: Re: [<a href="http://tickets.xsede.org"\
class="">tickets.xsede.org &lt;<a href="http://tickets.xsede.org/"\
class="">http://tickets.xsede.org/&gt;<br class="">\
&nbsp;&nbsp;#133559] Running containerized<br class=""><blockquote\
type="cite" class=""><blockquote type="cite" class=""><blockquote\
type="cite" class="">MPI applications on SDSC Comet<br\
class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br class="">Requestors: <a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">mailto:glk35@scarletmail.rutgers.edu&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""> &nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket URL: <a\
href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-<br class="">\
&nbsp;&nbsp;&lt;<a href="https://portal.xsede.org/group/xup/tickets/-"\
class="">https://portal.xsede.org/group/xup/tickets/-&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class="">/tickets/133559<br\
class=""><blockquote type="cite" class=""><br class=""><br class="">Hi\
George,<br class=""><br class="">There\'92s still at least one more issue\
I\'92m trying to sort out. I<br class="">don\'92t know if I\'92ll get back to\
it tonight, but definitely will<br class=""> &nbsp;&nbsp;work on<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class="">it again tomorrow and keep\
you posted.<br class=""><blockquote type="cite" class=""><br\
class="">Marty Kandes<br class="">SDSC User Services Group<br\
class=""><br class="">Get Outlook for iOS&lt;<a\
href="https://aka.ms/o0ukef" class="">https://aka.ms/o0ukef<br\
class=""> &nbsp;&nbsp;&lt;<a href="https://aka.ms/o0ukef"\
class="">https://aka.ms/o0ukef&gt;&gt;<br class=""><blockquote\
type="cite" class=""><blockquote type="cite" class=""><blockquote\
type="cite" class=""><blockquote type="cite"\
class="">________________________________<br class="">From: Mahidhar\
Tatineni via RT &lt;<a href="mailto:help@xsede.org"\
class="">help@xsede.org<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:help@xsede.org" class="">mailto:help@xsede.org&gt;&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class="">Sent: Thursday, April 16, 2020 10:19:20 PM<br class="">To:\
Kandes, Martin &lt;<a href="mailto:mkandes@sdsc.edu"\
class="">mkandes@sdsc.edu &lt;<a href="mailto:mkandes@sdsc.edu"\
class="">mailto:mkandes@sdsc.edu&gt;&gt;<br class="">Subject: [<a\
href="http://tickets.xsede.org" class="">tickets.xsede.org &lt;<a\
href="http://tickets.xsede.org/"\
class="">http://tickets.xsede.org/&gt;<br class="">\
&nbsp;&nbsp;#133559]<br class=""><blockquote type="cite" class="">\
&nbsp;&nbsp;Running containerized MPI<br class=""><blockquote\
type="cite" class=""><blockquote type="cite" class=""><blockquote\
type="cite" class="">applications on SDSC Comet<br\
class=""><blockquote type="cite" class=""><br class=""><br\
class="">Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.<br\
class="">Transaction: Given to mckandes by mahidhar<br class="">\
&nbsp;&nbsp;&nbsp;&nbsp;Queue: 0-SDSC<br class="">\
&nbsp;&nbsp;Subject: Running containerized MPI applications on SDSC<br\
class=""> &nbsp;&nbsp;Comet<br class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""> &nbsp;&nbsp;&nbsp;&nbsp;Owner: mckandes<br\
class="">Requestors: <a href="mailto:glk35@scarletmail.rutgers.edu"\
class="">glk35@scarletmail.rutgers.edu<br class=""> &nbsp;&nbsp;&lt;<a\
href="mailto:glk35@scarletmail.rutgers.edu"\
class="">mailto:glk35@scarletmail.rutgers.edu&gt;<br\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""> &nbsp;&nbsp;&nbsp;Status: open<br class="">Ticket\
&lt;URL:<br class=""><br class=""><br class=""> &nbsp;&nbsp;<a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
<br class=""><blockquote type="cite" class=""><br class="">\
&nbsp;&nbsp;&lt;<a\
href="https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
"\
class="">https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
<br class=""><blockquote type="cite" class=""><blockquote type="cite"\
class=""><br class=""><blockquote type="cite" class=""><blockquote\
type="cite" class="">2nOMdZ3ikPm94px2MBcFY$ &nbsp;&gt;<br\
class=""><blockquote type="cite" class=""><br class="">This\
transaction appears to have no content<br class=""><br class=""><br\
class=""><br class=""><br class=""><br class=""><br class=""><br\
class=""><br class=""><br class=""><br class=""><br class="">\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Tue Apr 21 17:51:28 2020\
\
Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge there\
is some sort of performance difference between the 3 containers I'm\
testing with here, one of them is your version of the container. But\
each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World test\
problem with your container [1]. Your numbers are the lower runtimes\
in the middle. But as you can see, they do fluctuate to the average as\
well for all containers.\
\
To move away from the simple Hello, World test problem, I then ran the\
OSU Microbenchmarks for both bandwidth [2] and latency [3]. As you can\
see here, the results are more consistent, with your container\
underperforming. The fundamental problem seems to be that your\
container does not properly utilize Comet's infiniband network. This\
can be seen in the high latency [4]. The latency should be on the\
order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only difference\
I see right now is that your container was built with Singularity\
3.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2? Also, how\
did you install your copy of Singularity 3.5.0? Did you compile from\
source? Or is that a binary executable that you downloaded and\
installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-singularity-\
omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-singularity-\
omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a0\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based formatted\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container. Can you provide we with a path to your working\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 directory\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on Comet where I can review the output files for your tests?\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 There\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0might be some clues as to why you're seeing a different runtime.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Hi George,\
\
The container available in the examples directory I provided you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0was\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0singularity'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0repository. It's also available here on Comet [1]. The 'centos'\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0one\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is a slightly updated version, but should not be significantly\
\'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0DockerHub is that the one from DockerHub is a very barebones\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 OS.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0As\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0supporting libraries, which were needed to support the Intel\
\'a0compilers and IntelMPI being mounted in the container. And yes,\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0it\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0shouldn't really make a difference between compiling within the\
\'a0container or outside the container as the software environments\
\'a0should be pretty close in nature. However, if I compile and run\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 a\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0code within a container, I usually just compile inside the\
\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
\'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0version\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0of Singularity are you building the container with? Is it built\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
Martin,\
\
Also, I have tried using the definition files for CentOS7 that\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0have on your Github profile, under both the \'93centos\'94 and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \'93naked-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0singularity\'94 repositories, but none of them worked (they\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 produce\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0my\
\'a0old results). It would be helpful if you could share the\
\'a0definition\
\'a0file you used for the container creation as well as the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 command\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you\
\'a0used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0centos.simg that you provide, and my experiments also yield\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\'a0inside\
\'a0or outside the container\
2) The container I create is only through the command (I don\'92t\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 do\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0anything else): $ singularity build centos7.sif\
\'a0docker://centos:centos7\
\
Having said this, what is the difference between the container\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0build and the one I pull, that makes such a drastic difference\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559 \'a0>>> \'a0\'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compile\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0and\
\'a0run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is\
\'a0really no significant difference in the performance when\
\'a0everything\
\'a0is setup correctly. i.e., I think the low runtimes you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 reported\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
\'a0the Singularity Slack channel were probably either incorrect\
\'a0and/or\
\'a0not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 if\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you have any questions. Note, however, the container being\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 used\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is\
\'a0not the same one you pulled from DockerHub. This one is our\
\'a0standard CentOS 7 Singularity container. I needed a gcc\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compiler\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
\'a0the container and the DockerHub one does not have one\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 installed\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0by\
\'a0default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Re: [tickets.xsede.org\
\'a0#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 out.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 will\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS >>> \'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0>>> \'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin \'a0\'a0\'a0\'a0>\
Subject: [tickets.xsede.org\
\'a0#133559]\
\'a0Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\'a0Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket \'a0>>>>>>\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Tue Apr 21 18:04:08 2020\
\
George,\
\
A side note: I finally had a look over the project you're working on.\
It looks pretty interesting. So is the idea here that you might --bind\
mount the local compilers/mpi on a resource into users containers when\
the container runs on that resource? If so, I'm still thinking this\
would be more complicated to support than simply having a set of\
static containers to choose from that would be compatible the\
compiler/mpi you find/discover at a resource. But maybe I'm missing\
something here.\
\
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 there\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0is some sort of performance difference between the 3 containers\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 I'm\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0testing with here, one of them is your version of the container.\
\'a0\'a0But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World test\
\'a0\'a0problem with your container [1]. Your numbers are the lower\
\'a0\'a0runtimes in the middle. But as you can see, they do fluctuate to\
\'a0\'a0the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then ran\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0can see here, the results are more consistent, with your\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0underperforming. The fundamental problem seems to be that your\
\'a0\'a0container does not properly utilize Comet's infiniband network.\
\'a0\'a0This can be seen in the high latency [4]. The latency should be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I see right now is that your container was built with Singularity\
\'a0\'a03.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2? Also,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 how\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0did you install your copy of Singularity 3.5.0? Did you compile\
\'a0\'a0from source? Or is that a binary executable that you downloaded\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0\'a0singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a0\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 formatted\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0container. Can you provide we with a path to your working\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0directory\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0on Comet where I can review the output files for your tests?\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0There\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0was\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0singularity'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 'centos'\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0one\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is a slightly updated version, but should not be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 significantly\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0DockerHub is that the one from DockerHub is a very barebones\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0OS.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0As\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0supporting libraries, which were needed to support the Intel\
\'a0compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 yes,\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0it\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0a\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0code within a container, I usually just compile inside the\
\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 our\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0version\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 built\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7 that\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0have on your Github profile, under both the \'93centos\'94 and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0\'93naked-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0singularity\'94 repositories, but none of them worked (they\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0produce\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0my\
\'a0old results). It would be helpful if you could share the\
\'a0definition\
\'a0file you used for the container creation as well as the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0command\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you\
\'a0used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0centos.simg that you provide, and my experiments also yield\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\'a0inside\
\'a0or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 don\'92t\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0do\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0anything else): $ singularity build centos7.sif\
\'a0docker://centos:centos7\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559 \'a0> >>> \'a0\'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0compile\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0and\
\'a0run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is\
\'a0really no significant difference in the performance when\
\'a0everything\
\'a0is setup correctly. i.e., I think the low runtimes you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0reported\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
\'a0the Singularity Slack channel were probably either incorrect\
\'a0and/or\
\'a0not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 know\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0if\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you have any questions. Note, however, the container being\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0used\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is\
\'a0not the same one you pulled from DockerHub. This one is our\
\'a0standard CentOS 7 Singularity container. I needed a gcc\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0compiler\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
\'a0the container and the DockerHub one does not have one\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0installed\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0by\
\'a0default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Re: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0out.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'a0I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0will\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS > >>> \'a0\'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0> >>> \'a0\'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin \'a0> \'a0\'a0\'a0>\
Subject: [tickets.xsede.org\
\'a0#133559]\
\'a0Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\'a0Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket \'a0> >>>>>>\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Tue Apr 21 18:40:57 2020\
\
Martin,\
\
This issue is very interesting indeed. It was nice of you to even go\
further and perform bandwidth and latency tests, which confirms the\
considerable performance difference between our containers, although\
we have not been able to locate the source of the difference so far. I\
do build with Singularity 3.5.0 installed from source. What is the\
exact command you use for building?\
\
And you are right regarding your intuition of what we want to do. We\
are still exploring the options of how we are going to distribute the\
containers to end users depending on the application they try to run.\
Of course, as you point out, the bindings would need to be specific to\
the remote resource.\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT \'a0wrote:\
\
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
George,\
\
A side note: I finally had a look over the project you're working\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on. It looks pretty interesting. So is the idea here that you might\
--bind mount the local compilers/mpi on a resource into users\
containers when the container runs on that resource? If so, I'm still\
thinking this would be more complicated to support than simply having\
a set of static containers to choose from that would be compatible the\
compiler/mpi you find/discover at a resource. But maybe I'm missing\
something here.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 there\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0is some sort of performance difference between the 3 containers\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 I'm\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0testing with here, one of them is your version of the container.\
\'a0But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World test\
\'a0problem with your container [1]. Your numbers are the lower\
\'a0runtimes in the middle. But as you can see, they do fluctuate to\
\'a0the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then ran\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0can see here, the results are more consistent, with your\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0underperforming. The fundamental problem seems to be that your\
\'a0container does not properly utilize Comet's infiniband network.\
\'a0This can be seen in the high latency [4]. The latency should be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 on\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0I see right now is that your container was built with Singularity\
\'a03.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2? Also,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 how\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0did you install your copy of Singularity 3.5.0? Did you compile\
\'a0from source? Or is that a binary executable that you downloaded\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 singularity-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\'a0wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 formatted\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0container. Can you provide we with a path to your working\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0directory\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0on Comet where I can review the output files for your tests?\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0There\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0might be some clues as to why you're seeing a different runtime.\
\
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided you\
\'a0was\
the one built using the latest version from the 'naked-\
\'a0singularity'\
repository. It's also available here on Comet [1]. The 'centos'\
\'a0one\
is a slightly updated version, but should not be significantly\
different in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0OS.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And yes,\
\'a0it\
shouldn't really make a difference between compiling within the\
container or outside the container as the software environments\
should be pretty close in nature. However, if I compile and run\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0a\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 our\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 CentOS 7 container that might been causing a problem. What\
\'a0version\
of Singularity are you building the container with? Is it built\
\'a0on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7 that\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 have on your Github profile, under both the \'93centos\'94 and\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0\'93naked-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 singularity\'94 repositories, but none of them worked (they\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0produce\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0command\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
However, I also replicated mine, but this time with the\
\'a0container\
centos.simg that you provide, and my experiments also yield\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
inside\
or outside the container\
2) The container I create is only through the command (I don\'92t\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0do\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the container\
\'a0you\
build and the one I pull, that makes such a drastic difference\
\'a0in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 >>>>> \'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0compile\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0reported\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the Singularity Slack channel were probably either incorrect\
and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0if\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you have any questions. Note, however, the container being\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0used\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0compiler\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the container and the DockerHub one does not have one\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0installed\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 #133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0out.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I\
don\'92t know if I\'92ll get back to it tonight, but definitely\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0will\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS>>>>> \'a0>\
________________________________\
From: Mahidhar Tatineni via RT >>>>> \'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin >> \'a0\'a0>\
Subject: [tickets.xsede.org\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket >>>>>>>>\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Wed Apr 22 11:36:13 2020\
\
George,\
\
I build the containers using the build command: sudo singularity build\
containter.simg container.deffile\
\
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 18:40:57 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
This issue is very interesting indeed. It was nice of you to even go\
\'a0\'a0further and perform bandwidth and latency tests, which confirms\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0considerable performance difference between our containers,\
\'a0\'a0although we have not been able to locate the source of the\
\'a0\'a0difference so far. I do build with Singularity 3.5.0 installed\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 from\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0source. What is the exact command you use for building?\
\
And you are right regarding your intuition of what we want to do. We\
\'a0\'a0are still exploring the options of how we are going to distribute\
\'a0\'a0the containers to end users depending on the application they try\
\'a0\'a0to run. Of course, as you point out, the bindings would need to\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 be\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0specific to the remote resource.\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
George,\
\
A side note: I finally had a look over the project you're working\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on. It looks pretty interesting. So is the idea here that you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 might\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0--bind mount the local compilers/mpi on a resource into users\
\'a0\'a0containers when the container runs on that resource? If so, I'm\
\'a0\'a0still thinking this would be more complicated to support than\
\'a0\'a0simply having a set of static containers to choose from that\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 would\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0be compatible the compiler/mpi you find/discover at a resource.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 But\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0maybe I'm missing something here.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is some sort of performance difference between the 3 containers\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I'm\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0testing with here, one of them is your version of the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 test\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0problem with your container [1]. Your numbers are the lower\
\'a0runtimes in the middle. But as you can see, they do fluctuate\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 to\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 ran\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0can see here, the results are more consistent, with your\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0underperforming. The fundamental problem seems to be that your\
\'a0container does not properly utilize Comet's infiniband network.\
\'a0This can be seen in the high latency [4]. The latency should be\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I see right now is that your container was built with\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Singularity\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a03.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2?\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Also,\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0how\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0did you install your copy of Singularity 3.5.0? Did you compile\
\'a0from source? Or is that a binary executable that you downloaded\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0and\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0singularity-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0singularity-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0formatted\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0container. Can you provide we with a path to your working\
\'a0directory\
\'a0on Comet where I can review the output files for your tests?\
\'a0There\
\'a0might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0was\
the one built using the latest version from the 'naked-\
\'a0singularity'\
repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 'centos'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0one\
is a slightly updated version, but should not be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 significantly\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 different in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones\
\'a0OS.\
\'a0As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 yes,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0it\
shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0a\
code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0our\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 CentOS 7 container that might been causing a problem. What\
\'a0version\
of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 built\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 that\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you\
have on your Github profile, under both the \'93centos\'94 and\
\'a0\'93naked-\
singularity\'94 repositories, but none of them worked (they\
\'a0produce\
my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
\'a0command\
you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 expected.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 However, I also replicated mine, but this time with the\
\'a0container\
centos.simg that you provide, and my experiments also yield\
\'a0the\
same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compiled\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 inside\
or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 don\'92t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0you\
build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\'a0Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 \'a0>>>>>> \'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
\'a0compile\
and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
\'a0there\
is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
\'a0reported\
in\
the Singularity Slack channel were probably either incorrect\
and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 know\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0if\
you have any questions. Note, however, the container being\
\'a0used\
is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
\'a0compiler\
in\
the container and the DockerHub one does not have one\
\'a0installed\
by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
\
#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
\'a0out.\
\'a0I\
don\'92t know if I\'92ll get back to it tonight, but definitely\
\'a0will\
work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS >>>>>> \'a0>\
________________________________\
From: Mahidhar Tatineni via RT \'a0>>>>>> \'a0>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin \'a0>>> \'a0\'a0>\
Subject: [tickets.xsede.org\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket \'a0>>>>>>>>>\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Re: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
From: George Koubbe\
Time: Wed Apr 22 12:03:12 2020\
\
Martin,\
\
Yeah, well, I have no clue what is going on then. It is what it is and\
thank you so much for your support throughout this whole issue anyway!\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 On Apr 22, 2020, at 12:36 PM, Martin Kandes via RT \'a0wrote:\
\
\
Wed Apr 22 11:36:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 /tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
George,\
\
I build the containers using the build command: sudo singularity\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 build containter.simg container.deffile\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 18:40:57 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 Martin,\
\
This issue is very interesting indeed. It was nice of you to even\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 go\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0further and perform bandwidth and latency tests, which confirms\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0considerable performance difference between our containers,\
\'a0although we have not been able to locate the source of the\
\'a0difference so far. I do build with Singularity 3.5.0 installed\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 from\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0source. What is the exact command you use for building?\
\
And you are right regarding your intuition of what we want to do.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 We\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0are still exploring the options of how we are going to distribute\
\'a0the containers to end users depending on the application they try\
\'a0to run. Of course, as you point out, the bindings would need to\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 be\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0specific to the remote resource.\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
George,\
\
A side note: I finally had a look over the project you're working\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0on. It looks pretty interesting. So is the idea here that you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 might\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0--bind mount the local compilers/mpi on a resource into users\
\'a0containers when the container runs on that resource? If so, I'm\
\'a0still thinking this would be more complicated to support than\
\'a0simply having a set of static containers to choose from that\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 would\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0be compatible the compiler/mpi you find/discover at a resource.\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 But\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0maybe I'm missing something here.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0there\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 is some sort of performance difference between the 3 containers\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0I'm\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 testing with here, one of them is your version of the container.\
But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 test\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 problem with your container [1]. Your numbers are the lower\
runtimes in the middle. But as you can see, they do fluctuate to\
the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 ran\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 can see here, the results are more consistent, with your\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 underperforming. The fundamental problem seems to be that your\
container does not properly utilize Comet's infiniband network.\
This can be seen in the high latency [4]. The latency should be\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 I see right now is that your container was built with\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Singularity\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 3.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2?\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Also,\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0how\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 did you install your copy of Singularity 3.5.0? Did you compile\
from source? Or is that a binary executable that you downloaded\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0and\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0singularity-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0singularity-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
00:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0formatted\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container. Can you provide we with a path to your working\
directory\
on Comet where I can review the output files for your tests?\
There\
might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 was\
the one built using the latest version from the 'naked-\
singularity'\
repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 'centos'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 one\
is a slightly updated version, but should not be significantly\
different in functionality.\
\
The primary difference between our container and the one from\
DockerHub is that the one from DockerHub is a very barebones\
OS.\
As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 yes,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it\
shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 run\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 a\
code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0our\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 CentOS 7 container that might been causing a problem. What\
version\
of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 built\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Also, I have tried using the definition files for CentOS7\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 that\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
have on your Github profile, under both the \'93centos\'94 and\
\'93naked-\
singularity\'94 repositories, but none of them worked (they\
produce\
my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
command\
you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 expected.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 However, I also replicated mine, but this time with the\
container\
centos.simg that you provide, and my experiments also yield\
the\
same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compiled\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 inside\
or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 don\'92t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 >>>>>>> /tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
compile\
and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job,\
there\
is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
reported\
in\
the Singularity Slack channel were probably either incorrect\
and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 know\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 if\
you have any questions. Note, however, the container being\
used\
is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
compiler\
in\
the container and the DockerHub one does not have one\
installed\
by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
\
#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort\
out.\
I\
don\'92t know if I\'92ll get back to it tonight, but definitely\
will\
work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS>>>>>>> >\
________________________________\
From: Mahidhar Tatineni via RT >>>>>>> >\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin >>>> \'a0>\
Subject: [tickets.xsede.org\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
Status: open\
Ticket >>>>>>>>>>\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
Subject: Running containerized MPI applications on SDSC Comet\
From: Martin Kandes\
Time: Wed Apr 22 12:24:09 2020\
\
George,\
\
No problem. Yeah, I'm not sure what else to try. The only thing I can\
think of is maybe there is/was a bug in Singularity 3.5.0 that has\
something about the bind mounts not working properly and preventing\
the container from using/seeing the infiniband drivers and libraries\
on the underlying host system. That's really my only hypothesis right\
now. But again, we usually just install compatible drivers and\
libraries inside the container.\
\
Anyhow, if I sort this out eventually, I'll let you know. But I'm not\
sure we'll sort it out anytime soon.\
\
Marty Kandes\
SDSC User Services Group\
\
On Wed Apr 22 12:03:12 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Yeah, well, I have no clue what is going on then. It is what it is\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0thank you so much for your support throughout this whole issue\
\'a0\'a0anyway!\
\
Best,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 22, 2020, at 12:36 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Wed Apr 22 11:36:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
George,\
\
I build the containers using the build command: sudo singularity\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0build containter.simg container.deffile\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 18:40:57 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
This issue is very interesting indeed. It was nice of you to even\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0go\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0further and perform bandwidth and latency tests, which confirms\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0considerable performance difference between our containers,\
\'a0although we have not been able to locate the source of the\
\'a0difference so far. I do build with Singularity 3.5.0 installed\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0from\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0source. What is the exact command you use for building?\
\
And you are right regarding your intuition of what we want to do.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0We\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0are still exploring the options of how we are going to\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 distribute\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the containers to end users depending on the application they\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 try\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0to run. Of course, as you point out, the bindings would need to\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0be\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0specific to the remote resource.\
\
Best,\
George.\
\
On Apr 21, 2020, at 7:04 PM, Martin Kandes via RT\
\'a0wrote:\
\
\
Tue Apr 21 18:04:08 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559\
\
\
George,\
\
A side note: I finally had a look over the project you're\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 working\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0on. It looks pretty interesting. So is the idea here that you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0might\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0--bind mount the local compilers/mpi on a resource into users\
\'a0containers when the container runs on that resource? If so, I'm\
\'a0still thinking this would be more complicated to support than\
\'a0simply having a set of static containers to choose from that\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0would\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0be compatible the compiler/mpi you find/discover at a resource.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0But\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0maybe I'm missing something here.\
\
Marty Kandes\
SDSC User Services Group\
\
On Tue Apr 21 17:51:28 2020, mckandes wrote:\
Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge\
\'a0there\
is some sort of performance difference between the 3\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 containers\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I'm\
testing with here, one of them is your version of the\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 container.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0test\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 problem with your container [1]. Your numbers are the lower\
runtimes in the middle. But as you can see, they do fluctuate\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 to\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 the average as well for all containers.\
\
To move away from the simple Hello, World test problem, I then\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0ran\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the\
OSU Microbenchmarks for both bandwidth [2] and latency [3]. As\
\'a0you\
can see here, the results are more consistent, with your\
\'a0container\
underperforming. The fundamental problem seems to be that your\
container does not properly utilize Comet's infiniband\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 network.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 This can be seen in the high latency [4]. The latency should\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 be\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0on\
the order of 1 us at small message sizes [5].\
\
Now, what's causing this difference? I don't know. The only\
\'a0difference\
I see right now is that your container was built with\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Singularity\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 3.5.0, while my two ones are with 2.6.1 and 3.5.2.\
\
Do you want to try maybe updating your Singularity to 3.5.2?\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Also,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0how\
did you install your copy of Singularity 3.5.0? Did you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 compile\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 from source? Or is that a binary executable that you\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 downloaded\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0and\
installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity-\
omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-\
\'a0singularity-\
omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 are:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 -t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 00:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ export\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT\
\
wrote:\
\
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 Comet\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559\
\
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based\
\'a0formatted\
container. Can you provide we with a path to your working\
directory\
on Comet where I can review the output files for your tests?\
There\
might be some clues as to why you're seeing a different\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0runtime.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
Hi George,\
\
The container available in the examples directory I provided\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 was\
the one built using the latest version from the 'naked-\
singularity'\
repository. It's also available here on Comet [1]. The\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0'centos'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 one\
is a slightly updated version, but should not be\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 significantly\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 different in functionality.\
\
The primary difference between our container and the one\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 from\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 DockerHub is that the one from DockerHub is a very barebones\
OS.\
As\
I already mentioned, it doesn't even have a gcc compiler and\
supporting libraries, which were needed to support the Intel\
compilers and IntelMPI being mounted in the container. And\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0yes,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it\
shouldn't really make a difference between compiling within\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0the\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 container or outside the container as the software\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0environments\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 should be pretty close in nature. However, if I compile and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0run\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 a\
code within a container, I usually just compile inside the\
container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 of\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0our\
CentOS 7 container that might been causing a problem. What\
version\
of Singularity are you building the container with? Is it\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0built\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 on\
a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu\
\'a0wrote:\
Martin,\
\
Also, I have tried using the definition files for CentOS7\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0that\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
have on your Github profile, under both the \'93centos\'94 and\
\'93naked-\
singularity\'94 repositories, but none of them worked (they\
produce\
my\
old results). It would be helpful if you could share the\
definition\
file you used for the container creation as well as the\
command\
you\
used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0expected.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 However, I also replicated mine, but this time with the\
container\
centos.simg that you provide, and my experiments also yield\
the\
same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0compiled\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 inside\
or outside the container\
2) The container I create is only through the command (I\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0don\'92t\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 do\
anything else): $ singularity build centos7.sif\
docker://centos:centos7\
\
Having said this, what is the difference between the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 you\
build and the one I pull, that makes such a drastic\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0difference\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 in\
the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
/tickets/133559 \'a0>>>>>>>> /tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that\
compile\
and\
run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 job,\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 there\
is\
really no significant difference in the performance when\
everything\
is setup correctly. i.e., I think the low runtimes you\
reported\
in\
the Singularity Slack channel were probably either\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 incorrect\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 and/or\
not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0know\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 if\
you have any questions. Note, however, the container being\
used\
is\
not the same one you pulled from DockerHub. This one is our\
standard CentOS 7 Singularity container. I needed a gcc\
compiler\
in\
the container and the DockerHub one does not have one\
installed\
by\
default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020,\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 upon.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Transaction: Correspondence added by mckandes\
\'a0Queue: 0-SDSC\
Subject: Re: [tickets.xsede.org\
\
#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
Status: open\
Ticket URL:\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 sort\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 out.\
I\
don\'92t know if I\'92ll get back to it tonight, but\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 definitely\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 will\
work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS >>>>>>>> >\
________________________________\
From: Mahidhar Tatineni via RT \'a0>>>>>>>> >\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin \'a0>>>>> \'a0>\
Subject: [tickets.xsede.org\
#133559]\
Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 upon.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Transaction: Given to mckandes by mahidhar\
\'a0Queue: 0-SDSC\
Subject: Running containerized MPI applications on SDSC\
Comet\
\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\
Status: open\
Ticket \'a0>>>>>>>>>>>\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\
\
\
\
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
------------------------------------------------\
\
\
-------------------------------------------------------------------------\
\'a0\'a0\'a0Common Information\
-------------------------------------------------------------------------\
\
There is no need to reply to this message unless you want to RE-OPEN your\
ticket with ID [tickets.xsede.org #133559].\
\
If you want to simply add a COMMENT to this ticket without re-opening the ticket, click below:\
mailto:helpcomments@tickets.xsede.org?subject=[tickets.xsede.org%20#133559]&body=%20\
\
\
Please note:\
- ALWAYS include the string [tickets.xsede.org #133559] in the subject line of all future correspondence about this issue.\
- Do NOT attach or include the content of previous emails already sent to you by rt.\
\
\
\page \pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 From: "Martin Kandes via RT" <help@xsede.org>\
Subject: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 21, 2020 at 6:51:29 PM EDT\
To: glk35@scarletmail.rutgers.edu\
Reply-To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 \
Tue Apr 21 17:51:28 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-/tickets/133559 \
\
\
Hi George,\
\
I'm a bit at a loss of what the issue is here. I do acknowledge there is some sort of performance difference between the 3 containers I'm testing with here, one of them is your version of the container. But each of them is effectively the same.\
\
I did confirm the performance improvement on the Hello, World test problem with your container [1]. Your numbers are the lower runtimes in the middle. But as you can see, they do fluctuate to the average as well for all containers. \
\
To move away from the simple Hello, World test problem, I then ran the OSU Microbenchmarks for both bandwidth [2] and latency [3]. As you can see here, the results are more consistent, with your container underperforming. The fundamental problem seems to be that your container does not properly utilize Comet's infiniband network. This can be seen in the high latency [4]. The latency should be on the order of 1 us at small message sizes [5]. \
\
Now, what's causing this difference? I don't know. The only difference I see right now is that your container was built with Singularity 3.5.0, while my two ones are with 2.6.1 and 3.5.2. \
\
Do you want to try maybe updating your Singularity to 3.5.2? Also, how did you install your copy of Singularity 3.5.0? Did you compile from source? Or is that a binary executable that you downloaded and installed?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-singularity.o* | grep real\
real 6.43\
real 7.35\
real 6.98\
real 6.94\
real 6.70\
real 6.42\
real 1.51\
real 1.71\
real 2.20\
real 6.94\
real 6.32\
real 6.15\
real 6.18\
real 6.42\
real 20.95\
\
[2]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-singularity-omb-bw.o* | grep real\
real 4.10\
real 4.42\
real 10.72\
real 9.13\
real 8.17\
real 16.22\
real 21.17\
real 11.26\
real 15.07\
real 5.65\
real 5.64\
real 5.64\
real 4.07\
real 2.73\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[3]\
\
[mkandes@comet-ln2 bind-host-compilers-mpi]$ cat intelmpi-singularity-omb-latency.o* | grep real\
real 6.89\
real 6.94\
real 6.65\
real 7.10\
real 7.10\
real 25.54\
real 31.74\
real 27.32\
real 25.73\
real 26.23\
real 8.38\
real 9.11\
real 8.49\
real 8.83\
real 8.22\
[mkandes@comet-ln2 bind-host-compilers-mpi]$\
\
[4]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.24\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.30\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.34\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a018.52\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a020.52\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a024.59\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a034.53\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a032.56\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a039.36\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.09\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.81\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a079.39\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0100.26\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0125.25\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0136.19\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0318.85\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0348.84\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0570.59\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01036.50\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01923.67\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03803.62\
real 26.23\
user 0.02\
sys 0.01\
\
[5]\
\
# OSU MPI Latency Test v5.6.2\
# Size \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Latency (us)\
0 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
1 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
2 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.18\
4 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.21\
8 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.22\
16 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.20\
32 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01.30\
64 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.09\
128 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.05\
256 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.10\
512 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.22\
1024 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a02.47\
2048 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.00\
4096 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a03.65\
8192 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a04.72\
16384 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a06.11\
32768 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a09.38\
65536 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a014.90\
131072 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a026.15\
262144 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a048.69\
524288 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0316.88\
1048576 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0431.61\
2097152 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0659.63\
4194304 \'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a01137.41\
real 6.65\
user 0.02\
sys 0.01\
\
On Mon Apr 20 19:09:55 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Definitely, the path to my current working directory is:\
\
/home/karahbit/test/not_working\
\
Once there, the exact commands I am running to see results are:\
\
$ srun --partition=debug --pty --nodes=2 --ntasks-per-node=24 -t\
\'a0\'a000:30:00 --wait=0 --export=ALL /bin/bash\
\
$ module purge\
\
$ module load intel intelmpi\
\
$ export\
\'a0\'a0SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/bin/intel64\
\
$ export\
\'a0\'a0SINGULARITYENV_LD_LIBRARY_PATH=/etc/libibverbs.d:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/compiler/lib/intel64_lin:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/../compiler/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/ipp/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mkl/lib/intel64:/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/tbb/lib/intel64/gcc4.4\
\
$ time -p mpirun singularity exec --bind /opt centos-repo.simg\
\'a0\'a0./hello_world_intel\
\
\
And I see the following result:\
\
\
\
I appreciate your support on this matter,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 20, 2020, at 8:00 PM, Martin Kandes via RT <help@xsede.org>\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
Mon Apr 20 19:00:13 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0Status: user_wait\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
George,\
\
I'm still not seeing a problem using a newer *.sif-based formatted\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container. Can you provide we with a path to your working directory\
\'a0\'a0on Comet where I can review the output files for your tests? There\
\'a0\'a0might be some clues as to why you're seeing a different runtime.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Thanks,\
\
Marty Kandes\
SDSC User Services Group\
\
On Mon Apr 20 11:41:51 2020, mckandes wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Hi George,\
\
The container available in the examples directory I provided you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0was\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the one built using the latest version from the 'naked-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0singularity'\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0repository. It's also available here on Comet [1]. The 'centos'\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0one\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0is a slightly updated version, but should not be significantly\
\'a0different in functionality.\
\
The primary difference between our container and the one from\
\'a0DockerHub is that the one from DockerHub is a very barebones OS.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0As\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0I already mentioned, it doesn't even have a gcc compiler and\
\'a0supporting libraries, which were needed to support the Intel\
\'a0compilers and IntelMPI being mounted in the container. And yes,\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0it\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0shouldn't really make a difference between compiling within the\
\'a0container or outside the container as the software environments\
\'a0should be pretty close in nature. However, if I compile and run a\
\'a0code within a container, I usually just compile inside the\
\'a0container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our\
\'a0CentOS 7 container that might been causing a problem. What\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0version\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0of Singularity are you building the container with? Is it built\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls\
\'a0/share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
\'a0have on your Github profile, under both the \'93centos\'94 and \'93naked-\
\'a0singularity\'94 repositories, but none of them worked (they produce\
\'a0my\
\'a0old results). It would be helpful if you could share the\
\'a0definition\
\'a0file you used for the container creation as well as the command\
\'a0you\
\'a0used to build it.\
\
Thank you,\
George.\
\
On Apr 19, 2020, at 5:57 PM, George Koubbe\
\'a0<glk35@scarletmail.rutgers.edu> wrote:\
\
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\'a0However, I also replicated mine, but this time with the\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0container\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0centos.simg that you provide, and my experiments also yield the\
\'a0same results as yours.\
\
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled\
\'a0inside\
\'a0or outside the container\
2) The container I create is only through the command (I don\'92t do\
\'a0anything else): $ singularity build centos7.sif\
\'a0docker://centos:centos7 <docker://centos:centos7>\
\
Having said this, what is the difference between the container\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0build and the one I pull, that makes such a drastic difference\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0in\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0the results?\
\
Thank you,\
George.\
\
On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<help@xsede.org\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0<mailto:help@xsede.org>> wrote:\
\
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0<mailto:glk35@scarletmail.rutgers.edu>\
\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559 <https://portal.xsede.org/group/xup/tickets/-\
\'a0/tickets/133559>\
\
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile\
\'a0and\
\'a0run your MPI hello, world code here [1].\
\
As you can see in the standard output files from each job, there\
\'a0is\
\'a0really no significant difference in the performance when\
\'a0everything\
\'a0is setup correctly. i.e., I think the low runtimes you reported\
\'a0in\
\'a0the Singularity Slack channel were probably either incorrect\
\'a0and/or\
\'a0not an apples-to-apples comparison.\
\
Anyhow, have a look at how I've set things up and let me know if\
\'a0you have any questions. Note, however, the container being used\
\'a0is\
\'a0not the same one you pulled from DockerHub. This one is our\
\'a0standard CentOS 7 Singularity container. I needed a gcc compiler\
\'a0in\
\'a0the container and the DockerHub one does not have one installed\
\'a0by\
\'a0default.\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\'a0<mailto:glk35@scarletmail.rutgers.edu> wrote:\
Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT\
\'a0<help@xsede.org\
\'a0<mailto:help@xsede.org>>\
wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Re: [tickets.xsede.org <http://tickets.xsede.org/>\
\'a0#133559] Running containerized\
MPI applications on SDSC Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0<mailto:glk35@scarletmail.rutgers.edu>\
\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\'a0<https://portal.xsede.org/group/xup/tickets/->\
/tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0I\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 don\'92t know if I\'92ll get back to it tonight, but definitely will\
\'a0work on\
it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<https://aka.ms/o0ukef\
\'a0<https://aka.ms/o0ukef>>\
________________________________\
From: Mahidhar Tatineni via RT <help@xsede.org\
\'a0<mailto:help@xsede.org>>\
Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<mailto:mkandes@sdsc.edu>>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Subject: [tickets.xsede.org <http://tickets.xsede.org/>\
\'a0#133559]\
\'a0Running containerized MPI\
applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0Queue: 0-SDSC\
\'a0Subject: Running containerized MPI applications on SDSC\
\'a0Comet\
\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\'a0<mailto:glk35@scarletmail.rutgers.edu>\
\'a0\'a0Status: open\
Ticket <URL:\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
\page \pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 From: "Martin Kandes via RT" <help@xsede.org>\
Subject: [tickets.xsede.org #133559] Running containerized MPI applications on SDSC Comet\
Date: April 20, 2020 at 12:41:51 PM EDT\
To: glk35@scarletmail.rutgers.edu\
Reply-To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 \
Mon Apr 20 11:41:51 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
\'a0Requestors: glk35@scarletmail.rutgers.edu\
\'a0\'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-/tickets/133559 \
\
\
Hi George,\
\
The container available in the examples directory I provided you was the one built using the latest version from the 'naked-singularity' repository. It's also available here on Comet [1]. The 'centos' one is a slightly updated version, but should not be significantly different in functionality. \
\
The primary difference between our container and the one from DockerHub is that the one from DockerHub is a very barebones OS. As I already mentioned, it doesn't even have a gcc compiler and supporting libraries, which were needed to support the Intel compilers and IntelMPI being mounted in the container. And yes, it shouldn't really make a difference between compiling within the container or outside the container as the software environments should be pretty close in nature. However, if I compile and run a code within a container, I usually just compile inside the container as it's the safer option.\
\
Anyhow, I'm not quite sure what went wrong with your build of our CentOS 7 container that might been causing a problem. What version of Singularity are you building the container with? Is it built on a Linux system?\
\
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
[mkandes@comet-ln2 ~]$ ls /share/apps/compute/singularity/images/centos/\
centos-openmpi.simg\
centos.simg\
centos-v6.10-20190916.simg\
centos-v7.6.1810-20190513.simg\
centos-v7.6.1810-openmpi-v1.8.4-20190514.simg\
centos-v7.7.1908-20190914.simg\
centos-v7.7.1908-20190919.simg\
centos-v7.7.1908-20200129.simg\
centos-v7.7.1908-openmpi-v1.8.4-20190919.simg\
centos-v7.7.1908-openmpi-v3.1.4-20200211.simg\
[mkandes@comet-ln2 ~]$\
\
On Sun Apr 19 18:02:01 2020, glk35@scarletmail.rutgers.edu wrote:\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 Martin,\
\
Also, I have tried using the definition files for CentOS7 that you\
\'a0\'a0have on your Github profile, under both the \'93centos\'94 and \'93naked-\
\'a0\'a0singularity\'94 repositories, but none of them worked (they produce my\
\'a0\'a0old results). It would be helpful if you could share the definition\
\'a0\'a0file you used for the container creation as well as the command you\
\'a0\'a0used to build it.\
\
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 On Apr 19, 2020, at 5:57 PM, George Koubbe\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<glk35@scarletmail.rutgers.edu> wrote:\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Dear Martin,\
\
I have narrowed down my problem to the container itself.\
\
I just replicated your experiments and it worked as expected.\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0However, I also replicated mine, but this time with the container\
\'a0\'a0centos.simg that you provide, and my experiments also yield the\
\'a0\'a0same results as yours.\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
It\'92s worth noting that:\
\
1) It did not make a difference if the executable is compiled inside\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0or outside the container\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 2) The container I create is only through the command (I don\'92t do\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0anything else): $ singularity build centos7.sif\
\'a0\'a0docker://centos:centos7 <docker://centos:centos7>\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Having said this, what is the difference between the container you\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0build and the one I pull, that makes such a drastic difference in\
\'a0\'a0the results?\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
Thank you,\
George.\
\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 On Apr 18, 2020, at 6:20 PM, Martin Kandes via RT <help@xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<mailto:help@xsede.org>> wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Sat Apr 18 17:20:07 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<mailto:glk35@scarletmail.rutgers.edu>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0/tickets/133559 <https://portal.xsede.org/group/xup/tickets/-\
\'a0\'a0/tickets/133559>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
\
Hi George,\
\
I've placed a copy of my example batch job scripts that compile and\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0run your MPI hello, world code here [1].\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
As you can see in the standard output files from each job, there is\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0really no significant difference in the performance when everything\
\'a0\'a0is setup correctly. i.e., I think the low runtimes you reported in\
\'a0\'a0the Singularity Slack channel were probably either incorrect and/or\
\'a0\'a0not an apples-to-apples comparison.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Anyhow, have a look at how I've set things up and let me know if\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0you have any questions. Note, however, the container being used is\
\'a0\'a0not the same one you pulled from DockerHub. This one is our\
\'a0\'a0standard CentOS 7 Singularity container. I needed a gcc compiler in\
\'a0\'a0the container and the DockerHub one does not have one installed by\
\'a0\'a0default.\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \
Marty Kandes\
SDSC User Services Group\
\
[1]\
\
/share/apps/examples/singularity/bind-host-compilers-mpi\
\
On Fri Apr 17 19:30:30 2020, glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<mailto:glk35@scarletmail.rutgers.edu> wrote:\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Thank you Martin, I appreciate your support.\
\
George.\
\
On Apr 17, 2020, at 8:15 PM, Martin Kandes via RT <help@xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<mailto:help@xsede.org>>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 wrote:\
\
\
Fri Apr 17 19:15:18 2020: Request 133559 was acted upon.\
Transaction: Correspondence added by mckandes\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Re: [tickets.xsede.org <http://tickets.xsede.org/>\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0#133559] Running containerized\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<mailto:glk35@scarletmail.rutgers.edu>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket URL: https://portal.xsede.org/group/xup/tickets/-\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<https://portal.xsede.org/group/xup/tickets/->\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 /tickets/133559\
\
\
Hi George,\
\
There\'92s still at least one more issue I\'92m trying to sort out. I\
don\'92t know if I\'92ll get back to it tonight, but definitely will\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0work on\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 it again tomorrow and keep you posted.\
\
Marty Kandes\
SDSC User Services Group\
\
Get Outlook for iOS<https://aka.ms/o0ukef\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<https://aka.ms/o0ukef>>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 ________________________________\
From: Mahidhar Tatineni via RT <help@xsede.org\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<mailto:help@xsede.org>>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 Sent: Thursday, April 16, 2020 10:19:20 PM\
To: Kandes, Martin <mkandes@sdsc.edu <mailto:mkandes@sdsc.edu>>\
Subject: [tickets.xsede.org <http://tickets.xsede.org/> #133559]\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0Running containerized MPI\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 applications on SDSC Comet\
\
\
Fri Apr 17 00:19:20 2020: Request 133559 was acted upon.\
Transaction: Given to mckandes by mahidhar\
\'a0\'a0\'a0\'a0Queue: 0-SDSC\
\'a0\'a0Subject: Running containerized MPI applications on SDSC Comet\
\'a0\'a0\'a0\'a0Owner: mckandes\
Requestors: glk35@scarletmail.rutgers.edu\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0<mailto:glk35@scarletmail.rutgers.edu>\
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 \'a0\'a0\'a0Status: open\
Ticket <URL:\
\
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \'a0\'a0https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\'a0\'a0<https://urldefense.com/v3/__https://tickets.xsede.org/Ticket/Display.html?id=133559__;!!Mih3wA!SgBMtwhKbJt3FH5TIS5xI2azjn5PGWoC_znC8KPQn-\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf6 \strokec6 2nOMdZ3ikPm94px2MBcFY$ \'a0>\
\
This transaction appears to have no content\
\
\
\
\
\pard\pardeftab720\partightenfactor0
\cf5 \strokec5 \
\pard\pardeftab720\partightenfactor0
\cf4 \strokec4 \
\pard\pardeftab720\partightenfactor0
\cf3 \strokec3 \
\
\
\page \pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf2 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 From: "XSEDE Help Desk via RT" <help@xsede.org>\
Subject: [tickets.xsede.org #133559] AutoReply: Running containerized MPI applications on SDSC Comet\
Date: April 16, 2020 at 9:35:38 PM EDT\
To: glk35@scarletmail.rutgers.edu\
Reply-To: help@xsede.org\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1 \cf2 \
\pard\pardeftab720\partightenfactor0
\cf3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 \
Greetings,\
\
This message has been automatically generated in response to the\
creation of a trouble ticket regarding:\
	"Running containerized MPI applications on SDSC Comet", \
a summary of which appears below.\
\
Your ticket has been assigned an ID of [tickets.xsede.org #133559].\
\
Please include the string:\
\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0[tickets.xsede.org #133559]\
\
in the subject line of all future correspondence about this issue. To do so, \
you may reply to this message.\
\
You may also view the ticket in the XSEDE User Portal: https://portal.xsede.org/group/xup/tickets/-/tickets/133559\
\
\
\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0\'a0Thank you,\
\
\
-------------------------------------------------------------------------\
Hello,\
\
I am running a containerized MPI Hello World application on SDSC Comet.\
\
1. I just tried running an mpirun command directly on the \'93debug" partition of Comet, requesting 2 nodes and using the full 48 cores of said nodes.\
2. I made sure the MPI implementation (Intel MPI 18.1) is the same for both containerized/non containerized \'93hello world\'94 executable.\
\
\
This is exactly what I do since I login to comet:\
\
For Intel MPI \
\
On login node:\
$ module purge\
$ module load intel singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c\
$ export PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/:$PATH\
$ export LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib:$LD_LIBRARY_PATH\
$ export SINGULARITYENV_PREPEND_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64/lib\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ module purge\
$ module load intel singularity\
$ mpicc mpi_hello_world.c -o hello_world_intel\
$ mpirun -n 48 singularity exec --bind /opt/intel/2018.1.163/compilers_and_libraries_2018.1.163/linux/mpi/intel64 $HOME/centos-openmpi.sif $HOME/hello_world_intel\
\
\
\
For mvapich2:\
\
On login node:\
$ module load singularity\
$ singularity build centos-mpi.sif docker://centos:centos7\
$ wget https://raw.githubusercontent.com/wesleykendall/mpitutorial/gh-pages/tutorials/mpi-hello-world/code/mpi_hello_world.c\
$ export SINGULARITYENV_PREPEND_PATH=/opt/mvapich2/intel/ib/bin/\
$ export SINGULARITYENV_LD_LIBRARY_PATH=/opt/mvapich2/intel/ib/lib/:/opt/intel/2018.1.163/lib/intel64:/etc/libibverbs.d\
$ srun --partition=compute --pty --nodes=2 --ntasks-per-node=24 -t 00:30:00 --wait=0 --export=ALL /bin/bash\
\
On compute node:\
$ mpicc mpi_hello_world.c -o hello_world_mpich\
$ mpirun -n 48 singularity exec --bind /opt/mvapich2/intel/ib/ --bind /lib64 --bind /opt/intel/2018.1.163/lib/intel64 --bind /etc/libibverbs.d $HOME/centos-openmpi.sif $HOME/hello_world_mpich\
\
\
\
The Question:\
I can\'92t find an explanation on why running \'93hello_world_intel\'94 inside the container is faster than outside, according only to the commands I typed of course. When I changed the MPI implementation to mvapich2, running the executable inside the container was slightly slower than outside (this is what I originally expected).\
\
\
\
The command I use to run outside the container is just:\
$ mpirun -n 48 $HOME/hello_world_intel\
\
\
The command used to measure elapsed time was:\
$ /usr/bin/time -v mpirun ... \'85 \'85\
\
\
Just to give you exact numbers, when running with intel I got:\
non-containerized: ~3.36 s\
containerized: ~0.48 s\
\
When running with mvapich2, I got:\
non-containerized: ~3.37 s\
containerized: ~4.01 s\
\
\
\
\
Your help would be greatly appreciated, you don\'92t know what it would mean to me. Thank you!\
George Koubbe.\
\
\
}