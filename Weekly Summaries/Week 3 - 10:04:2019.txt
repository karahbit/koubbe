{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Oblique;}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \expnd0\expndtw0\kerning0
Dear Shantenu,\
\
Below I attach the results of my first experiments with EnTK in the world of distributed/parallel computing on HPCs. A warm mention to Matteo for being so helpful.\
\
As for HPO:\
\
Any machine learning model has some values (hyperparameters) that need to be specified 
\f1\i a priori
\f0\i0  before the training of the dataset. They help adapt the model to the data and they influence the quality of the prediction. Hyperparameter optimization deals with the search of the best combination of values for the given model, and there are already many methods that help us find them.\
\
Our goal here is not to develop an engine for HPO, but some sort of wrapper that will use an already existing engine on top of RCT (EnTK). It will be like adding another layer to our already existing set of RCT. For this, I need to:\
\
1) Have something like a Top-10 HPO libraries that already exist.\
2) Explore which of them will benefit us the most: pros/cons?\
3) Elaborate a functional requirements document specifying inputs/outputs to/from our HPO library: dataset, parameters, CPUs, GPUs?\
\
Best,\
 George.}